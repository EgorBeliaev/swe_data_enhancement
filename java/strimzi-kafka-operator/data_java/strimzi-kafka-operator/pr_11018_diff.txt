diff --git a/development-docs/systemtests/io.strimzi.systemtest.kafka.KafkaST.md b/development-docs/systemtests/io.strimzi.systemtest.kafka.KafkaST.md
index 43fff6e2f83..6273d08ee53 100644
--- a/development-docs/systemtests/io.strimzi.systemtest.kafka.KafkaST.md
+++ b/development-docs/systemtests/io.strimzi.systemtest.kafka.KafkaST.md
@@ -60,8 +60,8 @@
 
 | Step | Action | Result |
 | - | - | - |
-| 1. | Deploy Kafka and its components with custom specifications, including specifying resources and JVM configuration. | Kafka and its components (ZooKeeper, Entity Operator) are deployed. |
-| 2. | For each component (Kafka, ZooKeeper, Topic Operator, User Operator), verify specified configuration of JVM, resources, and also environment variables. | Each of the components has requests and limits assigned correctly, JVM, and environment variables configured according to the specification. |
+| 1. | Deploy Kafka and its components with custom specifications, including specifying resources and JVM configuration. | Kafka and Entity Operator are deployed. |
+| 2. | For each component (Kafka, Topic Operator, User Operator), verify specified configuration of JVM, resources, and also environment variables. | Each of the components has requests and limits assigned correctly, JVM, and environment variables configured according to the specification. |
 | 3. | Wait for a time to observe that no initiated components need rolling update. | All Kafka components remain in stable state. |
 
 **Labels:**
@@ -69,23 +69,6 @@
 * [kafka](labels/kafka.md)
 
 
-## testKRaftMode
-
-**Description:** This test case verifies basic working of Kafka Cluster managed by Cluster Operator with KRaft.
-
-**Steps:**
-
-| Step | Action | Result |
-| - | - | - |
-| 1. | Deploy Kafka annotated to enable KRaft (and additionally annotated to enable KafkaNodePool management), and configure a KafkaNodePool resource to target the Kafka cluster. | Kafka is deployed, and the KafkaNodePool resource targets the cluster as expected. |
-| 2. | Produce and consume messages in given Kafka Cluster. | Clients can produce and consume messages. |
-| 3. | Trigger manual Rolling Update. | Rolling update is triggered and completed shortly after. |
-
-**Labels:**
-
-* [kafka](labels/kafka.md)
-
-
 ## testKafkaJBODDeleteClaimsTrueFalse
 
 **Description:** This test case verifies Kafka running with persistent JBOD storage, and configured with the `deleteClaim`  storage property.
@@ -144,13 +127,13 @@
 
 ## testReadOnlyRootFileSystem
 
-**Description:** This test case verifies that Kafka (with all its components, including Zookeeper, Entity Operator, KafkaExporter, CruiseControl) configured with 'withReadOnlyRootFilesystem' can be deployed and also works correctly.
+**Description:** This test case verifies that Kafka (with all its components, including Entity Operator, KafkaExporter, CruiseControl) configured with 'withReadOnlyRootFilesystem' can be deployed and also works correctly.
 
 **Steps:**
 
 | Step | Action | Result |
 | - | - | - |
-| 1. | Deploy persistent Kafka with 3 Kafka and Zookeeper replicas, Entity Operator, CruiseControl, and KafkaExporter. Each component has configuration 'withReadOnlyRootFilesystem' set to true. | Kafka and its components are deployed. |
+| 1. | Deploy persistent Kafka with 3 replicas, Entity Operator, CruiseControl, and KafkaExporter. Each component has configuration 'withReadOnlyRootFilesystem' set to true. | Kafka and its components are deployed. |
 | 2. | Create Kafka producer and consumer. | Kafka clients are successfully created. |
 | 3. | Produce and consume messages using created clients. | Messages are successfully sent and received. |
 
diff --git a/development-docs/systemtests/io.strimzi.systemtest.log.LoggingChangeST.md b/development-docs/systemtests/io.strimzi.systemtest.log.LoggingChangeST.md
index bca0946b4cc..4f279e7ea74 100644
--- a/development-docs/systemtests/io.strimzi.systemtest.log.LoggingChangeST.md
+++ b/development-docs/systemtests/io.strimzi.systemtest.log.LoggingChangeST.md
@@ -229,14 +229,14 @@
 
 ## testJSONFormatLogging
 
-**Description:** Test verifying that the logging in JSON format works correctly across Kafka, Zookeeper, and operators.
+**Description:** Test verifying that the logging in JSON format works correctly across Kafka and operators.
 
 **Steps:**
 
 | Step | Action | Result |
 | - | - | - |
 | 1. | Assume non-Helm and non-OLM installation. | Assumption holds true. |
-| 2. | Create ConfigMaps for Kafka, Zookeeper, and operators with JSON logging configuration. | ConfigMaps created and applied. |
+| 2. | Create ConfigMaps for Kafka and operators with JSON logging configuration. | ConfigMaps created and applied. |
 | 3. | Deploy Kafka cluster with the configured logging setup. | Kafka cluster deployed successfully. |
 | 4. | Perform pod snapshot for controllers, brokers, and entity operators. | Pod snapshots successfully captured. |
 | 5. | Verify logs are in JSON format for all components. | Logs are in JSON format. |
diff --git a/development-docs/systemtests/labels/kafka.md b/development-docs/systemtests/labels/kafka.md
index eb7c7d3b550..d251d5cbab3 100644
--- a/development-docs/systemtests/labels/kafka.md
+++ b/development-docs/systemtests/labels/kafka.md
@@ -67,7 +67,6 @@ These tests are crucial to ensure that Kafka clusters can handle production work
 - [testCustomCertNodePortAndTlsRollingUpdate](../io.strimzi.systemtest.kafka.listeners.ListenersST.md)
 - [testChangingInternalToExternalLoggingTriggerRollingUpdate](../io.strimzi.systemtest.log.LoggingChangeST.md)
 - [testKafkaQuotasPluginWithBandwidthLimitation](../io.strimzi.systemtest.kafka.QuotasST.md)
-- [testKRaftMode](../io.strimzi.systemtest.kafka.KafkaST.md)
 - [testLoggingHierarchy](../io.strimzi.systemtest.log.LoggingChangeST.md)
 - [testKafkaNodePoolBrokerIdsManagementUsingAnnotations](../io.strimzi.systemtest.kafka.KafkaNodePoolST.md)
 - [testLoadBalancer](../io.strimzi.systemtest.kafka.listeners.ListenersST.md)
diff --git a/systemtest/src/main/java/io/strimzi/systemtest/annotations/KRaftNotSupported.java b/systemtest/src/main/java/io/strimzi/systemtest/annotations/KRaftNotSupported.java
deleted file mode 100644
index 239ea9df644..00000000000
--- a/systemtest/src/main/java/io/strimzi/systemtest/annotations/KRaftNotSupported.java
+++ /dev/null
@@ -1,20 +0,0 @@
-/*
- * Copyright Strimzi authors.
- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).
- */
-package io.strimzi.systemtest.annotations;
-
-import org.junit.jupiter.api.extension.ExtendWith;
-
-import java.lang.annotation.ElementType;
-import java.lang.annotation.Retention;
-import java.lang.annotation.RetentionPolicy;
-import java.lang.annotation.Target;
-
-@Target({ElementType.METHOD, ElementType.TYPE})
-@Retention(RetentionPolicy.RUNTIME)
-@ExtendWith(KRaftNotSupportedCondition.class)
-public @interface KRaftNotSupported {
-
-    String value() default "KRaft is not supported with configuration in this test case.";
-}
diff --git a/systemtest/src/main/java/io/strimzi/systemtest/annotations/KRaftNotSupportedCondition.java b/systemtest/src/main/java/io/strimzi/systemtest/annotations/KRaftNotSupportedCondition.java
deleted file mode 100644
index 1b441dce353..00000000000
--- a/systemtest/src/main/java/io/strimzi/systemtest/annotations/KRaftNotSupportedCondition.java
+++ /dev/null
@@ -1,28 +0,0 @@
-/*
- * Copyright Strimzi authors.
- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).
- */
-package io.strimzi.systemtest.annotations;
-
-import io.strimzi.systemtest.Environment;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
-import org.junit.jupiter.api.extension.ConditionEvaluationResult;
-import org.junit.jupiter.api.extension.ExecutionCondition;
-import org.junit.jupiter.api.extension.ExtensionContext;
-
-public class KRaftNotSupportedCondition implements ExecutionCondition {
-    private static final Logger LOGGER = LogManager.getLogger(KRaftNotSupportedCondition.class);
-
-    @Override
-    public ConditionEvaluationResult evaluateExecutionCondition(ExtensionContext extensionContext) {
-        if (!Environment.isKRaftModeEnabled()) {
-            return ConditionEvaluationResult.enabled("Test is enabled");
-        } else {
-            LOGGER.warn("According to {} env variable with value: {}, the KRaft mode is used, skipping this test because is not KRaft mode compliant",
-                    Environment.STRIMZI_FEATURE_GATES_ENV,
-                    Environment.STRIMZI_FEATURE_GATES);
-            return ConditionEvaluationResult.disabled("Test is disabled");
-        }
-    }
-}
diff --git a/systemtest/src/main/java/io/strimzi/systemtest/metrics/ZookeeperMetricsComponent.java b/systemtest/src/main/java/io/strimzi/systemtest/metrics/ZookeeperMetricsComponent.java
deleted file mode 100644
index 9458a9cafce..00000000000
--- a/systemtest/src/main/java/io/strimzi/systemtest/metrics/ZookeeperMetricsComponent.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Copyright Strimzi authors.
- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).
- */
-package io.strimzi.systemtest.metrics;
-
-import io.fabric8.kubernetes.api.model.LabelSelector;
-import io.strimzi.api.kafka.model.kafka.KafkaResources;
-import io.strimzi.systemtest.TestConstants;
-import io.strimzi.systemtest.resources.crd.KafkaResource;
-
-/**
- * Concrete implementation of BaseMetricsComponent for Zookeeper.
- */
-public class ZookeeperMetricsComponent extends BaseMetricsComponent {
-
-    /**
-     * Factory method to create a new instance of ZookeeperMetricsComponent.
-     * @param componentName     the name of the component
-     * @return                  a new instance of ZookeeperMetricsComponent
-     */
-    public static ZookeeperMetricsComponent create(final String componentName) {
-        return new ZookeeperMetricsComponent(componentName);
-    }
-
-    /**
-     * Private constructor to enforce the use of the factory method.
-     */
-    private ZookeeperMetricsComponent(String componentName) {
-        super(null, componentName);
-    }
-
-    /**
-     * Provides the default metrics port specifically for Zookeeper.
-     * @return int representing the Zookeeper metrics port
-     */
-    @Override
-    public int getDefaultMetricsPort() {
-        return TestConstants.COMPONENTS_METRICS_PORT;
-    }
-
-    /**
-     * Provides the label selector specific to Zookeeper.
-     * @return LabelSelector for the Zookeeper deployment
-     */
-    @Override
-    public LabelSelector getLabelSelector() {
-        return KafkaResource.getLabelSelector(componentName, KafkaResources.zookeeperComponentName(componentName));
-    }
-}
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaNodePoolST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaNodePoolST.java
index 7d66eb793e2..a1dcc3e26e0 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaNodePoolST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaNodePoolST.java
@@ -189,7 +189,6 @@ void testKafkaNodePoolBrokerIdsManagementUsingAnnotations() {
         }
     )
     void testNodePoolsRolesChanging() {
-        assumeTrue(Environment.isKRaftModeEnabled());
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
         // volatile KNP which will be transitioned from mixed to -> controller only role and afterward to mixed role again
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java
index d72654c28c5..b28d23df01d 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java
@@ -108,7 +108,6 @@
 import static org.hamcrest.Matchers.emptyOrNullString;
 import static org.hamcrest.Matchers.not;
 import static org.hamcrest.Matchers.notNullValue;
-import static org.junit.jupiter.api.Assumptions.assumeTrue;
 
 @Tag(REGRESSION)
 @SuppressWarnings("checkstyle:ClassFanOutComplexity")
@@ -126,12 +125,11 @@ class KafkaST extends AbstractST {
     private static final String OPENSHIFT_CLUSTER_NAME = "openshift-my-cluster";
 
     @ParallelNamespaceTest
-    @SuppressWarnings({"checkstyle:MethodLength", "deprecation"}) // ZooKeeper is deprecated, but some API methods are still called in this method
     @TestDoc(
         description = @Desc("This test case verifies that Pod's resources (limits and requests), custom JVM configurations, and expected Java configuration are propagated correctly to Pods, containers, and processes."),
         steps = {
-            @Step(value = "Deploy Kafka and its components with custom specifications, including specifying resources and JVM configuration.", expected = "Kafka and its components (ZooKeeper, Entity Operator) are deployed."),
-            @Step(value = "For each component (Kafka, ZooKeeper, Topic Operator, User Operator), verify specified configuration of JVM, resources, and also environment variables.", expected = "Each of the components has requests and limits assigned correctly, JVM, and environment variables configured according to the specification."),
+            @Step(value = "Deploy Kafka and its components with custom specifications, including specifying resources and JVM configuration.", expected = "Kafka and Entity Operator are deployed."),
+            @Step(value = "For each component (Kafka, Topic Operator, User Operator), verify specified configuration of JVM, resources, and also environment variables.", expected = "Each of the components has requests and limits assigned correctly, JVM, and environment variables configured according to the specification."),
             @Step(value = "Wait for a time to observe that no initiated components need rolling update.", expected = "All Kafka components remain in stable state.")
         },
         labels = {
@@ -144,7 +142,6 @@ void testJvmAndResources() {
         ArrayList<SystemProperty> javaSystemProps = new ArrayList<>();
         javaSystemProps.add(new SystemPropertyBuilder().withName("javax.net.debug")
                 .withValue("verbose").build());
-
         Map<String, String> jvmOptionsXX = new HashMap<>();
         jvmOptionsXX.put("UseG1GC", "true");
 
@@ -197,10 +194,6 @@ void testJvmAndResources() {
                     .withResources(brokersResReq)
                     .withJvmOptions(brokerJvmOptions)
                 .endKafka()
-                .editZookeeper()
-                    .withResources(controlResReq)
-                    .withJvmOptions(controlJvmOptions)
-                .endZookeeper()
                 .withNewEntityOperator()
                     .withNewTopicOperator()
                         .withResources(
@@ -234,10 +227,6 @@ void testJvmAndResources() {
             .endSpec()
             .build();
 
-        if (Environment.isKRaftModeEnabled()) {
-            kafka.getSpec().setZookeeper(null);
-        }
-
         resourceManager.createResourceWithWait(kafka);
 
         // Make snapshots for Kafka cluster to make sure that there is no rolling update after CO reconciliation
@@ -254,14 +243,6 @@ void testJvmAndResources() {
         VerificationUtils.assertJvmOptions(testStorage.getNamespaceName(), brokerPodName, "kafka",
                 "-Xmx1g", "-Xms512m", "-XX:+UseG1GC");
 
-        if (!Environment.isKRaftModeEnabled()) {
-            LOGGER.info("Verifying resources and JVM configuration of ZooKeeper Broker Pod");
-            VerificationUtils.assertPodResourceRequests(testStorage.getNamespaceName(), KafkaResources.zookeeperPodName(testStorage.getClusterName(), 0), "zookeeper",
-                "1G", "500m", "500M", "25m");
-            VerificationUtils.assertJvmOptions(testStorage.getNamespaceName(), KafkaResources.zookeeperPodName(testStorage.getClusterName(), 0), "zookeeper",
-                "-Xmx1G", "-Xms512M", "-XX:+UseG1GC");
-        }
-
         LOGGER.info("Verifying resources, JVM configuration, and environment variables of Entity Operator's components");
 
         Optional<Pod> pod = kubeClient(testStorage.getNamespaceName()).listPods(testStorage.getNamespaceName())
@@ -301,9 +282,7 @@ void testJvmAndResources() {
         });
 
         LOGGER.info("Checking no rolling update for Kafka cluster");
-        if (!Environment.isKRaftModeEnabled()) {
-            RollingUpdateUtils.waitForNoRollingUpdate(testStorage.getNamespaceName(), testStorage.getControllerSelector(), controllerPods);
-        }
+        RollingUpdateUtils.waitForNoRollingUpdate(testStorage.getNamespaceName(), testStorage.getControllerSelector(), controllerPods);
         RollingUpdateUtils.waitForNoRollingUpdate(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), brokerPods);
         DeploymentUtils.waitForNoRollingUpdate(testStorage.getNamespaceName(), eoDepName, eoPods);
     }
@@ -549,7 +528,7 @@ void testRegenerateCertExternalAddressChange() {
     }
 
     @ParallelNamespaceTest
-    @SuppressWarnings({"checkstyle:JavaNCSS", "checkstyle:NPathComplexity", "checkstyle:MethodLength", "checkstyle:CyclomaticComplexity", "deprecation"}) // ZooKeeper is deprecated, but some methods from the API are still called here
+    @SuppressWarnings({"checkstyle:JavaNCSS", "checkstyle:NPathComplexity", "checkstyle:MethodLength", "checkstyle:CyclomaticComplexity"})
     @TestDoc(
         description = @Desc("This test case verifies the presence of expected Strimzi specific labels, also labels and annotations specified by user. Some user-specified labels are later modified (new one is added, one is modified) which triggers rolling update after which all changes took place as expected."),
         steps = {
@@ -643,19 +622,6 @@ void testLabelsExistenceAndManipulation() {
                 .endKafka()
             .endSpec();
 
-        // KRaft disabled we also use ZK with JBOD (otherwise we use controller...)
-        if (!Environment.isKRaftModeEnabled()) {
-            kafkaBuilder
-                .editSpec()
-                    .editZookeeper()
-                        .withNewTemplate()
-                            .withPersistentVolumeClaim(pvcResourceTemplate)
-                        .endTemplate()
-                        .withStorage(persistentClaimStorage)
-                    .endZookeeper()
-                .endSpec();
-        }
-
         resourceManager.createResourceWithWait(kafkaBuilder.build());
         resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(testStorage).build());
 
@@ -674,15 +640,13 @@ void testLabelsExistenceAndManipulation() {
 
         Map<String, String> kafkaLabelsObtained = StrimziPodSetUtils.getLabelsOfStrimziPodSet(testStorage.getNamespaceName(), testStorage.getBrokerComponentName());
 
-        LOGGER.info("Verifying labels of StrimziPodSet of Kafka resource");
+        LOGGER.info("Verifying labels of StrimziPodSet of Kafka resource - broker role");
         verifyAppLabels(kafkaLabelsObtained);
 
-        if (!Environment.isKRaftModeEnabled()) {
-            Map<String, String> zooLabels = StrimziPodSetUtils.getLabelsOfStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName());
+        kafkaLabelsObtained = StrimziPodSetUtils.getLabelsOfStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName());
 
-            LOGGER.info("Verifying labels of StrimziPodSet of ZooKeeper resource");
-            verifyAppLabels(zooLabels);
-        }
+        LOGGER.info("Verifying labels of StrimziPodSet of Kafka resource - controller role");
+        verifyAppLabels(kafkaLabelsObtained);
 
         LOGGER.info("---> SERVICES <---");
 
@@ -755,7 +719,7 @@ void testLabelsExistenceAndManipulation() {
 
         LOGGER.info("--> Test Customer specific labels manipulation (add, update) of Kafka CR and (update) PVC <--");
 
-        LOGGER.info("Take a snapshot of ZooKeeper and Kafka Pods in order to wait for their respawn after rollout");
+        LOGGER.info("Take a snapshot of Kafka Pods in order to wait for their respawn after rollout");
         Map<String, String> controllerPods = PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector());
         Map<String, String> brokerPods = PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getBrokerSelector());
 
@@ -773,41 +737,30 @@ void testLabelsExistenceAndManipulation() {
         LOGGER.info("New values of labels which are to modify label and annotation of PVC present in Kafka CR, with following values {}", customSpecifiedLabelOrAnnotationPvc);
 
         LOGGER.info("Edit Kafka labels in Kafka CR,as well as labels, and annotations of PVCs");
-        if (Environment.isKafkaNodePoolsEnabled()) {
-            KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), resource -> {
-                for (Map.Entry<String, String> label : customSpecifiedLabels.entrySet()) {
-                    resource.getMetadata().getLabels().put(label.getKey(), label.getValue());
-                }
-                resource.getSpec().getTemplate().getPersistentVolumeClaim().getMetadata().setLabels(customSpecifiedLabelOrAnnotationPvc);
-                resource.getSpec().getTemplate().getPersistentVolumeClaim().getMetadata().setAnnotations(customSpecifiedLabelOrAnnotationPvc);
-            });
 
-            if (Environment.isKRaftModeEnabled()) {
-                KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getControllerPoolName(), resource -> {
-                    for (Map.Entry<String, String> label : customSpecifiedLabels.entrySet()) {
-                        resource.getMetadata().getLabels().put(label.getKey(), label.getValue());
-                    }
-                    resource.getSpec().getTemplate().getPersistentVolumeClaim().getMetadata().setLabels(customSpecifiedLabelOrAnnotationPvc);
-                    resource.getSpec().getTemplate().getPersistentVolumeClaim().getMetadata().setAnnotations(customSpecifiedLabelOrAnnotationPvc);
-                });
+        KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), resource -> {
+            for (Map.Entry<String, String> label : customSpecifiedLabels.entrySet()) {
+                resource.getMetadata().getLabels().put(label.getKey(), label.getValue());
             }
-        }
+            resource.getSpec().getTemplate().getPersistentVolumeClaim().getMetadata().setLabels(customSpecifiedLabelOrAnnotationPvc);
+            resource.getSpec().getTemplate().getPersistentVolumeClaim().getMetadata().setAnnotations(customSpecifiedLabelOrAnnotationPvc);
+        });
 
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), resource -> {
+        KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getControllerPoolName(), resource -> {
             for (Map.Entry<String, String> label : customSpecifiedLabels.entrySet()) {
                 resource.getMetadata().getLabels().put(label.getKey(), label.getValue());
             }
-            resource.getSpec().getKafka().getTemplate().getPersistentVolumeClaim().getMetadata().setLabels(customSpecifiedLabelOrAnnotationPvc);
-            resource.getSpec().getKafka().getTemplate().getPersistentVolumeClaim().getMetadata().setAnnotations(customSpecifiedLabelOrAnnotationPvc);
+            resource.getSpec().getTemplate().getPersistentVolumeClaim().getMetadata().setLabels(customSpecifiedLabelOrAnnotationPvc);
+            resource.getSpec().getTemplate().getPersistentVolumeClaim().getMetadata().setAnnotations(customSpecifiedLabelOrAnnotationPvc);
+        });
 
-            // if KRaft disabled we can also configure ZK spec
-            if (!Environment.isKRaftModeEnabled()) {
-                resource.getSpec().getZookeeper().getTemplate().getPersistentVolumeClaim().getMetadata().setLabels(customSpecifiedLabelOrAnnotationPvc);
-                resource.getSpec().getZookeeper().getTemplate().getPersistentVolumeClaim().getMetadata().setAnnotations(customSpecifiedLabelOrAnnotationPvc);
+        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), resource -> {
+            for (Map.Entry<String, String> label : customSpecifiedLabels.entrySet()) {
+                resource.getMetadata().getLabels().put(label.getKey(), label.getValue());
             }
         });
 
-        LOGGER.info("Waiting for rolling update of ZooKeeper and Kafka");
+        LOGGER.info("Waiting for rolling update of Kafka");
         RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 1, controllerPods);
         RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
 
@@ -965,9 +918,9 @@ void testMessagesAndConsumerOffsetFilesOnDisk() {
     @ParallelNamespaceTest
     @Tag(CRUISE_CONTROL)
     @TestDoc(
-        description = @Desc("This test case verifies that Kafka (with all its components, including Zookeeper, Entity Operator, KafkaExporter, CruiseControl) configured with 'withReadOnlyRootFilesystem' can be deployed and also works correctly."),
+        description = @Desc("This test case verifies that Kafka (with all its components, including Entity Operator, KafkaExporter, CruiseControl) configured with 'withReadOnlyRootFilesystem' can be deployed and also works correctly."),
         steps = {
-            @Step(value = "Deploy persistent Kafka with 3 Kafka and Zookeeper replicas, Entity Operator, CruiseControl, and KafkaExporter. Each component has configuration 'withReadOnlyRootFilesystem' set to true.", expected = "Kafka and its components are deployed."),
+            @Step(value = "Deploy persistent Kafka with 3 replicas, Entity Operator, CruiseControl, and KafkaExporter. Each component has configuration 'withReadOnlyRootFilesystem' set to true.", expected = "Kafka and its components are deployed."),
             @Step(value = "Create Kafka producer and consumer.", expected = "Kafka clients are successfully created."),
             @Step(value = "Produce and consume messages using created clients.", expected = "Messages are successfully sent and received.")
         },
@@ -975,7 +928,6 @@ void testMessagesAndConsumerOffsetFilesOnDisk() {
             @Label(value = TestDocsLabels.KAFKA)
         }
     )
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
     void testReadOnlyRootFileSystem() {
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
@@ -988,13 +940,6 @@ void testReadOnlyRootFileSystem() {
                             .endKafkaContainer()
                         .endTemplate()
                     .endKafka()
-                    .editZookeeper()
-                        .withNewTemplate()
-                            .withNewZookeeperContainer()
-                                .withSecurityContext(new SecurityContextBuilder().withReadOnlyRootFilesystem(true).build())
-                            .endZookeeperContainer()
-                        .endTemplate()
-                    .endZookeeper()
                     .editEntityOperator()
                         .withNewTemplate()
                             .withNewTopicOperatorContainer()
@@ -1022,10 +967,6 @@ void testReadOnlyRootFileSystem() {
                 .endSpec()
                 .build();
 
-        if (Environment.isKRaftModeEnabled()) {
-            kafka.getSpec().setZookeeper(null);
-        }
-
         resourceManager.createResourceWithWait(
             NodePoolsConverter.convertNodePoolsIfNeeded(
                 KafkaNodePoolTemplates.brokerPool(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), testStorage.getClusterName(), 3)
@@ -1224,57 +1165,6 @@ void testResizeJbodVolumes() {
         // ##############################
     }
 
-    @ParallelNamespaceTest()
-    @TestDoc(
-        description = @Desc("This test case verifies basic working of Kafka Cluster managed by Cluster Operator with KRaft."),
-        steps = {
-            @Step(value = "Deploy Kafka annotated to enable KRaft (and additionally annotated to enable KafkaNodePool management), and configure a KafkaNodePool resource to target the Kafka cluster.", expected = "Kafka is deployed, and the KafkaNodePool resource targets the cluster as expected."),
-            @Step(value = "Produce and consume messages in given Kafka Cluster.", expected = "Clients can produce and consume messages."),
-            @Step(value = "Trigger manual Rolling Update.", expected = "Rolling update is triggered and completed shortly after.")
-        },
-        labels = {
-            @Label(value = TestDocsLabels.KAFKA)
-        }
-    )
-    void testKRaftMode() {
-        assumeTrue(Environment.isKRaftModeEnabled() && Environment.isKafkaNodePoolsEnabled());
-
-        final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
-        final int kafkaReplicas = 3;
-
-        resourceManager.createResourceWithWait(
-            KafkaNodePoolTemplates.brokerPoolPersistentStorage(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), testStorage.getClusterName(), kafkaReplicas).build(),
-            KafkaNodePoolTemplates.controllerPoolPersistentStorage(testStorage.getNamespaceName(), testStorage.getControllerPoolName(), testStorage.getClusterName(), kafkaReplicas).build(),
-            KafkaTemplates.kafkaPersistentKRaft(testStorage.getNamespaceName(), testStorage.getClusterName(), kafkaReplicas).build()
-        );
-
-        // Check that there is no ZooKeeper
-        Map<String, String> zkPods = PodUtils.podSnapshot(testStorage.getNamespaceName(),
-            KafkaResource.getLabelSelector(testStorage.getClusterName(), KafkaResources.zookeeperComponentName(testStorage.getClusterName())));
-        assertThat("No ZooKeeper Pods should exist", zkPods.size(), is(0));
-
-        // create KafkaTopic with replication factor on all brokers and min.insync replicas configuration to not loss data during Rolling Update.
-        resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(testStorage.getNamespaceName(), testStorage.getContinuousTopicName(), testStorage.getClusterName(), 1, kafkaReplicas, kafkaReplicas - 1).build());
-
-        KafkaClients clients = ClientUtils.getContinuousPlainClientBuilder(testStorage).build();
-        LOGGER.info("Producing and Consuming messages with continuous clients: {}, {} in Namespace {}", testStorage.getContinuousProducerName(), testStorage.getContinuousConsumerName(), testStorage.getNamespaceName());
-        resourceManager.createResourceWithWait(
-            clients.producerStrimzi(),
-            clients.consumerStrimzi()
-        );
-
-        // Roll Kafka
-        LOGGER.info("Forcing rolling update of Kafka via read-only configuration change");
-        final Map<String, String> brokerPods = PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getBrokerPoolSelector());
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> k.getSpec().getKafka().getConfig().put("log.retention.hours", 72));
-
-        LOGGER.info("Waiting for the next reconciliation to happen");
-        RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getBrokerPoolSelector(), kafkaReplicas, brokerPods);
-
-        LOGGER.info("Waiting for clients to finish sending/receiving messages");
-        ClientUtils.waitForContinuousClientSuccess(testStorage);
-    }
-
     @ParallelNamespaceTest
     @Tag(CONNECT)
     @Tag(BRIDGE)
@@ -1291,8 +1181,6 @@ void testKRaftMode() {
         }
     )
     void testAdditionalVolumes() {
-        assumeTrue(Environment.isKRaftModeEnabled());
-
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
         final int numberOfKafkaReplicas = 3;
         final String configMapName = "example-configmap";
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfSharedST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfSharedST.java
index 06a0abb3888..31d4e0d7967 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfSharedST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfSharedST.java
@@ -213,8 +213,6 @@ private static Map<String, Object> generateTestCases(String kafkaVersion) {
 
             // skipping these configuration exceptions
             testCases.remove("ssl.cipher.suites");
-            testCases.remove("zookeeper.connection.timeout.ms");
-            testCases.remove("zookeeper.connect");
         });
 
         return testCases;
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/kafka/listeners/ListenersST.java b/systemtest/src/test/java/io/strimzi/systemtest/kafka/listeners/ListenersST.java
index 7f5a44ce389..0533ffb8ff4 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/kafka/listeners/ListenersST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/kafka/listeners/ListenersST.java
@@ -2443,10 +2443,6 @@ void testNonExistingCustomCertificate() {
             .endSpec()
             .build());
 
-        if (!Environment.isKRaftModeEnabled()) {
-            PodUtils.waitForPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 1, true);
-        }
-
         KafkaUtils.waitUntilKafkaStatusConditionContainsMessage(testStorage.getNamespaceName(), testStorage.getClusterName(), ".*Secret " + nonExistingCertName + " with custom TLS certificate does not exist.*");
     }
 
@@ -2498,10 +2494,6 @@ void testCertificateWithNonExistingDataCrt() {
             .endSpec()
             .build());
 
-        if (!Environment.isKRaftModeEnabled()) {
-            PodUtils.waitForPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 1, true);
-        }
-
         KafkaUtils.waitUntilKafkaStatusConditionContainsMessage(testStorage.getNamespaceName(), testStorage.getClusterName(),
             ".*Secret " + clusterCustomCertServer1 + " does not contain certificate under the key " + nonExistingCertName + ".*");
     }
@@ -2554,10 +2546,6 @@ void testCertificateWithNonExistingDataKey() {
             .endSpec()
             .build());
 
-        if (!Environment.isKRaftModeEnabled()) {
-            PodUtils.waitForPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 1, true);
-        }
-
         KafkaUtils.waitUntilKafkaStatusConditionContainsMessage(testStorage.getNamespaceName(), testStorage.getClusterName(),
             ".*Secret " + clusterCustomCertServer1 + " does not contain custom certificate private key under the key " + nonExistingCertKey + ".*");
     }
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/log/LogSettingST.java b/systemtest/src/test/java/io/strimzi/systemtest/log/LogSettingST.java
index b9bcdc061e9..84f85a99f39 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/log/LogSettingST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/log/LogSettingST.java
@@ -102,7 +102,6 @@ class LogSettingST extends AbstractST {
         {
             put("kafka.root.logger.level", INFO);
             put("test.kafka.logger.level", INFO);
-            put("log4j.logger.org.apache.zookeeper", WARN);
             put("log4j.logger.kafka", TRACE);
             put("log4j.logger.org.apache.kafka", DEBUG);
             put("log4j.logger.kafka.request.logger", FATAL);
@@ -117,13 +116,6 @@ class LogSettingST extends AbstractST {
         }
     };
 
-    private static final Map<String, String> ZOOKEEPER_LOGGERS = new HashMap<>() {
-        {
-            put("zookeeper.root.logger", OFF);
-            put("test.zookeeper.logger.level", DEBUG);
-        }
-    };
-
     private static final Map<String, String> CONNECT_LOGGERS = new HashMap<>() {
         {
             put("connect.root.logger.level", INFO);
@@ -183,11 +175,9 @@ class LogSettingST extends AbstractST {
     };
 
     @IsolatedTest("Using shared Kafka")
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
     void testKafkaLogSetting() {
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
-        String zookeeperMap = KafkaResources.zookeeperMetricsAndLogConfigMapName(LOG_SETTING_CLUSTER_NAME);
         String topicOperatorMap = String.format("%s-%s", LOG_SETTING_CLUSTER_NAME, "entity-topic-operator-config");
         String userOperatorMap = String.format("%s-%s", LOG_SETTING_CLUSTER_NAME, "entity-user-operator-config");
 
@@ -205,60 +195,48 @@ void testKafkaLogSetting() {
         resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(Environment.TEST_SUITE_NAMESPACE, testStorage.getTopicName(), LOG_SETTING_CLUSTER_NAME).build());
         resourceManager.createResourceWithWait(KafkaUserTemplates.tlsUser(Environment.TEST_SUITE_NAMESPACE, testStorage.getKafkaUsername(), LOG_SETTING_CLUSTER_NAME).build());
 
-        LOGGER.info("Checking if Kafka, ZooKeeper, TO and UO of cluster: {} has log level set properly", LOG_SETTING_CLUSTER_NAME);
+        LOGGER.info("Checking if Kafka, TO and UO of cluster: {} has log level set properly", LOG_SETTING_CLUSTER_NAME);
         StUtils.getKafkaConfigurationConfigMaps(Environment.TEST_SUITE_NAMESPACE, LOG_SETTING_CLUSTER_NAME)
                 .forEach(cmName -> {
                     assertThat("Kafka's log level is set properly", checkLoggersLevel(Environment.TEST_SUITE_NAMESPACE, KAFKA_LOGGERS, cmName), is(true));
                 });
-        if (!Environment.isKRaftModeEnabled()) {
-            assertThat("ZooKeeper's log level is set properly", checkLoggersLevel(Environment.TEST_SUITE_NAMESPACE, ZOOKEEPER_LOGGERS, zookeeperMap), is(true));
-            assertThat("Topic Operator's log level is set properly", checkLoggersLevel(Environment.TEST_SUITE_NAMESPACE, OPERATORS_LOGGERS, topicOperatorMap), is(true));
-        }
+        assertThat("Topic Operator's log level is set properly", checkLoggersLevel(Environment.TEST_SUITE_NAMESPACE, OPERATORS_LOGGERS, topicOperatorMap), is(true));
         assertThat("User operator's log level is set properly", checkLoggersLevel(Environment.TEST_SUITE_NAMESPACE, OPERATORS_LOGGERS, userOperatorMap), is(true));
 
-        LOGGER.info("Checking if Kafka, ZooKeeper, TO and UO of cluster: {} has GC logging enabled in stateful sets/deployments", LOG_SETTING_CLUSTER_NAME);
+        LOGGER.info("Checking if Kafka, TO and UO of cluster: {} has GC logging enabled in StrimziPodSets/Deployments", LOG_SETTING_CLUSTER_NAME);
         checkGcLoggingPods(Environment.TEST_SUITE_NAMESPACE, brokerSelector, true);
-        if (!Environment.isKRaftModeEnabled()) {
-            checkGcLoggingPods(Environment.TEST_SUITE_NAMESPACE, controllerSelector, true);
-            assertThat("TO GC logging is enabled", checkGcLoggingDeployments(Environment.TEST_SUITE_NAMESPACE, eoDepName, "topic-operator"), is(true));
-        }
+        checkGcLoggingPods(Environment.TEST_SUITE_NAMESPACE, controllerSelector, true);
+
+        assertThat("TO GC logging is enabled", checkGcLoggingDeployments(Environment.TEST_SUITE_NAMESPACE, eoDepName, "topic-operator"), is(true));
         assertThat("UO GC logging is enabled", checkGcLoggingDeployments(Environment.TEST_SUITE_NAMESPACE, eoDepName, "user-operator"), is(true));
 
         LOGGER.info("Changing JVM options - setting GC logging to false");
-        if (Environment.isKafkaNodePoolsEnabled()) {
-            KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(Environment.TEST_SUITE_NAMESPACE, KafkaNodePoolResource.getBrokerPoolName(LOG_SETTING_CLUSTER_NAME),
-                knp -> knp.getSpec().setJvmOptions(JVM_OPTIONS));
-        }
+        KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(Environment.TEST_SUITE_NAMESPACE, KafkaNodePoolResource.getBrokerPoolName(LOG_SETTING_CLUSTER_NAME),
+            knp -> knp.getSpec().setJvmOptions(JVM_OPTIONS));
+        KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(Environment.TEST_SUITE_NAMESPACE, KafkaNodePoolResource.getControllerPoolName(LOG_SETTING_CLUSTER_NAME),
+            knp -> knp.getSpec().setJvmOptions(JVM_OPTIONS));
 
         KafkaResource.replaceKafkaResourceInSpecificNamespace(Environment.TEST_SUITE_NAMESPACE, LOG_SETTING_CLUSTER_NAME, kafka -> {
-            kafka.getSpec().getKafka().setJvmOptions(JVM_OPTIONS);
-            if (!Environment.isKRaftModeEnabled()) {
-                kafka.getSpec().getZookeeper().setJvmOptions(JVM_OPTIONS);
-                kafka.getSpec().getEntityOperator().getTopicOperator().setJvmOptions(JVM_OPTIONS);
-            }
+            kafka.getSpec().getEntityOperator().getTopicOperator().setJvmOptions(JVM_OPTIONS);
             kafka.getSpec().getEntityOperator().getUserOperator().setJvmOptions(JVM_OPTIONS);
         });
 
-        if (!Environment.isKRaftModeEnabled()) {
-            RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(Environment.TEST_SUITE_NAMESPACE, controllerSelector, 1, controllerPods);
-        }
+        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(Environment.TEST_SUITE_NAMESPACE, controllerSelector, 1, controllerPods);
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(Environment.TEST_SUITE_NAMESPACE, brokerSelector, 3, brokerPods);
         DeploymentUtils.waitTillDepHasRolled(Environment.TEST_SUITE_NAMESPACE, eoDepName, 1, eoPods);
 
-        LOGGER.info("Checking if Kafka, ZooKeeper, TO and UO of cluster: {} has GC logging disabled in stateful sets/deployments", LOG_SETTING_CLUSTER_NAME);
+        LOGGER.info("Checking if Kafka, TO and UO of cluster: {} has GC logging disabled in stateful sets/deployments", LOG_SETTING_CLUSTER_NAME);
         checkGcLoggingPods(Environment.TEST_SUITE_NAMESPACE, brokerSelector, false);
-        if (!Environment.isKRaftModeEnabled()) {
-            checkGcLoggingPods(Environment.TEST_SUITE_NAMESPACE, controllerSelector, false);
-            assertThat("TO GC logging is disabled", checkGcLoggingDeployments(Environment.TEST_SUITE_NAMESPACE, eoDepName, "topic-operator"), is(false));
-        }
+        checkGcLoggingPods(Environment.TEST_SUITE_NAMESPACE, controllerSelector, false);
+
+        assertThat("TO GC logging is disabled", checkGcLoggingDeployments(Environment.TEST_SUITE_NAMESPACE, eoDepName, "topic-operator"), is(false));
         assertThat("UO GC logging is disabled", checkGcLoggingDeployments(Environment.TEST_SUITE_NAMESPACE, eoDepName, "user-operator"), is(false));
 
-        LOGGER.info("Checking if Kafka, ZooKeeper, TO and UO of cluster: {} has GC logging disabled in stateful sets/deployments", GC_LOGGING_SET_NAME);
+        LOGGER.info("Checking if Kafka, TO and UO of cluster: {} has GC logging disabled in StrimziPodSets/Deployments", GC_LOGGING_SET_NAME);
         checkGcLoggingPods(Environment.TEST_SUITE_NAMESPACE, brokerSelector, false);
-        if (!Environment.isKRaftModeEnabled()) {
-            checkGcLoggingPods(Environment.TEST_SUITE_NAMESPACE, controllerSelector, false);
-            assertThat("TO GC logging is enabled", checkGcLoggingDeployments(Environment.TEST_SUITE_NAMESPACE, eoDepName, "topic-operator"), is(false));
-        }
+        checkGcLoggingPods(Environment.TEST_SUITE_NAMESPACE, controllerSelector, false);
+
+        assertThat("TO GC logging is enabled", checkGcLoggingDeployments(Environment.TEST_SUITE_NAMESPACE, eoDepName, "topic-operator"), is(false));
         assertThat("UO GC logging is enabled", checkGcLoggingDeployments(Environment.TEST_SUITE_NAMESPACE, eoDepName, "user-operator"), is(false));
 
         kubectlGetStrimziUntilOperationIsSuccessful(Environment.TEST_SUITE_NAMESPACE, LOG_SETTING_CLUSTER_NAME);
@@ -428,7 +406,7 @@ private synchronized void checkContainersHaveProcessOneAsTini(String namespaceNa
 
     private synchronized String configMap(String namespaceName, String configMapName) {
         Map<String, String> configMapData = kubeClient(namespaceName).getConfigMap(configMapName).getData();
-        // tries to get a log4j2 configuration file first (operator, bridge, ...) otherwise log4j one (kafka, zookeeper, ...)
+        // tries to get a log4j2 configuration file first (operator, bridge, ...) otherwise log4j one (kafka, ...)
         String configMapKey = configMapData.keySet()
                 .stream()
                 .filter(key -> key.equals("log4j2.properties") || key.equals("log4j.properties"))
@@ -489,7 +467,6 @@ private synchronized Boolean checkEnvVarValue(Container container) {
     }
 
     @BeforeAll
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
     void setup() {
         this.clusterOperator = this.clusterOperator
             .defaultInstallation()
@@ -537,14 +514,6 @@ void setup() {
                         .withGcLoggingEnabled(true)
                     .endJvmOptions()
                 .endKafka()
-                .editZookeeper()
-                    .withNewInlineLogging()
-                        .withLoggers(ZOOKEEPER_LOGGERS)
-                    .endInlineLogging()
-                    .withNewJvmOptions()
-                        .withGcLoggingEnabled(true)
-                    .endJvmOptions()
-                .endZookeeper()
                 .editEntityOperator()
                     .editOrNewUserOperator()
                         .withNewInlineLogging()
@@ -577,10 +546,6 @@ void setup() {
                     .withNewJvmOptions()
                     .endJvmOptions()
                 .endKafka()
-                .editZookeeper()
-                    .withNewJvmOptions()
-                    .endJvmOptions()
-                .endZookeeper()
                 .editEntityOperator()
                     .editTopicOperator()
                         .withNewJvmOptions()
@@ -594,10 +559,6 @@ void setup() {
             .endSpec()
             .build();
 
-        if (Environment.isKRaftModeEnabled()) {
-            logSettingKafka.getSpec().setZookeeper(null);
-            gcLoggingKafka.getSpec().setZookeeper(null);
-        }
         resourceManager.createResourceWithoutWait(logSettingKafka, gcLoggingKafka);
 
         // sync point wait for all resources
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java b/systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java
index 278dad678c9..ae4025d902a 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java
@@ -107,12 +107,11 @@ class LoggingChangeST extends AbstractST {
     private static final Pattern DEFAULT_LOG4J_PATTERN = Pattern.compile("^(?<date>[\\d-]+) (?<time>[\\d:,]+) (?<status>\\w+) (?<message>.+)");
 
     @ParallelNamespaceTest
-    @SuppressWarnings({"checkstyle:MethodLength", "deprecation"}) // ZooKeeper is deprecated, but some APi methods are still called here
     @TestDoc(
-        description = @Desc("Test verifying that the logging in JSON format works correctly across Kafka, Zookeeper, and operators."),
+        description = @Desc("Test verifying that the logging in JSON format works correctly across Kafka and operators."),
         steps = {
             @Step(value = "Assume non-Helm and non-OLM installation.", expected = "Assumption holds true."),
-            @Step(value = "Create ConfigMaps for Kafka, Zookeeper, and operators with JSON logging configuration.", expected = "ConfigMaps created and applied."),
+            @Step(value = "Create ConfigMaps for Kafka and operators with JSON logging configuration.", expected = "ConfigMaps created and applied."),
             @Step(value = "Deploy Kafka cluster with the configured logging setup.", expected = "Kafka cluster deployed successfully."),
             @Step(value = "Perform pod snapshot for controllers, brokers, and entity operators.", expected = "Pod snapshots successfully captured."),
             @Step(value = "Verify logs are in JSON format for all components.", expected = "Logs are in JSON format."),
@@ -133,7 +132,6 @@ void testJSONFormatLogging() {
             "log4j.appender.CONSOLE.layout=net.logstash.log4j.JSONEventLayoutV1\n" +
             "kafka.root.logger.level=INFO\n" +
             "log4j.rootLogger=${kafka.root.logger.level}, CONSOLE\n" +
-            "log4j.logger.org.apache.zookeeper=INFO\n" +
             "log4j.logger.kafka=INFO\n" +
             "log4j.logger.org.apache.kafka=INFO\n" +
             "log4j.logger.kafka.request.logger=WARN, CONSOLE\n" +
@@ -153,11 +151,6 @@ void testJSONFormatLogging() {
             "rootLogger.appenderRef.console.ref=STDOUT\n" +
             "rootLogger.additivity=false";
 
-        String loggersConfigZookeeper = "log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n" +
-            "log4j.appender.CONSOLE.layout=net.logstash.log4j.JSONEventLayoutV1\n" +
-            "zookeeper.root.logger=INFO\n" +
-            "log4j.rootLogger=${zookeeper.root.logger}, CONSOLE";
-
         String loggersConfigCO = "name = COConfig\n" +
             "appender.console.type = Console\n" +
             "appender.console.name = STDOUT\n" +
@@ -171,7 +164,6 @@ void testJSONFormatLogging() {
             "logger.kafka.additivity = false";
 
         String configMapOpName = "json-layout-operators";
-        String configMapZookeeperName = "json-layout-zookeeper";
         String configMapKafkaName = "json-layout-kafka";
         String configMapCOName = TestConstants.STRIMZI_DEPLOYMENT_NAME;
 
@@ -204,19 +196,6 @@ void testJSONFormatLogging() {
                 .withKey("log4j2.properties")
                 .build();
 
-        ConfigMap configMapZookeeper = new ConfigMapBuilder()
-            .withNewMetadata()
-                .withName(configMapZookeeperName)
-                .withNamespace(testStorage.getNamespaceName())
-            .endMetadata()
-            .addToData("log4j-custom.properties", loggersConfigZookeeper)
-            .build();
-
-        ConfigMapKeySelector zkLoggingCMselector = new ConfigMapKeySelectorBuilder()
-                .withName(configMapZookeeperName)
-                .withKey("log4j-custom.properties")
-                .build();
-
         ConfigMap configMapCO = new ConfigMapBuilder()
             .withNewMetadata()
                 .withName(configMapCOName)
@@ -228,7 +207,6 @@ void testJSONFormatLogging() {
 
         kubeClient().createConfigMapInNamespace(testStorage.getNamespaceName(), configMapKafka);
         kubeClient().createConfigMapInNamespace(testStorage.getNamespaceName(), configMapOperators);
-        kubeClient().createConfigMapInNamespace(testStorage.getNamespaceName(), configMapZookeeper);
         kubeClient().updateConfigMapInNamespace(clusterOperator.getDeploymentNamespace(), configMapCO);
 
         resourceManager.createResourceWithWait(
@@ -240,20 +218,12 @@ void testJSONFormatLogging() {
         Kafka kafka = KafkaTemplates.kafkaPersistent(testStorage.getNamespaceName(), testStorage.getClusterName(), 3, 3)
             .editOrNewSpec()
                 .editKafka()
-                    //.withLogging(new ExternalLoggingBuilder().withName(configMapKafkaName).build())
                     .withLogging(new ExternalLoggingBuilder()
                             .withNewValueFrom()
                                 .withConfigMapKeyRef(kafkaLoggingCMselector)
                             .endValueFrom()
                             .build())
                 .endKafka()
-                .editZookeeper()
-                    .withLogging(new ExternalLoggingBuilder()
-                            .withNewValueFrom()
-                                .withConfigMapKeyRef(zkLoggingCMselector)
-                            .endValueFrom()
-                            .build())
-                .endZookeeper()
                 .editEntityOperator()
                     .editTopicOperator()
                         .withLogging(new ExternalLoggingBuilder()
@@ -273,10 +243,6 @@ void testJSONFormatLogging() {
             .endSpec()
             .build();
 
-        if (Environment.isKRaftModeEnabled()) {
-            kafka.getSpec().setZookeeper(null);
-        }
-
         resourceManager.createResourceWithWait(kafka);
 
         Map<String, String> controllerPods = PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector());
@@ -696,12 +662,7 @@ void testDynamicallySetClusterOperatorLoggingLevels() {
             "    # Kafka AdminClient logging is a bit noisy at INFO level\n" +
             "    logger.kafka.name = org.apache.kafka\n" +
             "    logger.kafka.level = OFF\n" +
-            "    logger.kafka.additivity = false\n" +
-            "\n" +
-            "    # Zookeeper is very verbose even on INFO level -> We set it to WARN by default\n" +
-            "    logger.zookeepertrustmanager.name = org.apache.zookeeper\n" +
-            "    logger.zookeepertrustmanager.level = OFF\n" +
-            "    logger.zookeepertrustmanager.additivity = false";
+            "    logger.kafka.additivity = false";
 
         ConfigMap coMap = new ConfigMapBuilder()
             .withNewMetadata()
@@ -858,7 +819,6 @@ void testDynamicallyAndNonDynamicSetConnectLoggingLevels() {
             log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
             log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %X{connector.context}%m (%c) [%t]%n
             log4j.rootLogger=OFF, CONSOLE
-            log4j.logger.org.apache.zookeeper=ERROR
             log4j.logger.org.reflections=ERROR""";
 
         final String externalCmName = "external-cm";
@@ -955,7 +915,6 @@ void testDynamicallySetKafkaLoggingLevels() {
         InlineLogging ilOff = new InlineLogging();
         Map<String, String> log4jConfig = new HashMap<>();
         log4jConfig.put("kafka.root.logger.level", "OFF");
-        log4jConfig.put("log4j.logger.org.apache.zookeeper", "OFF");
         log4jConfig.put("log4j.logger.kafka", "OFF");
         log4jConfig.put("log4j.logger.org.apache.kafka", "OFF");
         log4jConfig.put("log4j.logger.kafka.request.logger", "OFF, CONSOLE");
@@ -1036,7 +995,6 @@ void testDynamicallySetKafkaLoggingLevels() {
                 "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n" +
                 "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\n" +
                 "log4j.rootLogger=INFO, CONSOLE\n" +
-                "log4j.logger.org.apache.zookeeper=INFO\n" +
                 "log4j.logger.kafka=INFO\n" +
                 "log4j.logger.org.apache.kafka=INFO\n" +
                 "log4j.logger.kafka.request.logger=WARN\n" +
@@ -1200,7 +1158,6 @@ void testDynamicallySetKafkaExternalLogging() {
                         "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n" +
                         "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\n" +
                         "log4j.rootLogger=INFO, CONSOLE\n" +
-                        "log4j.logger.org.apache.zookeeper=INFO\n" +
                         "log4j.logger.kafka=INFO\n" +
                         "log4j.logger.org.apache.kafka=INFO\n" +
                         "log4j.logger.kafka.request.logger=WARN\n" +
@@ -1259,7 +1216,6 @@ void testDynamicallySetKafkaExternalLogging() {
                         "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n" +
                         "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\n" +
                         "log4j.rootLogger=INFO, CONSOLE\n" +
-                        "log4j.logger.org.apache.zookeeper=ERROR\n" +
                         "log4j.logger.kafka=ERROR\n" +
                         "log4j.logger.org.apache.kafka=ERROR\n" +
                         "log4j.logger.kafka.request.logger=WARN\n" +
@@ -1290,7 +1246,6 @@ void testDynamicallySetKafkaExternalLogging() {
                         "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n" +
                         "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]\n" +
                         "log4j.rootLogger=INFO, CONSOLE\n" +
-                        "log4j.logger.org.apache.zookeeper=ERROR\n" +
                         "log4j.logger.kafka=ERROR\n" +
                         "log4j.logger.org.apache.kafka=ERROR\n" +
                         "log4j.logger.kafka.request.logger=WARN\n" +
@@ -1338,7 +1293,6 @@ void testDynamicallySetMM2LoggingLevels() {
         InlineLogging ilOff = new InlineLogging();
         Map<String, String> loggers = new HashMap<>();
         loggers.put("connect.root.logger.level", "OFF");
-        loggers.put("log4j.logger.org.apache.zookeeper", "OFF");
         loggers.put("log4j.logger.org.reflections", "OFF");
 
         ilOff.setLoggers(loggers);
@@ -1402,7 +1356,6 @@ void testDynamicallySetMM2LoggingLevels() {
                         "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n" +
                         "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %X{connector.context}%m (%c) [%t]%n\n" +
                         "log4j.rootLogger=OFF, CONSOLE\n" +
-                        "log4j.logger.org.apache.zookeeper=ERROR\n" +
                         "log4j.logger.org.reflections=ERROR";
 
         String externalCmName = "external-cm";
@@ -1490,7 +1443,6 @@ void testMM2LoggingLevelsHierarchy() {
                         "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n" +
                         "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\n" +
                         "log4j.rootLogger=OFF, CONSOLE\n" +
-                        "log4j.logger.org.apache.zookeeper=ERROR\n" +
                         "log4j.logger.org.eclipse.jetty.util.thread=FATAL\n" +
                         "log4j.logger.org.apache.kafka.connect.runtime.WorkerTask=OFF\n" +
                         "log4j.logger.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill=OFF\n" +
@@ -1542,7 +1494,6 @@ void testMM2LoggingLevelsHierarchy() {
                         "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n" +
                         "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\n" +
                         "log4j.rootLogger=INFO, CONSOLE\n" +
-                        "log4j.logger.org.apache.zookeeper=ERROR\n" +
                         "log4j.logger.org.eclipse.jetty.util.thread=WARN\n" +
                         "log4j.logger.org.reflections=ERROR";
 
@@ -1592,7 +1543,6 @@ void testNotExistingCMSetsDefaultLogging() {
             "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n" +
             "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\n" +
             "log4j.rootLogger=INFO, CONSOLE\n" +
-            "log4j.logger.org.apache.zookeeper=INFO\n" +
             "log4j.logger.kafka=INFO\n" +
             "log4j.logger.org.apache.kafka=INFO";
 
@@ -1779,7 +1729,6 @@ void testLoggingHierarchy() {
             @Label(value = TestDocsLabels.LOGGING)
         }
     )
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
     void testChangingInternalToExternalLoggingTriggerRollingUpdate() {
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
@@ -1793,17 +1742,13 @@ void testChangingInternalToExternalLoggingTriggerRollingUpdate() {
         resourceManager.createResourceWithWait(KafkaTemplates.kafkaEphemeral(testStorage.getNamespaceName(), testStorage.getClusterName(), 3, 3).build());
 
         Map<String, String> brokerPods = PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getBrokerSelector());
-        Map<String, String> controllerPods = null;
-        if (!Environment.isKRaftModeEnabled()) {
-            controllerPods = PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector());
-        }
+        Map<String, String> controllerPods = PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector());
 
         final String loggersConfig = "log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n" +
             "log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n" +
             "log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]\n" +
             "kafka.root.logger.level=INFO\n" +
             "log4j.rootLogger=${kafka.root.logger.level}, CONSOLE\n" +
-            "log4j.logger.org.apache.zookeeper=INFO\n" +
             "log4j.logger.kafka=INFO\n" +
             "log4j.logger.org.apache.kafka=INFO\n" +
             "log4j.logger.kafka.request.logger=WARN, CONSOLE\n" +
@@ -1837,31 +1782,21 @@ void testChangingInternalToExternalLoggingTriggerRollingUpdate() {
                 .withConfigMapKeyRef(log4jLoggimgCMselector)
                 .endValueFrom()
                 .build());
-            if (!Environment.isKRaftModeEnabled()) {
-                kafka.getSpec().getZookeeper().setLogging(new ExternalLoggingBuilder()
-                    .withNewValueFrom()
-                    .withConfigMapKeyRef(log4jLoggimgCMselector)
-                    .endValueFrom()
-                    .build());
-            }
         });
 
-        if (!Environment.isKRaftModeEnabled()) {
-            LOGGER.info("Waiting for Zookeeper pods to roll after change in logging");
-            controllerPods = RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
-        }
+        LOGGER.info("Waiting for controller pods to roll after change in logging");
+        controllerPods = RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
 
-        LOGGER.info("Waiting for Kafka pods to roll after change in logging");
+        LOGGER.info("Waiting for broker pods to roll after change in logging");
         brokerPods = RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
 
         configMapLoggers.getData().put("log4j-custom.properties", loggersConfig.replace("%p %m (%c) [%t]", "%p %m (%c) [%t]%n"));
         kubeClient().updateConfigMapInNamespace(testStorage.getNamespaceName(), configMapLoggers);
 
-        if (!Environment.isKRaftModeEnabled()) {
-            LOGGER.info("Waiting for Zookeeper pods to roll after change in logging properties config map");
-            RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
-        }
-        LOGGER.info("Waiting for Kafka pods to roll after change in logging properties config map");
+        LOGGER.info("Waiting for controller pods to roll after change in logging properties config map");
+        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
+
+        LOGGER.info("Waiting for broker pods to roll after change in logging properties config map");
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
     }
 
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/metrics/JmxST.java b/systemtest/src/test/java/io/strimzi/systemtest/metrics/JmxST.java
index 9211981be35..70e991670ba 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/metrics/JmxST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/metrics/JmxST.java
@@ -4,7 +4,6 @@
  */
 package io.strimzi.systemtest.metrics;
 
-import io.fabric8.kubernetes.api.model.Secret;
 import io.strimzi.api.kafka.model.common.jmx.KafkaJmxAuthenticationPassword;
 import io.strimzi.api.kafka.model.connect.KafkaConnect;
 import io.strimzi.api.kafka.model.connect.KafkaConnectResources;
@@ -30,7 +29,6 @@
 import org.junit.jupiter.api.BeforeAll;
 import org.junit.jupiter.api.Tag;
 
-import java.util.Arrays;
 import java.util.Collections;
 import java.util.Map;
 
@@ -41,7 +39,6 @@
 import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.Matchers.containsString;
-import static org.junit.jupiter.api.Assertions.assertTrue;
 
 @Tag(REGRESSION)
 public class JmxST extends AbstractST {
@@ -52,10 +49,8 @@ public class JmxST extends AbstractST {
     @Tag(CONNECT)
     @Tag(CONNECT_COMPONENTS)
     @FIPSNotSupported("JMX with auth is not working with FIPS")
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
-    void testKafkaZookeeperAndKafkaConnectWithJMX() {
+    void testKafkaAndKafkaConnectWithJMX() {
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
-        final String zkSecretName = testStorage.getClusterName() + "-zookeeper-jmx";
         final String connectJmxSecretName = testStorage.getClusterName() + "-kafka-connect-jmx";
         final String kafkaJmxSecretName = testStorage.getClusterName() + "-kafka-jmx";
 
@@ -74,11 +69,6 @@ void testKafkaZookeeperAndKafkaConnectWithJMX() {
                     .withNewJmxOptions()
                         .withAuthentication(new KafkaJmxAuthenticationPassword())
                     .endJmxOptions()
-                .endKafka()
-                .editOrNewZookeeper()
-                    .withNewJmxOptions()
-                        .withAuthentication(new KafkaJmxAuthenticationPassword())
-                    .endJmxOptions()
                     .editOrNewTemplate()
                         .withNewJmxSecret()
                             .withNewMetadata()
@@ -87,14 +77,10 @@ void testKafkaZookeeperAndKafkaConnectWithJMX() {
                             .endMetadata()
                         .endJmxSecret()
                     .endTemplate()
-                .endZookeeper()
+                .endKafka()
             .endSpec()
             .build();
 
-        if (Environment.isKRaftModeEnabled()) {
-            kafka.getSpec().setZookeeper(null);
-        }
-
         resourceManager.createResourceWithWait(kafka, ScraperTemplates.scraperPod(testStorage.getNamespaceName(), testStorage.getScraperName()).build());
         String scraperPodName = kubeClient().listPodsByPrefixInName(testStorage.getNamespaceName(), testStorage.getScraperName()).get(0).getMetadata().getName();
         JmxUtils.downloadJmxTermToPod(testStorage.getNamespaceName(), scraperPodName);
@@ -116,20 +102,6 @@ void testKafkaZookeeperAndKafkaConnectWithJMX() {
         String kafkaConnectResults = JmxUtils.collectJmxMetricsWithWait(testStorage.getNamespaceName(), KafkaConnectResources.serviceName(testStorage.getClusterName()), connectJmxSecretName, scraperPodName, "bean kafka.connect:type=app-info\nget -i *");
         assertThat("Result from Kafka JMX doesn't contain right version of Kafka, result: " + kafkaResults, kafkaResults, containsString("version = " + Environment.ST_KAFKA_VERSION));
         assertThat("Result from KafkaConnect JMX doesn't contain right version of Kafka, result: " + kafkaConnectResults, kafkaConnectResults, containsString("version = " + Environment.ST_KAFKA_VERSION));
-
-        if (!Environment.isKRaftModeEnabled()) {
-            Secret jmxZkSecret = kubeClient().getSecret(testStorage.getNamespaceName(), zkSecretName);
-
-            String zkBeans = JmxUtils.collectJmxMetricsWithWait(testStorage.getNamespaceName(), KafkaResources.zookeeperHeadlessServiceName(testStorage.getClusterName()), zkSecretName, scraperPodName, "domain org.apache.ZooKeeperService\nbeans");
-            String zkBean = Arrays.stream(zkBeans.split("\\n")).filter(bean -> bean.matches("org.apache.ZooKeeperService:name[0-9]+=ReplicatedServer_id[0-9]+")).findFirst().orElseThrow();
-
-            String zkResults = JmxUtils.collectJmxMetricsWithWait(testStorage.getNamespaceName(), KafkaResources.zookeeperHeadlessServiceName(testStorage.getClusterName()), zkSecretName, scraperPodName, "bean " + zkBean + "\nget -i *");
-            assertThat("Result from ZooKeeper JMX doesn't contain right quorum size, result: " + zkResults, zkResults, containsString("QuorumSize = 3"));
-
-            LOGGER.info("Checking that ZooKeeper JMX Secret is created with custom labels and annotations");
-            assertTrue(jmxZkSecret.getMetadata().getLabels().entrySet().containsAll(jmxSecretLabels.entrySet()));
-            assertTrue(jmxZkSecret.getMetadata().getAnnotations().entrySet().containsAll(jmxSecretAnnotations.entrySet()));
-        }
     }
 
     @BeforeAll
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/metrics/MetricsST.java b/systemtest/src/test/java/io/strimzi/systemtest/metrics/MetricsST.java
index ec836453582..4a47c2de007 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/metrics/MetricsST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/metrics/MetricsST.java
@@ -29,7 +29,6 @@
 import io.strimzi.systemtest.Environment;
 import io.strimzi.systemtest.TestConstants;
 import io.strimzi.systemtest.annotations.IsolatedTest;
-import io.strimzi.systemtest.annotations.KRaftNotSupported;
 import io.strimzi.systemtest.annotations.ParallelTest;
 import io.strimzi.systemtest.kafkaclients.internalClients.BridgeClients;
 import io.strimzi.systemtest.kafkaclients.internalClients.BridgeClientsBuilder;
@@ -133,7 +132,7 @@
  *     - All KafkaUsers and KafkaTopics are Ready
  *  7. - Setup NetworkPolicies to grant access to Operator Pods and KafkaExporter
  *     - NetworkPolicies created
- *  8. - Create collectors for Cluster Operator, Kafka, KafkaExporter, and Zookeeper (Non-KRaft)
+ *  8. - Create collectors for Cluster Operator, Kafka, and KafkaExporter
  *     - Metrics collected in collectors structs
  *
  * @afterAll
@@ -158,16 +157,13 @@ public class MetricsST extends AbstractST {
     private final String bridgeClusterName = "my-bridge";
 
     private String coScraperPodName;
-    private String testSuiteScraperPodName;
     private String scraperPodName;
-    private String secondNamespaceScraperPodName;
 
     private String bridgeTopicName = KafkaTopicUtils.generateRandomNameOfTopic();
     private String topicName = KafkaTopicUtils.generateRandomNameOfTopic();
     private final String kafkaExporterTopicName = KafkaTopicUtils.generateRandomNameOfTopic();
 
     private BaseMetricsCollector kafkaCollector;
-    private BaseMetricsCollector zookeeperCollector;
     private BaseMetricsCollector kafkaExporterCollector;
     private BaseMetricsCollector clusterOperatorCollector;
 
@@ -190,25 +186,6 @@ void testKafkaMetrics() {
         assertMetricValue(kafkaCollector, "kafka_server_replicamanager_underreplicatedpartitions", 0);
     }
 
-    /**
-     * @description This test case check several random metrics exposed by Zookeeper.
-     *
-     * @steps
-     *  1. - Check if specific metric is available in collected metrics from Zookeeper Pods
-     *     - Metric is available with expected value
-     *
-     * @usecase
-     *  - metrics
-     *  - zookeeper-metrics
-     */
-    @ParallelTest
-    @KRaftNotSupported("ZooKeeper is not supported by KRaft mode and is used in this test case")
-    void testZookeeperMetrics() {
-        assertMetricValueNotNull(zookeeperCollector, "zookeeper_quorumsize");
-        assertMetricCountHigherThan(zookeeperCollector, "zookeeper_numaliveconnections\\{.*\\}", 0L);
-        assertMetricCountHigherThan(zookeeperCollector, "zookeeper_inmemorydatatree_watchcount\\{.*\\}", 0L);
-    }
-
     /**
      * @description This test case check several random metrics exposed by Kafka Connect.
      *
@@ -308,12 +285,6 @@ void testKafkaExporterMetrics() {
 
         assertMetricValueNotNull(kafkaExporterCollector, "kafka_consumergroup_current_offset\\{.*\\}");
 
-        if (!Environment.isKRaftModeEnabled()) {
-            Pattern pattern = Pattern.compile("kafka_topic_partitions\\{topic=\"" + kafkaExporterTopicName + "\"} ([\\d])");
-            List<Double> values = kafkaExporterCollector.waitForSpecificMetricAndCollect(pattern);
-            assertThat(String.format("metric %s doesn't contain correct value", pattern), values.stream().mapToDouble(i -> i).sum(), is(7.0));
-        }
-
         kubeClient().listPods(namespaceFirst, brokerPodsSelector).forEach(pod -> {
             String address = pod.getMetadata().getName() + "." + kafkaClusterFirstName + "-kafka-brokers." + namespaceFirst + ".svc";
             Pattern pattern = Pattern.compile("kafka_broker_info\\{address=\"" + address + ".*\",.*} ([\\d])");
@@ -749,9 +720,7 @@ void setupEnvironment() throws Exception {
         resourceManager.createResourceWithWait(KafkaUserTemplates.tlsUser(namespaceFirst, KafkaUserUtils.generateRandomNameOfKafkaUser(), kafkaClusterFirstName).build());
 
         coScraperPodName = ResourceManager.kubeClient().listPodsByPrefixInName(TestConstants.CO_NAMESPACE, coScraperName).get(0).getMetadata().getName();
-        testSuiteScraperPodName = ResourceManager.kubeClient().listPodsByPrefixInName(Environment.TEST_SUITE_NAMESPACE, testSuiteScraperName).get(0).getMetadata().getName();
         scraperPodName = ResourceManager.kubeClient().listPodsByPrefixInName(namespaceFirst, scraperName).get(0).getMetadata().getName();
-        secondNamespaceScraperPodName = ResourceManager.kubeClient().listPodsByPrefixInName(namespaceSecond, secondScraperName).get(0).getMetadata().getName();
 
         // Allow connections from clients to operators pods when NetworkPolicies are set to denied by default
         NetworkPolicyResource.allowNetworkPolicySettingsForClusterOperator(TestConstants.CO_NAMESPACE);
@@ -767,13 +736,6 @@ void setupEnvironment() throws Exception {
             .withComponent(KafkaMetricsComponent.create(kafkaClusterFirstName))
             .build();
 
-        if (!Environment.isKRaftModeEnabled()) {
-            zookeeperCollector = kafkaCollector.toBuilder()
-                .withComponent(ZookeeperMetricsComponent.create(kafkaClusterFirstName))
-                .build();
-            zookeeperCollector.collectMetricsFromPods(TestConstants.METRICS_COLLECT_TIMEOUT);
-        }
-
         kafkaExporterCollector = kafkaCollector.toBuilder()
             .withComponent(KafkaExporterMetricsComponent.create(namespaceFirst, kafkaClusterFirstName))
             .build();
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/migration/MigrationST.java b/systemtest/src/test/java/io/strimzi/systemtest/migration/MigrationST.java
deleted file mode 100644
index 690a201dfcc..00000000000
--- a/systemtest/src/test/java/io/strimzi/systemtest/migration/MigrationST.java
+++ /dev/null
@@ -1,797 +0,0 @@
-/*
- * Copyright Strimzi authors.
- * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).
- */
-package io.strimzi.systemtest.migration;
-
-import io.fabric8.kubernetes.api.model.LabelSelector;
-import io.fabric8.kubernetes.api.model.PersistentVolumeClaim;
-import io.fabric8.kubernetes.api.model.Pod;
-import io.fabric8.kubernetes.api.model.Quantity;
-import io.fabric8.kubernetes.api.model.ResourceRequirementsBuilder;
-import io.strimzi.api.kafka.model.kafka.JbodStorageBuilder;
-import io.strimzi.api.kafka.model.kafka.KRaftMetadataStorage;
-import io.strimzi.api.kafka.model.kafka.KafkaMetadataState;
-import io.strimzi.api.kafka.model.kafka.KafkaResources;
-import io.strimzi.api.kafka.model.kafka.PersistentClaimStorage;
-import io.strimzi.api.kafka.model.kafka.PersistentClaimStorageBuilder;
-import io.strimzi.api.kafka.model.kafka.Storage;
-import io.strimzi.api.kafka.model.kafka.listener.GenericKafkaListenerBuilder;
-import io.strimzi.api.kafka.model.kafka.listener.KafkaListenerType;
-import io.strimzi.api.kafka.model.nodepool.KafkaNodePool;
-import io.strimzi.api.kafka.model.nodepool.ProcessRoles;
-import io.strimzi.api.kafka.model.user.acl.AclOperation;
-import io.strimzi.api.kafka.model.user.acl.AclResourcePatternType;
-import io.strimzi.operator.common.Annotations;
-import io.strimzi.operator.common.model.Labels;
-import io.strimzi.systemtest.AbstractST;
-import io.strimzi.systemtest.Environment;
-import io.strimzi.systemtest.TestConstants;
-import io.strimzi.systemtest.annotations.IsolatedTest;
-import io.strimzi.systemtest.kafkaclients.internalClients.KafkaClients;
-import io.strimzi.systemtest.kafkaclients.internalClients.KafkaClientsBuilder;
-import io.strimzi.systemtest.resources.ResourceManager;
-import io.strimzi.systemtest.resources.crd.KafkaNodePoolResource;
-import io.strimzi.systemtest.resources.crd.KafkaResource;
-import io.strimzi.systemtest.storage.TestStorage;
-import io.strimzi.systemtest.templates.crd.KafkaNodePoolTemplates;
-import io.strimzi.systemtest.templates.crd.KafkaTemplates;
-import io.strimzi.systemtest.templates.crd.KafkaTopicTemplates;
-import io.strimzi.systemtest.templates.crd.KafkaUserTemplates;
-import io.strimzi.systemtest.utils.ClientUtils;
-import io.strimzi.systemtest.utils.RollingUpdateUtils;
-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;
-import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;
-import io.strimzi.test.TestUtils;
-import io.strimzi.test.k8s.KubeClusterResource;
-import io.strimzi.test.k8s.exceptions.KubeClusterException;
-import org.apache.logging.log4j.Level;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
-import org.junit.jupiter.api.BeforeAll;
-import org.junit.jupiter.api.Tag;
-import org.junit.jupiter.params.ParameterizedTest;
-import org.junit.jupiter.params.provider.Arguments;
-import org.junit.jupiter.params.provider.MethodSource;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-import java.util.stream.Stream;
-
-import static io.strimzi.systemtest.TestTags.MIGRATION;
-import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;
-import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;
-import static org.hamcrest.CoreMatchers.is;
-import static org.hamcrest.MatcherAssert.assertThat;
-import static org.junit.jupiter.api.Assumptions.assumeTrue;
-
-@Tag(MIGRATION)
-public class MigrationST extends AbstractST {
-
-    private static final Logger LOGGER = LogManager.getLogger(MigrationST.class);
-    private static final String CONTINUOUS_SUFFIX = "-continuous";
-    private static final int METADATA_VOLUME_ID = 0;
-    private static final String METADATA_FOLDER_NAME_JBOD = "data-" + METADATA_VOLUME_ID;
-    private static final String METADATA_FOLDER_NAME = "data";
-    private String metadataFolderName;
-    private KafkaClients immediateClients;
-    private KafkaClients continuousClients;
-    private String postMigrationTopicName;
-    private String kraftTopicName;
-    private LabelSelector brokerSelector;
-    private LabelSelector controllerSelector;
-    private Map<String, String> brokerPodsSnapshot;
-    private Map<String, String> controllerPodsSnapshot;
-
-    /**
-     * @description This testcase is focused on migration process from ZK to KRaft.
-     * It goes through whole process, together with checking that message transmission throughout the test will not be
-     * disrupted.
-     *
-     * This is a parametrized test with following arguments:
-     * <ul>
-     *     <li>$1 - Delete Zookeeper claim: true, false</li>
-     *     <li>$2 - Delete Cluster Operator during the process: false, true</li>
-     *     <li>$3 - Use JBOD storage configuration: true, false</li>
-     * </ul>
-     *
-     * Based on the checks, the steps can be a bit different.
-     *
-     * @steps
-     *  1. - Deploys Kafka resource (with enabled NodePools and KRaft set to disabled) with Broker NodePool (with particular storage - JBOD/single PV)
-     *  2. - Creates topics for continuous and immediate message transmission, TLS user
-     *  3. - Starts continuous producer & consumer
-     *  4. - Does immediate message transmission
-     *  5. - Starts the migration
-     *  6. - Creates Controller NodePool (with particular storage - JBOD/single PV) - the Pods will not be created until the migration starts (after we apply the migration annotation)
-     *  7. - Annotates the Kafka resource with strimzi.io/kraft:migration
-     *  8. - Controllers will be created and moved to RUNNING state
-     *  9. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftMigration
-     *  ------------------------------- delete CO during process is set to true -------------------------------
-     *  10. - Waits for one of the broker Pods to start rolling update
-     *  11. - Deletes the ClusterOperator Pod
-     *  -------------------------------------------------------------------------------------------------------
-     *  12. - Waits for first rolling update of Broker Pods to be finished - bringing the Brokers to DualWrite mode
-     *  13. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftDualWriting
-     *  ------------------------------- delete CO during process is set to true -------------------------------
-     *  14. - Waits for one of the broker Pods to start rolling update
-     *  15. - Deletes the ClusterOperator Pod
-     *  -------------------------------------------------------------------------------------------------------
-     *  16. - Waits for second rolling update of Broker Pods to be finished - removing dependency on ZK
-     *  17. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftPostMigration
-     *  18. - Creates a new KafkaTopic and checks both ZK and KRaft metadata for presence of the KafkaTopic
-     *  19. - Does immediate message transmission to the new KafkaTopic
-     *  20. - Finishes the migration - annotates the Kafka resource with strimzi.io/kraft:enabled
-     *  ------------------------------- delete CO during process is set to true -------------------------------
-     *  21. - Waits for one of the controller Pods to start rolling update
-     *  22. - Deletes the ClusterOperator Pod
-     *  -------------------------------------------------------------------------------------------------------
-     *  23. - Waits for rolling update of Controller Pods will be finished
-     *  24. - ZK related resources will be deleted (except of the PVCs)
-     *  25. - Check that ZK PVCs are not deleted
-     *  26. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaft
-     *  27. - Removes LMFV and IBPV from Kafka configuration
-     *  28. - Broker and Controller Pods will be rolled
-     *  29. - Creates a new KafkaTopic and checks KRaft metadata for presence of the KafkaTopic
-     *  30. - Does immediate message transmission to the new KafkaTopic
-     *  31. - Waits until continuous clients are finished successfully
-     *
-     * @usecase
-     *  - zk-to-kraft-migration
-     */
-    @ParameterizedTest(name = "with: ZK delete claim - {0}, CO deletion during process - {1}, JBOD storage - {2}")
-    @MethodSource("getArgumentsForMigrationParametrizedTests")
-    void testMigration(Boolean zkDeleteClaim, Boolean deleteCoDuringProcess, Boolean withJbodStorage) {
-        final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
-
-        setupMigrationTestCase(testStorage, zkDeleteClaim, withJbodStorage);
-        doFirstPartOfMigration(testStorage, deleteCoDuringProcess, withJbodStorage);
-        doSecondPartOfMigration(testStorage, deleteCoDuringProcess, zkDeleteClaim);
-    }
-
-    /**
-     * @description This testcase is focused on rollback process after first part of the migration from ZK to KRaft is done.
-     * It goes through whole process, together with checking that message transmission throughout the test will not be
-     * disrupted.
-     *
-     * This is a parametrized test with following arguments:
-     * <ul>
-     *     <li>$1 - Delete Cluster Operator during the process: false, true</li>
-     *     <li>$2 - Use JBOD storage configuration: true, false</li>
-     * </ul>
-     *
-     * Based on the checks, the steps can be a bit different.
-     * @steps
-     *  1. - Deploys Kafka resource (with enabled NodePools and KRaft set to disabled) with Broker NodePool (with particular storage - JBOD/single PV)
-     *  2. - Creates topics for continuous and immediate message transmission, TLS user
-     *  3. - Starts continuous producer & consumer
-     *  4. - Does immediate message transmission
-     *  5. - Starts the migration
-     *  6. - Creates Controller NodePool (with particular storage - JBOD/single PV) - the Pods will not be created until the migration starts (after we apply the migration annotation)
-     *  7. - Annotates the Kafka resource with strimzi.io/kraft:migration
-     *  8. - Controllers will be created and moved to RUNNING state
-     *  9. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftMigration
-     *  ------------------------------- delete CO during process is set to true -------------------------------
-     *  10. - Waits for one of the Broker Pods to start rolling update
-     *  11. - Deletes the ClusterOperator Pod
-     *  -------------------------------------------------------------------------------------------------------
-     *  12. - Waits for first rolling update of Broker Pods to be finished - bringing the Brokers to DualWrite mode
-     *  13. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftDualWriting
-     *  ------------------------------- delete CO during process is set to true -------------------------------
-     *  14. - Waits for one of the Broker Pods to start rolling update
-     *  15. - Deletes the ClusterOperator Pod
-     *  -------------------------------------------------------------------------------------------------------
-     *  16. - Waits for second rolling update of Broker Pods to be finished - removing dependency on ZK
-     *  17. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftPostMigration
-     *  18. - Creates a new KafkaTopic and checks both ZK and KRaft metadata for presence of the KafkaTopic
-     *  19. - Does immediate message transmission to the new KafkaTopic
-     *  16. - Rolling back the migration - annotates the Kafka resource with strimzi.io/kraft:rollback
-     *  ------------------------------- delete CO during process is set to true -------------------------------
-     *  17. - Waits for one of the Broker Pods to start rolling update
-     *  18. - Deletes the ClusterOperator Pod
-     *  -------------------------------------------------------------------------------------------------------
-     *  19. - Waits for rolling update of Broker Pods - adding dependency on ZK back
-     *  20. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftDualWriting
-     *  19. - Deletes the Controller NodePool
-     *  20. - Finishes rollback - annotates the Kafka resource with strimzi.io/kraft:disabled
-     *  ------------------------------- delete CO during process is set to true -------------------------------
-     *  21. - Waits for one of the Broker Pods to start rolling update
-     *  22. - Deletes the ClusterOperator Pod
-     *  -------------------------------------------------------------------------------------------------------
-     *  23. - Waits for rolling update of Broker Pods - rolling back from DualWrite mode to ZooKeeper
-     *  24. - Checks that Kafka CR has .status.kafkaMetadataState set to ZooKeeper
-     *  25. - Checks that __cluster_metadata topic doesn't exist in Kafka Brokers
-     *  26. - Waits until continuous clients are finished successfully
-     *
-     * @usecase
-     *  - zk-to-kraft-migration
-     */
-    @ParameterizedTest(name = "with: CO deletion during process - {0}, JBOD storage - {1}")
-    @MethodSource("getArgumentsForRollbackParametrizedTests")
-    void testRollback(Boolean deleteCoDuringProcess, Boolean withJbodStorage) {
-        final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
-
-        setupMigrationTestCase(testStorage, true, withJbodStorage);
-        doFirstPartOfMigration(testStorage, deleteCoDuringProcess, withJbodStorage);
-        doRollback(testStorage, deleteCoDuringProcess);
-    }
-
-    /**
-     * @description This testcase is focused on full circle of the migration process. The test starts with the migration,
-     * then, after the first phase, the migration is rolled back to the KRaftDualWriting (half-way of rollback). After that
-     * the `strimzi.io/kraft:migration` annotation is applied again and we are waiting for the movement to KRaftPostMigration state, and
-     * continue with the migration to the end. The test-case should verify that even if the migration is rolled back to KRaftDualWriting phase, it can
-     * be continued again without issues.
-     *
-     * @steps
-     *  1. - Deploys Kafka resource (with enabled NodePools and KRaft set to disabled) with Broker NodePool
-     *  2. - Creates topics for continuous and immediate message transmission, TLS user
-     *  3. - Starts continuous producer & consumer
-     *  4. - Does immediate message transmission
-     *  5. - Starts the migration
-     *  6. - Creates Controller NodePool - the Pods will not be created until the migration starts (after we apply the migration annotation)
-     *  7. - Annotates the Kafka resource with strimzi.io/kraft:migration
-     *  8. - Controllers will be created and moved to RUNNING state
-     *  9. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftMigration
-     *  10. - Waits for first rolling update of Broker Pods - bringing the Brokers to DualWrite mode
-     *  11. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftDualWriting
-     *  12. - Waits for second rolling update of Broker Pods - removing dependency on ZK
-     *  13. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftPostMigration
-     *  14. - Creates a new KafkaTopic and checks both ZK and KRaft metadata for presence of the KafkaTopic
-     *  15. - Does immediate message transmission to the new KafkaTopic
-     *  --------------------------------------------------------------------------------------------------------------------
-     *  16. - Rolling back the migration to KRaftDualWriting - annotates the Kafka resource with strimzi.io/kraft:rollback
-     *  17. - Waits for rolling update of Broker Pods - adding dependency on ZK back
-     *  18. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftDualWriting
-     *  --------------------------------------------------------------------------------------------------------------------
-     *  19. - Continuing with migration - annotates the Kafka resource with strimzi.io/kraft:migration
-     *  20. - Waits for rolling update of Broker Pods - removing dependency on ZK
-     *  21. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaftPostMigration
-     *  22. - Creates a new KafkaTopic and checks both ZK and KRaft metadata for presence of the KafkaTopic
-     *  23. - Does immediate message transmission to the new KafkaTopic
-     *  24. - Finishes the migration - annotates the Kafka resource with strimzi.io/kraft:enabled
-     *  25. - Controller Pods will be rolled
-     *  26. - ZK related resources will be deleted
-     *  27. - Checks that Kafka CR has .status.kafkaMetadataState set to KRaft
-     *  28. - Removes LMFV and IBPV from Kafka configuration
-     *  29. - Broker and Controller Pods will be rolled
-     *  30. - Creates a new KafkaTopic and checks KRaft metadata for presence of the KafkaTopic
-     *  31. - Does immediate message transmission to the new KafkaTopic
-     *  32. - Waits until continuous clients are finished successfully
-     *
-     * @usecase
-     *  - zk-to-kraft-migration
-     */
-    @IsolatedTest
-    void testMigrationWithRollback() {
-        final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
-        final boolean zkDeleteClaim = true;
-        final boolean deleteCoDuringProcess = false;
-        final boolean withJbodStorage = false;
-
-        setupMigrationTestCase(testStorage, zkDeleteClaim, withJbodStorage);
-        doFirstPartOfMigration(testStorage, deleteCoDuringProcess, withJbodStorage);
-        doFirstPartOfRollback(testStorage, deleteCoDuringProcess);
-
-        LOGGER.info("Applying the {} annotation with value: {}", Annotations.ANNO_STRIMZI_IO_KRAFT, "migration");
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> kafka.getMetadata().getAnnotations().put(Annotations.ANNO_STRIMZI_IO_KRAFT, "migration")
-        );
-
-        moveToKRaftPostMigrationState(testStorage, postMigrationTopicName + "-2");
-        doSecondPartOfMigration(testStorage, deleteCoDuringProcess, zkDeleteClaim);
-    }
-
-    @SuppressWarnings({"checkstyle:MethodLength"})
-    private void setupMigrationTestCase(TestStorage testStorage, boolean zkDeleteClaim, boolean withJbodStorage) {
-        // we assume that users will have broker NodePool named "kafka", so we will name it completely same to follow this use-case
-        String brokerPoolName = "kafka";
-
-        metadataFolderName = withJbodStorage ? METADATA_FOLDER_NAME_JBOD : METADATA_FOLDER_NAME;
-        postMigrationTopicName = testStorage.getTopicName() + "-post-migration";
-        kraftTopicName = testStorage.getTopicName() + "-kraft";
-        String immediateConsumerGroup = ClientUtils.generateRandomConsumerGroup();
-
-        String continuousTopicName = testStorage.getTopicName() + CONTINUOUS_SUFFIX;
-        String continuousProducerName = testStorage.getProducerName() + CONTINUOUS_SUFFIX;
-        String continuousConsumerName = testStorage.getConsumerName() + CONTINUOUS_SUFFIX;
-        String continuousUserName = testStorage.getUsername() + CONTINUOUS_SUFFIX;
-        String continuousConsumerGroupName = ClientUtils.generateRandomConsumerGroup();
-        int continuousMessageCount = 500;
-
-        brokerSelector = KafkaNodePoolResource.getLabelSelector(testStorage.getClusterName(), brokerPoolName, ProcessRoles.BROKER);
-        controllerSelector = KafkaNodePoolResource.getLabelSelector(testStorage.getClusterName(), testStorage.getControllerPoolName(), ProcessRoles.CONTROLLER);
-
-        String clientsAdditionConfiguration = "delivery.timeout.ms=40000\nrequest.timeout.ms=5000\nacks=all\n";
-
-        immediateClients = new KafkaClientsBuilder()
-            .withNamespaceName(testStorage.getNamespaceName())
-            .withProducerName(testStorage.getProducerName())
-            .withConsumerName(testStorage.getConsumerName())
-            .withBootstrapAddress(KafkaResources.tlsBootstrapAddress(testStorage.getClusterName()))
-            .withTopicName(testStorage.getTopicName())
-            .withMessageCount(testStorage.getMessageCount())
-            .withUsername(testStorage.getUsername())
-            .withConsumerGroup(immediateConsumerGroup)
-            .build();
-
-        continuousClients = new KafkaClientsBuilder()
-            .withNamespaceName(testStorage.getNamespaceName())
-            .withProducerName(continuousProducerName)
-            .withConsumerName(continuousConsumerName)
-            .withBootstrapAddress(KafkaResources.plainBootstrapAddress(testStorage.getClusterName()))
-            .withTopicName(continuousTopicName)
-            .withMessageCount(continuousMessageCount)
-            .withDelayMs(500)
-            .withConsumerGroup(continuousConsumerGroupName)
-            .withUsername(continuousUserName)
-            .withAdditionalConfig(clientsAdditionConfiguration)
-            .build();
-
-        LOGGER.info("Deploying Kafka resource with Broker NodePool");
-
-        // create Kafka resource with ZK and Broker NodePool
-        resourceManager.createResourceWithWait(
-                KafkaNodePoolTemplates.brokerPoolPersistentStorage(testStorage.getNamespaceName(), brokerPoolName, testStorage.getClusterName(), 3)
-                    .editSpec()
-                        .withStorage(getStorageBasedOnTestCase(withJbodStorage))
-                    .endSpec()
-                    .build(),
-                KafkaTemplates.kafkaPersistentNodePools(testStorage.getNamespaceName(), testStorage.getClusterName(), 3, 3)
-                    .editMetadata()
-                        .addToAnnotations(Annotations.ANNO_STRIMZI_IO_NODE_POOLS, "enabled")
-                        .addToAnnotations(Annotations.ANNO_STRIMZI_IO_KRAFT, "disabled")
-                    .endMetadata()
-                    .editSpec()
-                        .editOrNewKafka()
-                            .withListeners(
-                                new GenericKafkaListenerBuilder()
-                                    .withName(TestConstants.PLAIN_LISTENER_DEFAULT_NAME)
-                                    .withPort(9092)
-                                    .withType(KafkaListenerType.INTERNAL)
-                                    .withTls(false)
-                                    .withNewKafkaListenerAuthenticationScramSha512Auth()
-                                    .endKafkaListenerAuthenticationScramSha512Auth()
-                                    .build(),
-                                new GenericKafkaListenerBuilder()
-                                    .withName(TestConstants.TLS_LISTENER_DEFAULT_NAME)
-                                    .withPort(9093)
-                                    .withType(KafkaListenerType.INTERNAL)
-                                    .withTls(true)
-                                    .withNewKafkaListenerAuthenticationTlsAuth()
-                                    .endKafkaListenerAuthenticationTlsAuth()
-                                    .build())
-                            .withNewKafkaAuthorizationSimple()
-                            .endKafkaAuthorizationSimple()
-                            .addToConfig("default.replication.factor", 3)
-                            .addToConfig("min.insync.replicas", 2)
-                        .endKafka()
-                        .editZookeeper()
-                            .withNewPersistentClaimStorage()
-                            .withSize("1Gi")
-                            .withDeleteClaim(zkDeleteClaim)
-                            .endPersistentClaimStorage()
-                        .endZookeeper()
-                        .editOrNewEntityOperator()
-                            .editOrNewTemplate()
-                                .editOrNewTopicOperatorContainer()
-                                    .addNewEnv()
-                                        .withName("STRIMZI_USE_FINALIZERS")
-                                        .withValue("false")
-                                    .endEnv()
-                                .endTopicOperatorContainer()
-                            .endTemplate()
-                        .endEntityOperator()
-                    .endSpec()
-                    .build());
-
-        brokerPodsSnapshot = PodUtils.podSnapshot(testStorage.getNamespaceName(), brokerSelector);
-
-        // at this moment, everything should be ready, so we should ideally create some topics and send + receive the messages (to have some data present in Kafka + metadata about topics in ZK)
-        LOGGER.info("Creating two topics for immediate and continuous message transmission and KafkaUser for the TLS");
-        resourceManager.createResourceWithWait(
-                KafkaTopicTemplates.topic(testStorage).build(),
-                KafkaTopicTemplates.topic(testStorage.getNamespaceName(), continuousTopicName, testStorage.getClusterName(), 3, 3, 2).build(),
-                KafkaUserTemplates.tlsUser(testStorage)
-                    .editOrNewSpec()
-                        .withNewKafkaUserAuthorizationSimple()
-                            .addNewAcl()
-                                .withNewAclRuleTopicResource()
-                                    .withPatternType(AclResourcePatternType.PREFIX)
-                                    .withName(testStorage.getTopicName())
-                                .endAclRuleTopicResource()
-                                .withOperations(AclOperation.READ, AclOperation.WRITE, AclOperation.DESCRIBE, AclOperation.CREATE)
-                            .endAcl()
-                            .addNewAcl()
-                                .withNewAclRuleGroupResource()
-                                    .withPatternType(AclResourcePatternType.LITERAL)
-                                    .withName(immediateConsumerGroup)
-                                .endAclRuleGroupResource()
-                                .withOperations(AclOperation.READ)
-                            .endAcl()
-                        .endKafkaUserAuthorizationSimple()
-                    .endSpec()
-                    .build(),
-                KafkaUserTemplates.scramShaUser(testStorage.getNamespaceName(), continuousUserName, testStorage.getClusterName())
-                    .editOrNewSpec()
-                        .withNewKafkaUserAuthorizationSimple()
-                            .addNewAcl()
-                                .withNewAclRuleTopicResource()
-                                    .withPatternType(AclResourcePatternType.LITERAL)
-                                    .withName(continuousTopicName)
-                                .endAclRuleTopicResource()
-                                .withOperations(AclOperation.READ, AclOperation.WRITE, AclOperation.DESCRIBE, AclOperation.CREATE)
-                            .endAcl()
-                            .addNewAcl()
-                                .withNewAclRuleGroupResource()
-                                    .withPatternType(AclResourcePatternType.LITERAL)
-                                    .withName(continuousConsumerGroupName)
-                                .endAclRuleGroupResource()
-                                .withOperations(AclOperation.READ)
-                            .endAcl()
-                        .endKafkaUserAuthorizationSimple()
-                    .endSpec()
-                    .build()
-        );
-
-        // sanity check that kafkaMetadataState shows ZooKeeper
-        KafkaUtils.waitUntilKafkaStatusContainsKafkaMetadataState(testStorage.getNamespaceName(), testStorage.getClusterName(), KafkaMetadataState.ZooKeeper);
-
-        // do the immediate message transmission
-        resourceManager.createResourceWithWait(
-            immediateClients.producerTlsStrimzi(testStorage.getClusterName()),
-            immediateClients.consumerTlsStrimzi(testStorage.getClusterName())
-        );
-
-        ClientUtils.waitForClientsSuccess(testStorage.getNamespaceName(), testStorage.getConsumerName(), testStorage.getProducerName(), testStorage.getMessageCount());
-    }
-
-    private void doFirstPartOfMigration(TestStorage testStorage, boolean deleteCoDuringProcess, boolean withJbodStorage) {
-        // starting the migration
-        LOGGER.info("Starting the migration process");
-
-        LOGGER.info("Starting continuous clients");
-        resourceManager.createResourceWithWait(
-            continuousClients.producerScramShaPlainStrimzi(),
-            continuousClients.consumerScramShaPlainStrimzi()
-        );
-
-        LOGGER.info("Creating controller NodePool");
-        // the controller pods will not be up and running, because we are using the ZK nodes as controllers, they will be created once the migration starts
-        // creating it here (before KafkaTopics) to correctly delete KafkaTopics and prevent stuck because UTO cannot connect to controllers
-        resourceManager.createResourceWithoutWait(KafkaNodePoolTemplates.controllerPoolPersistentStorage(testStorage.getNamespaceName(), testStorage.getControllerPoolName(), testStorage.getClusterName(), 3)
-            .editSpec()
-                .withStorage(getStorageBasedOnTestCase(withJbodStorage))
-                // for controllers we have 256Mi set in the memory limits/requests, but during migration, more memory is needed
-                .withResources(new ResourceRequirementsBuilder()
-                    .addToLimits("memory", new Quantity("384Mi"))
-                    .addToRequests("memory", new Quantity("384Mi"))
-                    .build())
-            .endSpec()
-            .build());
-
-        LOGGER.info("Applying the {} annotation with value: {}", Annotations.ANNO_STRIMZI_IO_KRAFT, "migration");
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> kafka.getMetadata().getAnnotations().put(Annotations.ANNO_STRIMZI_IO_KRAFT, "migration")
-        );
-
-        LOGGER.info("Waiting for controller Pods to be up and running");
-        PodUtils.waitForPodsReady(testStorage.getNamespaceName(), controllerSelector, 3, true);
-        controllerPodsSnapshot = PodUtils.podSnapshot(testStorage.getNamespaceName(), controllerSelector);
-
-        LOGGER.info("Waiting until .status.kafkaMetadataState in Kafka will contain KRaftMigration state");
-        KafkaUtils.waitUntilKafkaStatusContainsKafkaMetadataState(testStorage.getNamespaceName(), testStorage.getClusterName(), KafkaMetadataState.KRaftMigration);
-
-        if (deleteCoDuringProcess) {
-            LOGGER.info("Waiting for first Broker Pod starts with rolling update, so CO can be deleted");
-            RollingUpdateUtils.waitTillComponentHasStartedRolling(testStorage.getNamespaceName(), brokerSelector, brokerPodsSnapshot);
-            LOGGER.info("Deleting ClusterOperator's Pod - it should be recreated and continue with the first rolling update of broker Pods");
-            deleteClusterOperator();
-        }
-
-        LOGGER.info("Waiting for first rolling update of broker Pods - bringing to DualWrite mode");
-        brokerPodsSnapshot = RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), brokerSelector, brokerPodsSnapshot);
-
-        LOGGER.info("Waiting until .status.kafkaMetadataState in Kafka will contain KRaftDualWriting state");
-        KafkaUtils.waitUntilKafkaStatusContainsKafkaMetadataState(testStorage.getNamespaceName(), testStorage.getClusterName(), KafkaMetadataState.KRaftDualWriting);
-
-        if (deleteCoDuringProcess) {
-            LOGGER.info("Waiting for first Broker Pod starts with rolling update, so CO can be deleted");
-            RollingUpdateUtils.waitTillComponentHasStartedRolling(testStorage.getNamespaceName(), brokerSelector, brokerPodsSnapshot);
-            LOGGER.info("Deleting ClusterOperator's Pod - it should be recreated and continue with the second rolling update of broker Pods");
-            deleteClusterOperator();
-        }
-
-        moveToKRaftPostMigrationState(testStorage, postMigrationTopicName);
-    }
-
-    private void moveToKRaftPostMigrationState(TestStorage testStorage, String postMigrationTopicName) {
-        LOGGER.info("Waiting for second rolling update of broker Pods - removing dependency on ZK");
-        brokerPodsSnapshot = RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), brokerSelector, 3, brokerPodsSnapshot);
-
-        LOGGER.info("Waiting until .status.kafkaMetadataState in Kafka will contain KRaftPostMigration state");
-        KafkaUtils.waitUntilKafkaStatusContainsKafkaMetadataState(testStorage.getNamespaceName(), testStorage.getClusterName(), KafkaMetadataState.KRaftPostMigration);
-
-        createKafkaTopicAndCheckMetadataWithMessageTransmission(testStorage, postMigrationTopicName, true);
-    }
-
-    private void doSecondPartOfMigration(TestStorage testStorage, boolean deleteCoDuringProcess, boolean zkDeleteClaim) {
-        LOGGER.info("Finishing migration - applying the {} annotation with value: {}, controllers should be rolled", Annotations.ANNO_STRIMZI_IO_KRAFT, "enabled");
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> kafka.getMetadata().getAnnotations().put(Annotations.ANNO_STRIMZI_IO_KRAFT, "enabled")
-        );
-
-        if (deleteCoDuringProcess) {
-            LOGGER.info("Waiting for first controller Pod starts with rolling update, so CO can be deleted");
-            RollingUpdateUtils.waitTillComponentHasStartedRolling(testStorage.getNamespaceName(), controllerSelector, controllerPodsSnapshot);
-            LOGGER.info("Deleting ClusterOperator's Pod - it should be recreated and continue with the rolling update of the controller Pods");
-            deleteClusterOperator();
-        }
-
-        controllerPodsSnapshot = RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), controllerSelector, 3, controllerPodsSnapshot);
-
-        LOGGER.info("ZK related resources should be removed now, so waiting for all resources to be deleted");
-
-        waitForZooKeeperResourcesDeletion(testStorage, zkDeleteClaim);
-
-        LOGGER.info("Everything related to ZK is deleted, waiting until .status.kafkaMetadataState in Kafka will contain KRaft state");
-
-        KafkaUtils.waitUntilKafkaStatusContainsKafkaMetadataState(testStorage.getNamespaceName(), testStorage.getClusterName(), KafkaMetadataState.KRaft);
-
-        // the configuration of LMFV and IBPV is done (encapsulated) inside the KafkaTemplates.kafkaPersistent() method
-        LOGGER.info("Removing LMFV and IBPV from Kafka config -> Brokers and Controllers should be rolled");
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> {
-            kafka.getSpec().getKafka().getConfig().remove("log.message.format.version");
-            kafka.getSpec().getKafka().getConfig().remove("inter.broker.protocol.version");
-        });
-
-        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), brokerSelector, 3, brokerPodsSnapshot);
-        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), controllerSelector, 3, controllerPodsSnapshot);
-
-        createKafkaTopicAndCheckMetadataWithMessageTransmission(testStorage, kraftTopicName, false);
-
-        LOGGER.info("Migration is completed, waiting for continuous clients to finish");
-        ClientUtils.waitForClientsSuccess(testStorage.getNamespaceName(), continuousClients.getConsumerName(), continuousClients.getProducerName(), continuousClients.getMessageCount());
-    }
-
-    private void doRollback(TestStorage testStorage, boolean deleteCoDuringProcess) {
-        doFirstPartOfRollback(testStorage, deleteCoDuringProcess);
-        doSecondPartOfRollback(testStorage, deleteCoDuringProcess);
-    }
-
-    private void doFirstPartOfRollback(TestStorage testStorage, boolean deleteCoDuringProcess) {
-        LOGGER.info("Checking that __cluster_metadata topic does exist in Kafka Brokers");
-        assertThatClusterMetadataTopicPresentInBrokerPod(testStorage.getNamespaceName(), brokerSelector, true);
-
-        LOGGER.info("From {} state we are going to roll back to ZK", KafkaMetadataState.KRaftPostMigration.name());
-
-        LOGGER.info("Rolling migration back - applying the {} annotation with value: {}", Annotations.ANNO_STRIMZI_IO_KRAFT, "rollback");
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> kafka.getMetadata().getAnnotations().put(Annotations.ANNO_STRIMZI_IO_KRAFT, "rollback")
-        );
-
-        if (deleteCoDuringProcess) {
-            LOGGER.info("Waiting for first broker Pod starts with rolling update, so CO can be deleted");
-            RollingUpdateUtils.waitTillComponentHasStartedRolling(testStorage.getNamespaceName(), brokerSelector, brokerPodsSnapshot);
-            LOGGER.info("Deleting ClusterOperator's Pod - it should be recreated and continue with the rolling update of the broker Pods");
-            deleteClusterOperator();
-        }
-
-        LOGGER.info("Waiting for Broker Pods to be rolled - bringing back dependency on ZK");
-        brokerPodsSnapshot = RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), brokerSelector, 3, brokerPodsSnapshot);
-
-        LOGGER.info("Waiting until .status.kafkaMetadataState contains {} state", KafkaMetadataState.KRaftDualWriting.name());
-        KafkaUtils.waitUntilKafkaStatusContainsKafkaMetadataState(testStorage.getNamespaceName(), testStorage.getClusterName(), KafkaMetadataState.KRaftDualWriting);
-    }
-
-    private void doSecondPartOfRollback(TestStorage testStorage, boolean deleteCoDuringProcess) {
-        LOGGER.info("Deleting Controller's NodePool");
-        KafkaNodePool controllerPool = KafkaNodePoolResource.kafkaNodePoolClient().inNamespace(testStorage.getNamespaceName()).withName(testStorage.getControllerPoolName()).get();
-        resourceManager.deleteResource(controllerPool);
-
-        LOGGER.info("Finishing the rollback - applying the {} annotation with value: {}", Annotations.ANNO_STRIMZI_IO_KRAFT, "disabled");
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> kafka.getMetadata().getAnnotations().put(Annotations.ANNO_STRIMZI_IO_KRAFT, "disabled")
-        );
-
-        if (deleteCoDuringProcess) {
-            LOGGER.info("Waiting for first broker Pod starts with rolling update, so CO can be deleted");
-            RollingUpdateUtils.waitTillComponentHasStartedRolling(testStorage.getNamespaceName(), brokerSelector, brokerPodsSnapshot);
-            LOGGER.info("Deleting ClusterOperator's Pod - it should be recreated and continue with the rolling update of the broker Pods");
-            deleteClusterOperator();
-        }
-
-        LOGGER.info("Waiting for Broker Pods to be rolled - rolling back from DualWrite mode to Zookeeper");
-        brokerPodsSnapshot = RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), brokerSelector, 3, brokerPodsSnapshot);
-
-        LOGGER.info("Waiting until .status.kafkaMetadataState contains {} state", KafkaMetadataState.ZooKeeper.name());
-        KafkaUtils.waitUntilKafkaStatusContainsKafkaMetadataState(testStorage.getNamespaceName(), testStorage.getClusterName(), KafkaMetadataState.ZooKeeper);
-
-        LOGGER.info("Checking that __cluster_metadata topic does not exist in Kafka Brokers");
-        assertThatClusterMetadataTopicPresentInBrokerPod(testStorage.getNamespaceName(), brokerSelector, false);
-
-        LOGGER.info("Rollback completed, waiting until continuous messages transmission is finished");
-        ClientUtils.waitForClientsSuccess(testStorage.getNamespaceName(), continuousClients.getConsumerName(), continuousClients.getProducerName(), continuousClients.getMessageCount());
-    }
-
-    private void createKafkaTopicAndCheckMetadataWithMessageTransmission(TestStorage testStorage, String newTopicName, boolean checkZk) {
-        LOGGER.info("Creating KafkaTopic: {} and checking if the metadata are in both ZK and KRaft", newTopicName);
-        resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(testStorage.getNamespaceName(), newTopicName, testStorage.getClusterName()).build());
-
-        LOGGER.info("Checking if metadata about KafkaTopic: {} are in KRaft controller", newTopicName);
-
-        assertThatTopicIsPresentInKRaftMetadata(testStorage.getNamespaceName(), controllerSelector, newTopicName);
-
-        if (checkZk) {
-            LOGGER.info("Checking if metadata about KafkaTopic: {} are in ZK", newTopicName);
-            assertThatTopicIsPresentInZKMetadata(testStorage.getNamespaceName(),
-                    KafkaResource.getLabelSelector(testStorage.getClusterName(), KafkaResources.zookeeperComponentName(testStorage.getClusterName())), newTopicName);
-        }
-
-        LOGGER.info("Checking if we are able to do a message transmission on KafkaTopic: {}", newTopicName);
-
-        immediateClients = new KafkaClientsBuilder(immediateClients)
-            .withTopicName(newTopicName)
-            .build();
-
-        resourceManager.createResourceWithWait(
-                immediateClients.producerTlsStrimzi(testStorage.getClusterName()),
-                immediateClients.consumerTlsStrimzi(testStorage.getClusterName())
-        );
-
-        ClientUtils.waitForClientsSuccess(testStorage.getNamespaceName(), testStorage.getConsumerName(), testStorage.getProducerName(), testStorage.getMessageCount());
-    }
-
-    private void waitForZooKeeperResourcesDeletion(TestStorage testStorage, boolean deleteClaim) {
-        List<String> listOfZkResources = new ArrayList<>(List.of("networkpolicy", "serviceaccount", "service", "secret", "configmap", "poddisruptionbudget", "strimzipodset", "pod"));
-
-        if (deleteClaim) {
-            listOfZkResources.add("persistentvolumeclaim");
-        }
-
-        LOGGER.info("Waiting until all ZK resources: {} will be deleted", String.join(",", listOfZkResources));
-
-        try {
-            cmdKubeClient().namespace(testStorage.getNamespaceName())
-                .execInCurrentNamespace(Level.INFO, "wait", "--for", "delete", String.join(",", listOfZkResources),
-                    "-l", Labels.STRIMZI_NAME_LABEL + "=" + KafkaResources.zookeeperComponentName(testStorage.getClusterName()), "--timeout=300s");
-        } catch (KubeClusterException e) {
-            if (!e.getMessage().contains("no matching resources found")) {
-                // in case that the exception contains message about "no matching resources found", the ZK resources were deleted before execution of the command
-                // otherwise we should throw the exception
-                throw e;
-            }
-        }
-
-        LOGGER.info("All ZK related resources were deleted");
-
-        if (!deleteClaim) {
-            LOGGER.info("Checking that ZK PVCs are not deleted, because deleteClaim was set to: false");
-            List<PersistentVolumeClaim> zkPvcs = kubeClient().listPersistentVolumeClaims(testStorage.getNamespaceName(), KafkaResources.zookeeperComponentName(testStorage.getClusterName()));
-            assertThat("Zookeeper PVCs were deleted even if the deleteClaim was set to false", !zkPvcs.isEmpty(), is(true));
-            zkPvcs.forEach(pvc -> kubeClient().deletePersistentVolumeClaim(testStorage.getNamespaceName(), pvc.getMetadata().getName()));
-        }
-    }
-
-    private void deleteClusterOperator() {
-        LOGGER.info("Deleting Cluster Operator Pod");
-        Pod coPod = KubeClusterResource.kubeClient().listPodsByPrefixInName(clusterOperator.getDeploymentNamespace(), clusterOperator.getClusterOperatorName()).get(0);
-        KubeClusterResource.kubeClient().deletePod(clusterOperator.getDeploymentNamespace(), coPod);
-        LOGGER.info("Cluster Operator Pod deleted");
-    }
-
-    /**
-     * Based on {@param withJbodStorage} it returns the {@link Storage} - either with just one Persistent Volume, or with
-     * JBOD storage containing two Persistent Volumes.
-     *
-     * @param withJbodStorage   boolean parameter determining if the JBOD storage should be used in the test case
-     *
-     * @return  {@link Storage} for the particular testcase (single PV/JBOD)
-     */
-    private Storage getStorageBasedOnTestCase(boolean withJbodStorage) {
-        String pvSize = "1Gi";
-
-        PersistentClaimStorage volume = new PersistentClaimStorageBuilder()
-            .withDeleteClaim(true)
-            .withId(METADATA_VOLUME_ID)
-            .withKraftMetadata(KRaftMetadataStorage.SHARED)
-            .withSize(pvSize)
-            .build();
-
-        if (withJbodStorage) {
-            PersistentClaimStorage secondVolume = new PersistentClaimStorageBuilder()
-                .withDeleteClaim(true)
-                .withId(1)
-                .withSize(pvSize)
-                .build();
-
-            return new JbodStorageBuilder()
-                .withVolumes(volume, secondVolume)
-                .build();
-        }
-
-        return volume;
-    }
-
-    /**
-     * Method returning simple stream of arguments for migration parametrized tests.
-     * First argument determines if the ZK delete claim should be set to true/false, second if
-     * the CO should be deleted during the migration/rollback process, and third if the JBOD storage should be used.
-     *
-     * @return  {@link Stream} of {@link Arguments} for each test case.
-     */
-    private Stream<Arguments> getArgumentsForMigrationParametrizedTests() {
-        // ZK delete claim: true, delete CO during process: false, with JBOD storage: true
-        Arguments firstTest = Arguments.of(true, false, true);
-        // ZK delete claim: false, delete CO during process: true, with JBOD storage: false
-        Arguments secondTest = Arguments.of(false, true, false);
-
-        return Stream.of(firstTest, secondTest);
-    }
-
-    /**
-     * Method returning simple stream of arguments for rollback parametrized tests.
-     * First argument determines if the CO should be deleted during the migration/rollback process
-     * and second if the JBOD storage should be used.
-     *
-     * @return  {@link Stream} of {@link Arguments} for each test case.
-     */
-    private Stream<Arguments> getArgumentsForRollbackParametrizedTests() {
-        // delete CO during process: false, with JBOD storage: true
-        Arguments firstTest = Arguments.of(false, true);
-        // delete CO during process: true, with JBOD storage: false
-        Arguments secondTest = Arguments.of(true, false);
-
-        return Stream.of(firstTest, secondTest);
-    }
-
-    private void assertThatTopicIsPresentInZKMetadata(String namespaceName, LabelSelector zkSelector, String topicName) {
-        String zkPodName = kubeClient().namespace(namespaceName).listPods(zkSelector).get(0).getMetadata().getName();
-
-        TestUtils.waitFor(String.join("KafkaTopic: %s to be present in ZK metadata"), TestConstants.GLOBAL_POLL_INTERVAL_MEDIUM, TestConstants.GLOBAL_STATUS_TIMEOUT,
-            () -> {
-                try {
-                    String commandOutput = cmdKubeClient().namespace(namespaceName).execInPod(zkPodName, "./bin/zookeeper-shell.sh", "localhost:12181", "ls", "/brokers/topics").out().trim();
-                    return commandOutput.contains(topicName);
-                } catch (Exception e) {
-                    LOGGER.warn("Exception caught during command execution, message: {}", e.getMessage());
-                    return false;
-                }
-            });
-    }
-
-    private void assertThatTopicIsPresentInKRaftMetadata(String namespaceName, LabelSelector controllerSelector, String topicName) {
-        String controllerPodName = kubeClient().namespace(namespaceName).listPods(controllerSelector).get(0).getMetadata().getName();
-        String kafkaLogDirName = KafkaUtils.getKafkaLogFolderNameInPod(namespaceName, controllerPodName, metadataFolderName);
-
-        TestUtils.waitFor(String.join("KafkaTopic: %s to be present in KRaft metadata"), TestConstants.GLOBAL_POLL_INTERVAL_MEDIUM, TestConstants.GLOBAL_STATUS_TIMEOUT,
-            () -> {
-                try {
-                    String commandOutput = cmdKubeClient().namespace(namespaceName).execInPod(controllerPodName, "/bin/bash", "-c", "./bin/kafka-dump-log.sh --cluster-metadata-decoder --skip-record-metadata" +
-                        " --files /var/lib/kafka/" + metadataFolderName + "/" + kafkaLogDirName + "/__cluster_metadata-0/00000000000000000000.log | grep " + topicName).out().trim();
-                    return commandOutput.contains(topicName);
-                } catch (Exception e) {
-                    LOGGER.warn("Exception caught during command execution, message: {}", e.getMessage());
-                    return false;
-                }
-            });
-    }
-
-    private void assertThatClusterMetadataTopicPresentInBrokerPod(String namespaceName, LabelSelector brokerSelector, boolean clusterMetadataShouldExist) {
-        List<Pod> brokerPods = kubeClient().namespace(namespaceName).listPods(brokerSelector);
-
-        for (Pod brokerPod : brokerPods) {
-            String kafkaLogDirName = KafkaUtils.getKafkaLogFolderNameInPod(namespaceName, brokerPod.getMetadata().getName(), metadataFolderName);
-
-            String commandOutput = cmdKubeClient().namespace(namespaceName).execInPod(brokerPod.getMetadata().getName(), "/bin/bash", "-c", "ls /var/lib/kafka/" + metadataFolderName + "/" + kafkaLogDirName).out().trim();
-
-            assertThat(String.join("__cluster_metadata topic is present in Kafka Pod: %s, but it shouldn't", brokerPod.getMetadata().getName()),
-                commandOutput.contains("__cluster_metadata"), is(clusterMetadataShouldExist));
-        }
-    }
-
-    @BeforeAll
-    void setup() {
-        assumeTrue(Environment.isKafkaNodePoolsEnabled() && Environment.isKRaftModeEnabled());
-        this.clusterOperator = this.clusterOperator
-            .defaultInstallation()
-            .createInstallation()
-            .runInstallation();
-    }
-}
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/mirrormaker/MirrorMaker2ST.java b/systemtest/src/test/java/io/strimzi/systemtest/mirrormaker/MirrorMaker2ST.java
index 844cb8bca8f..8d2061412b8 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/mirrormaker/MirrorMaker2ST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/mirrormaker/MirrorMaker2ST.java
@@ -23,10 +23,8 @@
 import io.strimzi.operator.common.Annotations;
 import io.strimzi.operator.common.model.Labels;
 import io.strimzi.systemtest.AbstractST;
-import io.strimzi.systemtest.Environment;
 import io.strimzi.systemtest.TestConstants;
 import io.strimzi.systemtest.annotations.ParallelNamespaceTest;
-import io.strimzi.systemtest.cli.KafkaCmdClient;
 import io.strimzi.systemtest.kafkaclients.internalClients.KafkaClients;
 import io.strimzi.systemtest.kafkaclients.internalClients.KafkaClientsBuilder;
 import io.strimzi.systemtest.kafkaclients.internalClients.admin.AdminClient;
@@ -88,7 +86,6 @@
 import static org.hamcrest.Matchers.is;
 import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;
 import static org.junit.jupiter.api.Assertions.assertFalse;
-import static org.junit.jupiter.api.Assertions.assertNotNull;
 import static org.junit.jupiter.api.Assertions.assertTrue;
 import static org.valid4j.matchers.jsonpath.JsonPathMatchers.hasJsonPath;
 
@@ -641,17 +638,6 @@ void testIdentityReplicationPolicy() {
         final KafkaClients targetClients = ClientUtils.getInstantPlainClients(testStorage, KafkaResources.plainBootstrapAddress(testStorage.getTargetClusterName()));
         resourceManager.createResourceWithWait(targetClients.consumerStrimzi());
         ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
-
-        if (!Environment.isKRaftModeEnabled()) {
-            LOGGER.info("Checking if the mirrored Topic name is same as the original one");
-
-            List<String> kafkaTopics = KafkaCmdClient.listTopicsUsingPodCli(testStorage.getNamespaceName(), scraperPodName, KafkaResources.plainBootstrapAddress(testStorage.getTargetClusterName()));
-            assertNotNull(kafkaTopics.stream().filter(kafkaTopic -> kafkaTopic.equals(testStorage.getTopicName())).findAny());
-
-            String kafkaTopicSpec = KafkaCmdClient.describeTopicUsingPodCli(testStorage.getNamespaceName(), scraperPodName, KafkaResources.plainBootstrapAddress(testStorage.getTargetClusterName()), testStorage.getTopicName());
-            assertThat(kafkaTopicSpec, containsString("Topic: " + testStorage.getTopicName()));
-            assertThat(kafkaTopicSpec, containsString("PartitionCount: 3"));
-        }
     }
 
     @ParallelNamespaceTest
@@ -1219,9 +1205,7 @@ void testKMM2RollAfterSecretsCertsUpdateTLS() {
         String sourceClusterCaSecretName = KafkaResources.clusterCaCertificateSecretName(testStorage.getSourceClusterName());
         SecretUtils.annotateSecret(testStorage.getNamespaceName(), sourceClusterCaSecretName, Annotations.ANNO_STRIMZI_IO_FORCE_RENEW, "true");
 
-        if (!Environment.isKRaftModeEnabled()) {
-            RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), controlSourceSelector, 1, controlSourcePods);
-        }
+        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), controlSourceSelector, 1, controlSourcePods);
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), brokerSourceSelector, 1, brokerSourcePods);
         DeploymentUtils.waitTillDepHasRolled(testStorage.getNamespaceName(), KafkaResources.entityOperatorDeploymentName(testStorage.getSourceClusterName()), 1, eoSourcePods);
         mmSnapshot = RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getMM2Selector(), 1, mmSnapshot);
@@ -1230,9 +1214,7 @@ void testKMM2RollAfterSecretsCertsUpdateTLS() {
         String targetClusterCaSecretName = KafkaResources.clusterCaCertificateSecretName(testStorage.getTargetClusterName());
         SecretUtils.annotateSecret(testStorage.getNamespaceName(), targetClusterCaSecretName, Annotations.ANNO_STRIMZI_IO_FORCE_RENEW, "true");
 
-        if (!Environment.isKRaftModeEnabled()) {
-            RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), controlTargetSelector, 1, controlTargetPods);
-        }
+        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), controlTargetSelector, 1, controlTargetPods);
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), brokerTargetSelector, 1, brokerTargetPods);
         DeploymentUtils.waitTillDepHasRolled(testStorage.getNamespaceName(), KafkaResources.entityOperatorDeploymentName(testStorage.getTargetClusterName()), 1, eoTargetPods);
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getMM2Selector(), 1, mmSnapshot);
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/operators/NamespaceDeletionRecoveryST.java b/systemtest/src/test/java/io/strimzi/systemtest/operators/NamespaceDeletionRecoveryST.java
index e67cdb842c2..d4a31ea0535 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/operators/NamespaceDeletionRecoveryST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/operators/NamespaceDeletionRecoveryST.java
@@ -16,7 +16,6 @@
 import io.strimzi.api.kafka.model.topic.KafkaTopic;
 import io.strimzi.operator.common.Annotations;
 import io.strimzi.systemtest.AbstractST;
-import io.strimzi.systemtest.Environment;
 import io.strimzi.systemtest.TestConstants;
 import io.strimzi.systemtest.annotations.IsolatedTest;
 import io.strimzi.systemtest.cli.KafkaCmdClient;
@@ -47,7 +46,6 @@
 
 import static io.strimzi.systemtest.TestTags.RECOVERY;
 import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;
-import static org.junit.jupiter.api.Assumptions.assumeTrue;
 
 /**
  * Suite for testing topic recovery in case of namespace deletion.
@@ -318,8 +316,6 @@ private void deleteAndRecreateNamespace(String namespace) {
 
     @BeforeAll
     void createStorageClass() {
-        assumeTrue(Environment.isKRaftModeEnabled() && Environment.isKafkaNodePoolsEnabled());
-
         // Delete specific StorageClass if present from previous
         kubeClient().getClient().storage().v1().storageClasses().withName(storageClassName).delete();
 
@@ -346,7 +342,7 @@ void teardown() {
         kubeClient().deleteStorageClassWithName(storageClassName);
 
         kubeClient().getClient().persistentVolumes().list().getItems().stream()
-            .filter(pv -> pv.getSpec().getClaimRef().getName().contains("kafka") || pv.getSpec().getClaimRef().getName().contains("zookeeper"))
+            .filter(pv -> pv.getSpec().getClaimRef().getName().contains("kafka"))
             .forEach(pv -> kubeClient().getClient().persistentVolumes().resource(pv).delete());
     }
 }
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/operators/RecoveryST.java b/systemtest/src/test/java/io/strimzi/systemtest/operators/RecoveryST.java
index fb0aaca19ea..8365bc710fe 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/operators/RecoveryST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/operators/RecoveryST.java
@@ -42,7 +42,6 @@
 import static io.strimzi.systemtest.TestTags.REGRESSION;
 import static io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils.generateRandomNameOfKafka;
 import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;
-import static org.junit.jupiter.api.Assumptions.assumeTrue;
 
 @Tag(REGRESSION)
 class RecoveryST extends AbstractST {
@@ -157,8 +156,6 @@ void testRecoveryFromKafkaPodDeletion() {
 
     @BeforeEach
     void setup() {
-        assumeTrue(Environment.isKRaftModeEnabled() && Environment.isKafkaNodePoolsEnabled());
-
         this.clusterOperator = this.clusterOperator.defaultInstallation()
             .withReconciliationInterval(TestConstants.CO_OPERATION_TIMEOUT_SHORT)
             .createInstallation()
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/operators/topic/TopicReplicasChangeST.java b/systemtest/src/test/java/io/strimzi/systemtest/operators/topic/TopicReplicasChangeST.java
index 53accc8509a..9b590eacc21 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/operators/topic/TopicReplicasChangeST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/operators/topic/TopicReplicasChangeST.java
@@ -101,9 +101,7 @@ void testMoreReplicasThanAvailableBrokersWithFreshKafkaTopic() {
         KafkaTopicUtils.waitForKafkaTopicNotReady(sharedTestStorage.getNamespaceName(), testStorage.getTopicName());
         KafkaTopicStatus kafkaTopicStatus = KafkaTopicResource.kafkaTopicClient().inNamespace(sharedTestStorage.getNamespaceName()).withName(testStorage.getTopicName()).get().getStatus();
 
-        String errorMessage = Environment.isKRaftModeEnabled() ?
-                "org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 5 time(s): The target replication factor of 5 cannot be reached because only 3 broker(s) are registered." :
-                "org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 5 larger than available brokers: 3";
+        String errorMessage = "org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 5 time(s): The target replication factor of 5 cannot be reached because only 3 broker(s) are registered.";
 
         assertThat(kafkaTopicStatus.getConditions().get(0).getMessage(), containsString(errorMessage));
         assertThat(kafkaTopicStatus.getConditions().get(0).getReason(), containsString("KafkaError"));
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/operators/topic/TopicST.java b/systemtest/src/test/java/io/strimzi/systemtest/operators/topic/TopicST.java
index 3426cccb092..e6d85dcaab7 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/operators/topic/TopicST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/operators/topic/TopicST.java
@@ -89,9 +89,7 @@ void testMoreReplicasThanAvailableBrokers() {
         assertThat("Topic exists in Kafka CR (Kubernetes)", hasTopicInCRK8s(kafkaTopic, testStorage.getTopicName()));
         assertThat("Topic doesn't exists in Kafka itself", !hasTopicInKafka(testStorage.getTopicName(), sharedTestStorage.getClusterName(), scraperPodName));
 
-        String errorMessage = Environment.isKRaftModeEnabled() ?
-            "org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 5 time(s): The target replication factor of 5 cannot be reached because only 3 broker(s) are registered." :
-            "org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 5 larger than available brokers: 3";
+        String errorMessage = "org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 5 time(s): The target replication factor of 5 cannot be reached because only 3 broker(s) are registered.";
 
         KafkaTopicUtils.waitForKafkaTopicNotReady(Environment.TEST_SUITE_NAMESPACE, testStorage.getTopicName());
         KafkaTopicStatus kafkaTopicStatus = KafkaTopicResource.kafkaTopicClient().inNamespace(Environment.TEST_SUITE_NAMESPACE).withName(testStorage.getTopicName()).get().getStatus();
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java b/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java
index 252ce9a98a9..01d7c40f811 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java
@@ -62,7 +62,6 @@
 import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.MatcherAssert.assertThat;
-import static org.junit.jupiter.api.Assumptions.assumeTrue;
 
 @Tag(REGRESSION)
 @Tag(ROLLING_UPDATE)
@@ -135,24 +134,22 @@ void testManualTriggeringRollingUpdate() {
         resourceManager.createResourceWithWait(instantClients.consumerTlsStrimzi(testStorage.getClusterName()));
         ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
 
-        if (!Environment.isKRaftModeEnabled()) {
-            // rolling update for zookeeper
-            // set annotation to trigger Zookeeper rolling update
-            LOGGER.info("Annotate ZooKeeper: {} - {}/{} with manual rolling update annotation", StrimziPodSet.RESOURCE_KIND, testStorage.getNamespaceName(), testStorage.getControllerComponentName());
+        // rolling update for controller pods
+        // set annotation to trigger controller rolling update
+        LOGGER.info("Annotate controller: {} - {}/{} with manual rolling update annotation", StrimziPodSet.RESOURCE_KIND, testStorage.getNamespaceName(), testStorage.getControllerComponentName());
 
-            StrimziPodSetUtils.annotateStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName(), Collections.singletonMap(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE, "true"));
+        StrimziPodSetUtils.annotateStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName(), Collections.singletonMap(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE, "true"));
 
-            // check annotation to trigger rolling update
-            assertThat(Boolean.parseBoolean(StrimziPodSetUtils.getAnnotationsOfStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName())
-                    .get(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE)), is(true));
+        // check annotation to trigger rolling update
+        assertThat(Boolean.parseBoolean(StrimziPodSetUtils.getAnnotationsOfStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName())
+                .get(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE)), is(true));
 
-            RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
+        RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
 
-            // wait when annotation will be removed
-            TestUtils.waitFor("CO removes rolling update annotation", TestConstants.WAIT_FOR_ROLLING_UPDATE_INTERVAL, TestConstants.GLOBAL_TIMEOUT,
-                    () -> StrimziPodSetUtils.getAnnotationsOfStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName()) == null
-                            || !StrimziPodSetUtils.getAnnotationsOfStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName()).containsKey(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE));
-        }
+        // wait when annotation will be removed
+        TestUtils.waitFor("CO removes rolling update annotation", TestConstants.WAIT_FOR_ROLLING_UPDATE_INTERVAL, TestConstants.GLOBAL_TIMEOUT,
+                () -> StrimziPodSetUtils.getAnnotationsOfStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName()) == null
+                        || !StrimziPodSetUtils.getAnnotationsOfStrimziPodSet(testStorage.getNamespaceName(), testStorage.getControllerComponentName()).containsKey(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE));
 
         instantClients.generateNewConsumerGroup();
         resourceManager.createResourceWithWait(instantClients.consumerTlsStrimzi(testStorage.getClusterName()));
@@ -253,20 +250,18 @@ void testManualRollingUpdateForSinglePod() {
         );
         resourceManager.createResourceWithWait(KafkaTemplates.kafkaPersistent(testStorage.getNamespaceName(), testStorage.getClusterName(), 3).build());
 
-        Pod kafkaPod = kubeClient().listPods(testStorage.getNamespaceName(), testStorage.getBrokerSelector()).get(0);
-        String kafkaPodName = kafkaPod.getMetadata().getName();
+        Pod brokerPod = kubeClient().listPods(testStorage.getNamespaceName(), testStorage.getBrokerSelector()).get(0);
+        String brokerPodName = brokerPod.getMetadata().getName();
+
+        Pod controllerPod = kubeClient().listPods(testStorage.getNamespaceName(), testStorage.getBrokerSelector()).get(0);
+        String controllerPodName = controllerPod.getMetadata().getName();
 
         // snapshot of one single Kafka pod
-        Map<String, String> kafkaSnapshot = Collections.singletonMap(kafkaPod.getMetadata().getName(), kafkaPod.getMetadata().getUid());
-        Map<String, String> zkSnapshot = null;
-        if (!Environment.isKRaftModeEnabled()) {
-            Pod zkPod = kubeClient(testStorage.getNamespaceName()).getPod(KafkaResources.zookeeperPodName(testStorage.getClusterName(), 0));
-            // snapshot of one single ZK pod
-            zkSnapshot = Collections.singletonMap(zkPod.getMetadata().getName(), zkPod.getMetadata().getUid());
-        }
+        Map<String, String> brokerSnapshot = Collections.singletonMap(brokerPodName, brokerPod.getMetadata().getUid());
+        Map<String, String> controllerSnapshot = Collections.singletonMap(controllerPodName, controllerPod.getMetadata().getUid());
 
-        LOGGER.info("Trying to roll just single Kafka and single ZK Pod");
-        kubeClient(testStorage.getNamespaceName()).editPod(kafkaPodName).edit(pod -> new PodBuilder(pod)
+        LOGGER.info("Trying to roll just single broker and single controller Pod");
+        kubeClient(testStorage.getNamespaceName()).editPod(brokerPodName).edit(pod -> new PodBuilder(pod)
             .editMetadata()
                 .addToAnnotations(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE, "true")
             .endMetadata()
@@ -274,21 +269,30 @@ void testManualRollingUpdateForSinglePod() {
 
         // here we are waiting just to one pod's snapshot will be changed and all 3 pods ready -> if we set expectedPods to 1,
         // the check will pass immediately without waiting for all pods to be ready -> the method picks first ready pod and return true
-        kafkaSnapshot = RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, kafkaSnapshot);
+        brokerSnapshot = RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerSnapshot);
+
+        kubeClient(testStorage.getNamespaceName()).editPod(controllerPodName).edit(pod -> new PodBuilder(pod)
+            .editMetadata()
+                .addToAnnotations(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE, "true")
+            .endMetadata()
+            .build());
 
-        if (!Environment.isKRaftModeEnabled()) {
-            kubeClient(testStorage.getNamespaceName()).editPod(KafkaResources.zookeeperPodName(testStorage.getClusterName(), 0)).edit(pod -> new PodBuilder(pod)
+        // same as above
+        controllerSnapshot = RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerSnapshot);
+
+        LOGGER.info("Adding anno to all broker and controller Pods");
+        brokerSnapshot.keySet().forEach(podName -> {
+            kubeClient(testStorage.getNamespaceName()).editPod(podName).edit(pod -> new PodBuilder(pod)
                 .editMetadata()
                     .addToAnnotations(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE, "true")
                 .endMetadata()
                 .build());
+        });
 
-            // same as above
-            zkSnapshot = RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, zkSnapshot);
-        }
+        LOGGER.info("Checking if the rolling update will be successful for brokers");
+        RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerSnapshot);
 
-        LOGGER.info("Adding anno to all ZK and Kafka Pods");
-        kafkaSnapshot.keySet().forEach(podName -> {
+        controllerSnapshot.keySet().forEach(podName -> {
             kubeClient(testStorage.getNamespaceName()).editPod(podName).edit(pod -> new PodBuilder(pod)
                 .editMetadata()
                     .addToAnnotations(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE, "true")
@@ -296,21 +300,8 @@ void testManualRollingUpdateForSinglePod() {
                 .build());
         });
 
-        LOGGER.info("Checking if the rolling update will be successful for Kafka");
-        RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, kafkaSnapshot);
-
-        if (!Environment.isKRaftModeEnabled()) {
-            zkSnapshot.keySet().forEach(podName -> {
-                kubeClient(testStorage.getNamespaceName()).editPod(podName).edit(pod -> new PodBuilder(pod)
-                    .editMetadata()
-                        .addToAnnotations(Annotations.ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE, "true")
-                    .endMetadata()
-                    .build());
-            });
-
-            LOGGER.info("Checking if the rolling update will be successful for ZK");
-            RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, zkSnapshot);
-        }
+        LOGGER.info("Checking if the rolling update will be successful for controllers");
+        RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerSnapshot);
     }
 
     /**
@@ -487,9 +478,6 @@ void testAddingAndRemovingJbodVolumes() {
     @ParallelNamespaceTest
     @SuppressWarnings("deprecation") // Storage is deprecated, but some API methods are still called here
     void testJbodMetadataLogRelocation() {
-        // This test makes sense only in KRaft mode
-        assumeTrue(Environment.isKRaftModeEnabled());
-
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
         final int numberOfKafkaReplicas = 3;
 
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/KafkaRollerST.java b/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/KafkaRollerST.java
index 47fed988dbe..ac38af0a8a2 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/KafkaRollerST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/KafkaRollerST.java
@@ -70,7 +70,6 @@
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.jupiter.api.Assertions.assertEquals;
 import static org.junit.jupiter.api.Assertions.assertTrue;
-import static org.junit.jupiter.api.Assumptions.assumeTrue;
 
 @Tag(REGRESSION)
 @Tag(ROLLING_UPDATE)
@@ -119,11 +118,7 @@ void testKafkaDoesNotRollsWhenTopicIsUnderReplicated() {
         ClientUtils.waitForInstantClientSuccess(testStorage);
 
         LOGGER.info("Scale Kafka up from 3 to 4 brokers");
-        if (Environment.isKafkaNodePoolsEnabled()) {
-            KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), knp -> knp.getSpec().setReplicas(scaledUpBrokerReplicaCount));
-        } else {
-            KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> k.getSpec().getKafka().setReplicas(scaledUpBrokerReplicaCount));
-        }
+        KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), knp -> knp.getSpec().setReplicas(scaledUpBrokerReplicaCount));
         RollingUpdateUtils.waitForComponentScaleUpOrDown(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), scaledUpBrokerReplicaCount);
 
         LOGGER.info("Create kafkaTopic: {}/{} with replica on each broker", testStorage.getNamespaceName(), topicNameWith4Replicas);
@@ -131,7 +126,7 @@ void testKafkaDoesNotRollsWhenTopicIsUnderReplicated() {
         resourceManager.createResourceWithWait(kafkaTopicWith4Replicas);
 
         // last pod has index 3 (as it is 4th) or 6 (being 7th) as there are also 3 controllers
-        final int scaledBrokerPodIndex = !Environment.isKRaftModeEnabled() ? 3 : 6;
+        final int scaledBrokerPodIndex = 6;
         String uid = kubeClient(testStorage.getNamespaceName()).getPodUid(KafkaResource.getKafkaPodName(testStorage.getClusterName(), KafkaNodePoolResource.getBrokerPoolName(testStorage.getClusterName()),  scaledBrokerPodIndex));
         List<Event> events = kubeClient(testStorage.getNamespaceName()).listEventsByResourceUid(uid);
         assertThat(events, hasAllOfReasons(Scheduled, Pulled, Created, Started));
@@ -148,14 +143,9 @@ void testKafkaDoesNotRollsWhenTopicIsUnderReplicated() {
         ClientUtils.waitForInstantClientSuccess(testStorage);
 
         LOGGER.info("Scaling down to {}", initialBrokerReplicaCount);
-        if (Environment.isKafkaNodePoolsEnabled()) {
-            KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), knp -> knp.getSpec().setReplicas(initialBrokerReplicaCount));
-            KafkaNodePoolUtils.waitForKafkaNodePoolStatusUpdate(testStorage.getNamespaceName(), testStorage.getBrokerPoolName());
-            assertThat("NodePool still has old number of replicas", KafkaNodePoolResource.kafkaNodePoolClient().inNamespace(testStorage.getNamespaceName()).withName(testStorage.getBrokerPoolName()).get().getStatus().getReplicas(), is(4));
-        } else {
-            KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> k.getSpec().getKafka().setReplicas(initialBrokerReplicaCount));
-            KafkaUtils.waitForKafkaStatusUpdate(testStorage.getNamespaceName(), testStorage.getClusterName());
-        }
+        KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), knp -> knp.getSpec().setReplicas(initialBrokerReplicaCount));
+        KafkaNodePoolUtils.waitForKafkaNodePoolStatusUpdate(testStorage.getNamespaceName(), testStorage.getBrokerPoolName());
+        assertThat("NodePool still has old number of replicas", KafkaNodePoolResource.kafkaNodePoolClient().inNamespace(testStorage.getNamespaceName()).withName(testStorage.getBrokerPoolName()).get().getStatus().getReplicas(), is(4));
 
         LOGGER.info("Scale-down should have been reverted and the cluster should be still Ready");
         KafkaUtils.waitForKafkaReady(testStorage.getNamespaceName(), testStorage.getClusterName());
@@ -287,7 +277,6 @@ void testKafkaPodCrashLooping() {
     }
 
     @ParallelNamespaceTest
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
     void testKafkaPodImagePullBackOff() {
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
@@ -303,10 +292,6 @@ void testKafkaPodImagePullBackOff() {
 
         KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> {
             kafka.getSpec().getKafka().setImage("quay.io/strimzi/kafka:not-existent-tag");
-
-            if (!Environment.isKRaftModeEnabled()) {
-                kafka.getSpec().getZookeeper().setImage(kafkaImage);
-            }
         });
 
         KafkaUtils.waitForKafkaNotReady(testStorage.getNamespaceName(), testStorage.getClusterName());
@@ -416,8 +401,6 @@ void testKafkaPodPendingDueToRack() {
      */
     @ParallelNamespaceTest
     void testKafkaRollingUpdatesOfSingleRoleNodePools() {
-        assumeTrue(Environment.isKRaftModeEnabled());
-
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
         final int brokerPoolReplicas = 3, controllerPoolReplicas = 3;
@@ -507,8 +490,6 @@ void testKafkaRollingUpdatesOfSingleRoleNodePools() {
      */
     @ParallelNamespaceTest
     void testKafkaRollingUpdatesOfMixedNodes() {
-        assumeTrue(Environment.isKRaftModeEnabled());
-
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
         final int mixedPoolReplicas = 6;
 
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/RollingUpdateST.java b/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/RollingUpdateST.java
index be80ae3a857..04eb180b8c5 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/RollingUpdateST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/RollingUpdateST.java
@@ -14,24 +14,19 @@
 import io.fabric8.kubernetes.api.model.LabelSelector;
 import io.fabric8.kubernetes.api.model.Quantity;
 import io.fabric8.kubernetes.api.model.ResourceRequirements;
-import io.fabric8.kubernetes.api.model.ResourceRequirementsBuilder;
 import io.skodjob.testframe.MetricsCollector;
 import io.strimzi.api.kafka.model.common.ProbeBuilder;
 import io.strimzi.api.kafka.model.common.metrics.JmxPrometheusExporterMetrics;
 import io.strimzi.api.kafka.model.common.metrics.JmxPrometheusExporterMetricsBuilder;
-import io.strimzi.api.kafka.model.kafka.KafkaResources;
 import io.strimzi.api.kafka.model.topic.KafkaTopic;
 import io.strimzi.operator.common.Annotations;
 import io.strimzi.systemtest.AbstractST;
 import io.strimzi.systemtest.Environment;
 import io.strimzi.systemtest.TestConstants;
 import io.strimzi.systemtest.annotations.IsolatedTest;
-import io.strimzi.systemtest.annotations.KRaftNotSupported;
 import io.strimzi.systemtest.annotations.ParallelNamespaceTest;
 import io.strimzi.systemtest.kafkaclients.internalClients.KafkaClients;
-import io.strimzi.systemtest.kafkaclients.internalClients.KafkaClientsBuilder;
 import io.strimzi.systemtest.metrics.KafkaMetricsComponent;
-import io.strimzi.systemtest.metrics.ZookeeperMetricsComponent;
 import io.strimzi.systemtest.resources.NodePoolsConverter;
 import io.strimzi.systemtest.resources.ResourceManager;
 import io.strimzi.systemtest.resources.crd.KafkaNodePoolResource;
@@ -46,8 +41,6 @@
 import io.strimzi.systemtest.utils.RollingUpdateUtils;
 import io.strimzi.systemtest.utils.StUtils;
 import io.strimzi.systemtest.utils.VerificationUtils;
-import io.strimzi.systemtest.utils.kafkaUtils.KafkaTopicUtils;
-import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;
 import io.strimzi.systemtest.utils.kubeUtils.objects.PersistentVolumeClaimUtils;
 import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;
 import io.strimzi.test.TestUtils;
@@ -56,177 +49,24 @@
 import org.junit.jupiter.api.BeforeAll;
 import org.junit.jupiter.api.Tag;
 
-import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
-import java.util.List;
 import java.util.Map;
-import java.util.regex.Pattern;
 
 import static io.strimzi.systemtest.TestTags.ACCEPTANCE;
 import static io.strimzi.systemtest.TestTags.COMPONENT_SCALING;
 import static io.strimzi.systemtest.TestTags.REGRESSION;
 import static io.strimzi.systemtest.TestTags.ROLLING_UPDATE;
-import static io.strimzi.systemtest.k8s.Events.Killing;
-import static io.strimzi.systemtest.matchers.Matchers.hasAllOfReasons;
 import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;
 import static java.util.Collections.singletonMap;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.jupiter.api.Assertions.assertEquals;
-import static org.junit.jupiter.api.Assumptions.assumeTrue;
 
 @Tag(REGRESSION)
 class RollingUpdateST extends AbstractST {
 
     private static final Logger LOGGER = LogManager.getLogger(RollingUpdateST.class);
-    private static final Pattern ZK_SERVER_STATE = Pattern.compile("zk_server_state\\s+(leader|follower)");
-
-    /**
-     * @description This test case checks recover during Kafka Rolling Update in Zookeeper based Kafka cluster.
-     *
-     * @steps
-     *  1. - Deploy Kafka Cluster with 3 replicas
-     *  2. - Deploy Kafka producer and send messages targeting created KafkaTopic
-     *  3. - Modify Zookeeper to unreasonable CPU request causing Rolling Update
-     *     - One of Zookeeper Pods is in Pending state
-     *  4. - Consume messages from KafkaTopic created previously
-     *  5. - Modify Zookeeper to reasonable CPU request
-     *     - Zookeeper Pods are rolled including previously pending Pod
-     *  6. - Modify Kafka with an unreasonable CPU request to trigger a Rolling Update
-     *     - One of Kafka Pods is in Pending state
-     *  7. - Consume messages from KafkaTopic created previously
-     *  8. - Modify Kafka to the reasonable CPU request
-     *     - Pods are rolled including previously pending Pod
-     *  9. - Create a new KafkaTopic and transmit messages using this topic
-     *     - Topic is created and messages transmitted, verifying both Zookeeper and Kafka works
-     *
-     * @usecase
-     *  - kafka
-     *  - zookeeper
-     *  - rolling-update
-     */
-    @ParallelNamespaceTest
-    @Tag(ROLLING_UPDATE)
-    @KRaftNotSupported("ZooKeeper is not supported by KRaft mode and is used in this test case")
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
-    void testRecoveryDuringZookeeperBasedRollingUpdate() {
-        final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
-
-        resourceManager.createResourceWithWait(
-            NodePoolsConverter.convertNodePoolsIfNeeded(
-                KafkaNodePoolTemplates.brokerPoolPersistentStorage(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), testStorage.getClusterName(), 3).build(),
-                KafkaNodePoolTemplates.controllerPoolPersistentStorage(testStorage.getNamespaceName(), testStorage.getControllerPoolName(), testStorage.getClusterName(), 3).build()
-            )
-        );
-        resourceManager.createResourceWithWait(
-            KafkaTemplates.kafkaPersistent(testStorage.getNamespaceName(), testStorage.getClusterName(), 3).build(),
-            KafkaTopicTemplates.topic(testStorage.getNamespaceName(), testStorage.getTopicName(), testStorage.getClusterName(), 2, 2).build()
-        );
-
-        // produce messages
-        KafkaClients clients = ClientUtils.getInstantPlainClients(testStorage);
-        resourceManager.createResourceWithWait(clients.producerStrimzi());
-        ClientUtils.waitForInstantProducerClientSuccess(testStorage);
-
-        // zookeeper recovery
-
-        LOGGER.info("Update resources for Pods");
-        // modify zookeeper resource to unreasonable value
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> {
-            k.getSpec()
-                .getZookeeper()
-                .setResources(new ResourceRequirementsBuilder()
-                    .addToRequests("cpu", new Quantity("100000m"))
-                    .build());
-        });
-
-        // consume messages
-        resourceManager.createResourceWithWait(clients.consumerStrimzi());
-        ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
-
-        PodUtils.waitForPendingPod(testStorage.getNamespaceName(), testStorage.getControllerComponentName());
-        LOGGER.info("Verifying stability of ZooKeeper Pods except the one, which is in pending phase");
-        PodUtils.verifyThatRunningPodsAreStable(testStorage.getNamespaceName(), testStorage.getControllerComponentName());
-
-        LOGGER.info("Verifying stability of Kafka Pods");
-        PodUtils.verifyThatRunningPodsAreStable(testStorage.getNamespaceName(), testStorage.getBrokerComponentName());
-
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> {
-            k.getSpec()
-                .getZookeeper()
-                .setResources(new ResourceRequirementsBuilder()
-                    .addToRequests("cpu", new Quantity("200m"))
-                    .build());
-        });
-
-        RollingUpdateUtils.waitForComponentAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3);
-
-        // consume messages with newly generated consumer group
-        clients.generateNewConsumerGroup();
-        resourceManager.createResourceWithWait(clients.consumerStrimzi());
-        ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
-
-        // Kafka recovery
-
-        // change kafka to unreasonable CPU request causing trigger of Rolling update and recover by second modification
-        // if kafka KafkaNodePool is enabled change specification directly in KNP CR as changing it in kafka would have no impact in case it is already specified in KNP
-        if (Environment.isKafkaNodePoolsEnabled()) {
-            KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), knp -> {
-                knp.getSpec()
-                    .setResources(new ResourceRequirementsBuilder()
-                        .addToRequests("cpu", new Quantity("100000m"))
-                        .build());
-            });
-        } else {
-            KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> {
-                k.getSpec()
-                    .getKafka()
-                    .setResources(new ResourceRequirementsBuilder()
-                        .addToRequests("cpu", new Quantity("100000m"))
-                        .build());
-            });
-        }
-
-        PodUtils.waitForPendingPod(testStorage.getNamespaceName(), testStorage.getBrokerComponentName());
-
-        // consume messages with newly generated consumer group
-        clients.generateNewConsumerGroup();
-        resourceManager.createResourceWithWait(clients.consumerStrimzi());
-        ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
-
-        LOGGER.info("Recover Kafka {}/{} from pending state by modifying its resource request to realistic value", testStorage.getClusterName(), testStorage.getNamespaceName());
-        // if kafka KafkaNodePool is enabled change specification directly in KNP CR as changing it in kafka would have no impact in case it is already specified in KNP
-        if (Environment.isKafkaNodePoolsEnabled()) {
-            KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), knp -> {
-                knp.getSpec()
-                    .setResources(new ResourceRequirementsBuilder()
-                        .addToRequests("cpu", new Quantity("200m"))
-                        .build());
-            });
-        } else {
-            KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> {
-                k.getSpec()
-                    .getKafka()
-                    .setResources(new ResourceRequirementsBuilder()
-                        .addToRequests("cpu", new Quantity("200m"))
-                        .build());
-            });
-        }
-
-        // Create new topic to ensure, that ZK is working properly
-        String newTopicName = KafkaTopicUtils.generateRandomNameOfTopic();
-
-        resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(testStorage.getNamespaceName(), newTopicName, testStorage.getClusterName()).build());
-
-        clients = new KafkaClientsBuilder(clients)
-            .withTopicName(newTopicName)
-            .withConsumerGroup(ClientUtils.generateRandomConsumerGroup())
-            .build();
-
-        resourceManager.createResourceWithWait(clients.producerStrimzi(), clients.consumerStrimzi());
-        ClientUtils.waitForInstantClientSuccess(testStorage);
-    }
 
     /**
      * @description This test case checks recover during Kafka Rolling Update in KRaft based Kafka cluster.
@@ -252,8 +92,6 @@ void testRecoveryDuringZookeeperBasedRollingUpdate() {
     @ParallelNamespaceTest
     @Tag(ROLLING_UPDATE)
     void testRecoveryDuringKRaftRollingUpdate() {
-        assumeTrue(Environment.isKRaftModeEnabled());
-
         // kafka with 1 knp broker and 1 knp controller
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
@@ -305,7 +143,6 @@ void testRecoveryDuringKRaftRollingUpdate() {
     @ParallelNamespaceTest
     @Tag(ACCEPTANCE)
     @Tag(COMPONENT_SCALING)
-    @SuppressWarnings("deprecation") // Replicas in Kafka CR are deprecated, but some API methods are still called here
     void testKafkaScaleUpScaleDown() {
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
@@ -325,8 +162,8 @@ void testKafkaScaleUpScaleDown() {
             .editSpec()
                 .editKafka()
                     // Topic Operator doesn't support KRaft, yet, using auto topic creation and default replication factor as workaround
-                    .addToConfig(singletonMap("default.replication.factor", Environment.isKRaftModeEnabled() ? 3 : 1))
-                    .addToConfig("auto.create.topics.enable", Environment.isKRaftModeEnabled())
+                    .addToConfig(singletonMap("default.replication.factor", 3))
+                    .addToConfig("auto.create.topics.enable", true)
                 .endKafka()
             .endSpec()
             .build(),
@@ -354,14 +191,8 @@ void testKafkaScaleUpScaleDown() {
         final int scaleTo = initialReplicas + 2;
         LOGGER.info("Scale up Kafka to {}", scaleTo);
 
-        if (Environment.isKafkaNodePoolsEnabled()) {
-            KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), knp ->
-                knp.getSpec().setReplicas(scaleTo));
-        } else {
-            KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> {
-                kafka.getSpec().getKafka().setReplicas(scaleTo);
-            });
-        }
+        KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), knp ->
+            knp.getSpec().setReplicas(scaleTo));
 
         RollingUpdateUtils.waitForComponentScaleUpOrDown(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), scaleTo);
 
@@ -375,7 +206,6 @@ void testKafkaScaleUpScaleDown() {
         ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
 
         // new topic has more replicas than there was available Kafka brokers before scaling up
-
         LOGGER.info("Create new KafkaTopic with replica count requiring existence of brokers added by scaling up");
         KafkaTopic scaledUpKafkaTopicResource = KafkaTopicTemplates.topic(testStorage.getNamespaceName(), topicNameScaledUp, testStorage.getClusterName())
             .editSpec()
@@ -401,12 +231,8 @@ void testKafkaScaleUpScaleDown() {
         // scale down
 
         LOGGER.info("Scale down Kafka to {}", initialReplicas);
-        if (Environment.isKafkaNodePoolsEnabled()) {
-            KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(),
-                knp -> knp.getSpec().setReplicas(initialReplicas));
-        } else {
-            KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> k.getSpec().getKafka().setReplicas(initialReplicas));
-        }
+        KafkaNodePoolResource.replaceKafkaNodePoolResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(),
+            knp -> knp.getSpec().setReplicas(initialReplicas));
 
         RollingUpdateUtils.waitForComponentScaleUpOrDown(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), initialReplicas);
         LOGGER.info("Kafka scale down to {} finished", initialReplicas);
@@ -420,8 +246,7 @@ void testKafkaScaleUpScaleDown() {
 
         PersistentVolumeClaimUtils.waitForPersistentVolumeClaimDeletion(testStorage, initialReplicas);
 
-        // Create new topic to ensure, that ZK or KRaft is working properly
-
+        // Create new topic to ensure, that KRaft is working properly
         LOGGER.info("Creating new KafkaTopic: {}/{} and producing consuming data", testStorage.getNamespaceName(), topicNameScaledBackDown);
 
         resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(testStorage.getNamespaceName(), topicNameScaledBackDown, testStorage.getClusterName()).build());
@@ -435,164 +260,6 @@ void testKafkaScaleUpScaleDown() {
         ClientUtils.waitForInstantClientSuccess(testStorage);
     }
 
-    /**
-     * @description This test case checks scaling Zookeeper up and down and that it works correctly during and after this event.
-     *
-     * @steps
-     *  1. - Deploy persistent Kafka Cluster with 3 replicas, first KafkaTopic, and KafkaUser
-     *     - Cluster with 3 replicas and other resources are created and ready
-     *  2. - Deploy Kafka clients, produce and consume messages targeting created KafkaTopic
-     *     - Data are produced and consumed successfully
-     *  3. - Scale up Zookeeper Cluster from 3 to 7 replicas
-     *     - Cluster scales to 7 replicas, quorum is temporarily lost but regained afterwards
-     *  4. - Deploy new KafkaTopic and new clients which will target this KafkaTopic, and also first KafkaTopic
-     *     - New KafkaTopic is deployed and ready, clients successfully communicate with topics represented by mentioned KafkaTopics
-     *  5. - Scale down Zookeeper Cluster back from 7 to 3 replicas
-     *     - Cluster scales down to 3, Zookeeper
-     *  6. - Deploy new KafkaTopic and new clients targeting it
-     *     - New KafkaTopic is created and ready, all clients can communicate successfully
-     *
-     * @usecase
-     *  - zookeeper
-     *  - scale-up
-     *  - scale-down
-     */
-    @ParallelNamespaceTest
-    @Tag(COMPONENT_SCALING)
-    @KRaftNotSupported("Zookeeper is not supported by KRaft mode and is used in this test case")
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
-    void testZookeeperScaleUpScaleDown() {
-        final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
-
-        resourceManager.createResourceWithWait(
-            NodePoolsConverter.convertNodePoolsIfNeeded(
-                KafkaNodePoolTemplates.brokerPoolPersistentStorage(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), testStorage.getClusterName(), 3).build(),
-                KafkaNodePoolTemplates.controllerPoolPersistentStorage(testStorage.getNamespaceName(), testStorage.getControllerPoolName(), testStorage.getClusterName(), 3).build()
-            )
-        );
-        resourceManager.createResourceWithWait(
-            KafkaTemplates.kafkaPersistent(testStorage.getNamespaceName(), testStorage.getClusterName(), 3, 3).build(),
-            KafkaTopicTemplates.topic(testStorage).build(),
-            KafkaUserTemplates.tlsUser(testStorage).build()
-        );
-
-        // kafka cluster already deployed
-        LOGGER.info("Running zookeeperScaleUpScaleDown with cluster {}", testStorage.getClusterName());
-        final int initialZkReplicas = kubeClient().getClient().pods().inNamespace(testStorage.getNamespaceName()).withLabelSelector(testStorage.getControllerSelector()).list().getItems().size();
-        assertThat(initialZkReplicas, is(3));
-
-        KafkaClients clients = ClientUtils.getInstantTlsClients(testStorage);
-        resourceManager.createResourceWithWait(
-            clients.producerTlsStrimzi(testStorage.getClusterName()),
-            clients.consumerTlsStrimzi(testStorage.getClusterName())
-        );
-        ClientUtils.waitForInstantClientSuccess(testStorage);
-
-        final int scaleZkTo = initialZkReplicas + 4;
-        final List<String> newZkPodNames = new ArrayList<String>() {{
-                for (int i = initialZkReplicas; i < scaleZkTo; i++) {
-                    add(KafkaResources.zookeeperPodName(testStorage.getClusterName(), i));
-                }
-            }};
-
-        LOGGER.info("Scale up ZooKeeper to {}", scaleZkTo);
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> k.getSpec().getZookeeper().setReplicas(scaleZkTo));
-
-        clients.generateNewConsumerGroup();
-        resourceManager.createResourceWithWait(clients.consumerTlsStrimzi(testStorage.getClusterName()));
-        ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
-
-        RollingUpdateUtils.waitForComponentAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), scaleZkTo);
-        // check the new node is either in leader or follower state
-        KafkaUtils.waitForZkMntr(testStorage.getNamespaceName(), testStorage.getClusterName(), ZK_SERVER_STATE, 0, 1, 2, 3, 4, 5, 6);
-
-
-        String newTopicName = KafkaTopicUtils.generateRandomNameOfTopic();
-
-        LOGGER.info("Creating new KafkaTopic {}, and producing/consuming data in to to verify Zookeeper handles it after scaling up", newTopicName);
-        resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(testStorage.getClusterName(), newTopicName, testStorage.getNamespaceName()).build());
-
-        clients = new KafkaClientsBuilder(clients)
-            .withTopicName(newTopicName)
-            .build();
-
-        resourceManager.createResourceWithWait(clients.producerTlsStrimzi(testStorage.getClusterName()), clients.consumerTlsStrimzi(testStorage.getClusterName()));
-        ClientUtils.waitForInstantClientSuccess(testStorage);
-
-        // Create new topic to ensure, that ZK is working properly
-        String scaleUpTopicName = KafkaTopicUtils.generateRandomNameOfTopic();
-
-        resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(testStorage.getNamespaceName(), scaleUpTopicName, testStorage.getClusterName(), 1, 1).build());
-
-        clients = new KafkaClientsBuilder(clients)
-            .withTopicName(scaleUpTopicName)
-            .build();
-
-        resourceManager.createResourceWithWait(clients.producerTlsStrimzi(testStorage.getClusterName()), clients.consumerTlsStrimzi(testStorage.getClusterName()));
-        ClientUtils.waitForInstantClientSuccess(testStorage);
-
-        // scale down
-        LOGGER.info("Scale down ZooKeeper to {}", initialZkReplicas);
-        // Get zk-3 uid before deletion
-        String uid = kubeClient(testStorage.getNamespaceName()).getPodUid(newZkPodNames.get(3));
-
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> k.getSpec().getZookeeper().setReplicas(initialZkReplicas));
-
-        RollingUpdateUtils.waitForComponentAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), initialZkReplicas);
-
-        // Wait for one zk pods will became leader and others follower state
-        KafkaUtils.waitForZkMntr(testStorage.getNamespaceName(), testStorage.getClusterName(), ZK_SERVER_STATE, 0, 1, 2);
-
-        clients.generateNewConsumerGroup();
-        resourceManager.createResourceWithWait(clients.consumerTlsStrimzi(testStorage.getClusterName()));
-        ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
-
-        // Create new topic to ensure, that ZK is working properly
-        String scaleDownTopicName = KafkaTopicUtils.generateRandomNameOfTopic();
-        resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(testStorage.getNamespaceName(), scaleDownTopicName, testStorage.getClusterName(), 1, 1).build());
-
-        clients = new KafkaClientsBuilder(clients)
-            .withTopicName(scaleDownTopicName)
-            .build();
-
-        resourceManager.createResourceWithWait(clients.producerTlsStrimzi(testStorage.getClusterName()), clients.consumerTlsStrimzi(testStorage.getClusterName()));
-        ClientUtils.waitForInstantClientSuccess(testStorage);
-
-        //Test that the second pod has event 'Killing'
-        assertThat(kubeClient(testStorage.getNamespaceName()).listEventsByResourceUid(uid), hasAllOfReasons(Killing));
-    }
-
-    @ParallelNamespaceTest
-    @Tag(ROLLING_UPDATE)
-    void testBrokerConfigurationChangeTriggerRollingUpdate() {
-        final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
-
-        resourceManager.createResourceWithWait(
-            NodePoolsConverter.convertNodePoolsIfNeeded(
-                KafkaNodePoolTemplates.brokerPoolPersistentStorage(testStorage.getNamespaceName(), testStorage.getBrokerPoolName(), testStorage.getClusterName(), 3).build(),
-                KafkaNodePoolTemplates.controllerPoolPersistentStorage(testStorage.getNamespaceName(), testStorage.getControllerPoolName(), testStorage.getClusterName(), 3).build()
-            )
-        );
-        resourceManager.createResourceWithWait(KafkaTemplates.kafkaPersistent(testStorage.getNamespaceName(), testStorage.getClusterName(), 3, 3).build());
-
-        Map<String, String> brokerPods = PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getBrokerSelector());
-        Map<String, String> controllerPods = null;
-
-        if (!Environment.isKRaftModeEnabled()) {
-            controllerPods = PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector());
-        }
-
-        // Changes to readiness probe should trigger a rolling update
-        KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> {
-            kafka.getSpec().getKafka().setReadinessProbe(new ProbeBuilder().withTimeoutSeconds(6).build());
-        });
-
-        RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
-        if (!Environment.isKRaftModeEnabled()) {
-            assertThat(PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector()), is(controllerPods));
-        }
-    }
-
     /**
      * @description This test case verifies that cluster operator can finish rolling update of components despite being restarted.
      *
@@ -612,7 +279,6 @@ void testBrokerConfigurationChangeTriggerRollingUpdate() {
      */
     @IsolatedTest("Deleting Pod of Shared Cluster Operator")
     @Tag(ROLLING_UPDATE)
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
     void testClusterOperatorFinishAllRollingUpdates() {
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
@@ -630,9 +296,6 @@ void testClusterOperatorFinishAllRollingUpdates() {
         // Changes to readiness probe should trigger a rolling update
         KafkaResource.replaceKafkaResourceInSpecificNamespace(Environment.TEST_SUITE_NAMESPACE, testStorage.getClusterName(), kafka -> {
             kafka.getSpec().getKafka().setReadinessProbe(new ProbeBuilder().withTimeoutSeconds(6).build());
-            if (!Environment.isKRaftModeEnabled()) {
-                kafka.getSpec().getZookeeper().setReadinessProbe(new ProbeBuilder().withTimeoutSeconds(6).build());
-            }
         });
 
         TestUtils.waitFor("rolling update starts", TestConstants.GLOBAL_POLL_INTERVAL, TestConstants.GLOBAL_STATUS_TIMEOUT,
@@ -644,10 +307,10 @@ void testClusterOperatorFinishAllRollingUpdates() {
         kubeClient(clusterOperator.getDeploymentNamespace()).deletePodsByLabelSelector(coLabelSelector);
         LOGGER.info("Cluster Operator Pod deleted");
 
-        LOGGER.info("Rolling Update is taking place, starting with roll of Zookeeper Pods with labels {}", testStorage.getControllerSelector());
+        LOGGER.info("Rolling Update is taking place, starting with roll of controller Pods with labels {}", testStorage.getControllerSelector());
         RollingUpdateUtils.waitTillComponentHasRolled(Environment.TEST_SUITE_NAMESPACE, testStorage.getControllerSelector(), 3, controllerPods);
 
-        LOGGER.info("Wait till first Kafka Pod rolls");
+        LOGGER.info("Wait till broker Pods roll");
         RollingUpdateUtils.waitTillComponentHasStartedRolling(Environment.TEST_SUITE_NAMESPACE, testStorage.getBrokerSelector(), brokerPods);
 
         LOGGER.info("Deleting Cluster Operator Pod with labels {}, while Rolling update rolls Kafka Pods", coLabelSelector);
@@ -662,13 +325,13 @@ void testClusterOperatorFinishAllRollingUpdates() {
      * @description This test case check that enabling metrics and metrics manipulation triggers Rolling Update.
      *
      * @steps
-     *  1. - Deploy Kafka Cluster with Zookeeper and with disabled metrics configuration
+     *  1. - Deploy Kafka Cluster with disabled metrics configuration
      *     - Cluster is deployed
-     *  2. - Change specification of Kafka Cluster by configuring metrics for Kafka, Zookeeper, and configuring metrics Exporter
+     *  2. - Change specification of Kafka Cluster by configuring metrics for Kafka and configuring metrics Exporter
      *     - Allowing metrics does not trigger Rolling Update
      *  3. - Setup or deploy necessary scraper, metric rules, and collectors and collect metrics
      *     - Metrics are successfully collected
-     *  4. - Modify patterns in rules for collecting metrics in Zookeeper and Kafka by updating respective Config Maps
+     *  4. - Modify patterns in rules for collecting metrics in Kafka by updating respective Config Maps
      *     - Respective changes do not trigger Rolling Update, Cluster remains stable and metrics are exposed according to new rules
      *  5. - Change specification of Kafka Cluster by removing any metric related configuration
      *     - Rolling Update is triggered and metrics are no longer present.
@@ -680,8 +343,6 @@ void testClusterOperatorFinishAllRollingUpdates() {
      */
     @IsolatedTest
     @Tag(ROLLING_UPDATE)
-    @KRaftNotSupported("Zookeeper is not supported by KRaft mode and is used in this test class")
-    @SuppressWarnings({"checkstyle:MethodLength", "deprecation"}) // ZooKeeper is deprecated, but some APi methods are still called here
     void testMetricsChange() throws JsonProcessingException {
         final TestStorage testStorage = new TestStorage(ResourceManager.getTestContext());
 
@@ -717,40 +378,7 @@ void testMetricsChange() throws JsonProcessingException {
             .endValueFrom()
             .build();
 
-        //Zookeeper
-        Map<String, Object> zookeeperLabels = new HashMap<>();
-        zookeeperLabels.put("replicaId", "$2");
-
-        Map<String, Object> zookeeperRule = new HashMap<>();
-        zookeeperRule.put("labels", zookeeperLabels);
-        zookeeperRule.put("name", "zookeeper_$3");
-        zookeeperRule.put("pattern", "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+)><>(\\w+)");
-
-        Map<String, Object> zookeeperMetrics = new HashMap<>();
-        zookeeperMetrics.put("lowercaseOutputName", true);
-        zookeeperMetrics.put("rules", Collections.singletonList(zookeeperRule));
-
-        String metricsCMNameZk = "zk-metrics-cm";
-        ConfigMap metricsCMZk = new ConfigMapBuilder()
-            .withNewMetadata()
-                .withName(metricsCMNameZk)
-                .withNamespace(testStorage.getNamespaceName())
-            .endMetadata()
-            .withData(singletonMap("metrics-config.yml", mapper.writeValueAsString(zookeeperMetrics)))
-            .build();
-
-        JmxPrometheusExporterMetrics zkMetricsConfig = new JmxPrometheusExporterMetricsBuilder()
-            .withNewValueFrom()
-            .withConfigMapKeyRef(new ConfigMapKeySelectorBuilder()
-                .withName(metricsCMNameZk)
-                .withKey("metrics-config.yml")
-                .withOptional(true)
-                .build())
-            .endValueFrom()
-            .build();
-
         kubeClient().createConfigMapInNamespace(testStorage.getNamespaceName(), metricsCMK);
-        kubeClient().createConfigMapInNamespace(testStorage.getNamespaceName(), metricsCMZk);
 
         resourceManager.createResourceWithWait(
             NodePoolsConverter.convertNodePoolsIfNeeded(
@@ -763,9 +391,6 @@ void testMetricsChange() throws JsonProcessingException {
                 .editKafka()
                     .withMetricsConfig(kafkaMetricsConfig)
                 .endKafka()
-                .editOrNewZookeeper()
-                    .withMetricsConfig(zkMetricsConfig)
-                .endZookeeper()
                 .withNewKafkaExporter()
                 .endKafkaExporter()
             .endSpec()
@@ -784,16 +409,10 @@ void testMetricsChange() throws JsonProcessingException {
             .withComponent(KafkaMetricsComponent.create(testStorage.getClusterName()))
             .build();
 
-        MetricsCollector zkCollector = kafkaCollector.toBuilder()
-            .withComponent(ZookeeperMetricsComponent.create(testStorage.getClusterName()))
-            .build();
-
-        LOGGER.info("Check if metrics are present in Pod of Kafka and ZooKeeper");
+        LOGGER.info("Check if metrics are present in Pod of Kafka");
         kafkaCollector.collectMetricsFromPods(TestConstants.METRICS_COLLECT_TIMEOUT);
-        zkCollector.collectMetricsFromPods(TestConstants.METRICS_COLLECT_TIMEOUT);
 
         assertThat(kafkaCollector.getCollectedData().values().toString().contains("kafka_"), is(true));
-        assertThat(zkCollector.getCollectedData().values().toString().contains("replicaId"), is(true));
 
         LOGGER.info("Changing metrics to something else");
 
@@ -802,20 +421,6 @@ void testMetricsChange() throws JsonProcessingException {
         kafkaRule.replace("name", "kafka_$1_$2_$3_count", "kafka_$1_$2_$3_percent");
         kafkaRule.replace("type", "COUNTER", "GAUGE");
 
-        zookeeperRule.replace("pattern",
-            "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+)><>(\\w+)",
-            "org.apache.ZooKeeperService<name0=StandaloneServer_port(\\d+)><>(\\w+)");
-        zookeeperRule.replace("name", "zookeeper_$3", "zookeeper_$2");
-        zookeeperRule.replace("labels", zookeeperLabels, null);
-
-        metricsCMZk = new ConfigMapBuilder()
-            .withNewMetadata()
-                .withName(metricsCMNameZk)
-                .withNamespace(testStorage.getNamespaceName())
-            .endMetadata()
-            .withData(singletonMap("metrics-config.yml", mapper.writeValueAsString(zookeeperMetrics)))
-            .build();
-
         metricsCMK = new ConfigMapBuilder()
             .withNewMetadata()
                 .withName(metricsCMNameK)
@@ -825,52 +430,42 @@ void testMetricsChange() throws JsonProcessingException {
             .build();
 
         kubeClient().updateConfigMapInNamespace(testStorage.getNamespaceName(), metricsCMK);
-        kubeClient().updateConfigMapInNamespace(testStorage.getNamespaceName(), metricsCMZk);
 
         PodUtils.verifyThatRunningPodsAreStable(testStorage.getNamespaceName(), testStorage.getControllerComponentName());
         PodUtils.verifyThatRunningPodsAreStable(testStorage.getNamespaceName(), testStorage.getBrokerComponentName());
 
-        LOGGER.info("Check if Kafka and ZooKeeper Pods didn't roll");
+        LOGGER.info("Check if Kafka Pods didn't roll");
         assertThat(PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector()), is(controllerPods));
         assertThat(PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getBrokerSelector()), is(brokerPods));
 
-        LOGGER.info("Check if Kafka and ZooKeeper metrics are changed");
+        LOGGER.info("Check if Kafka metrics are changed");
         ObjectMapper yamlReader = new ObjectMapper(new YAMLFactory());
         String kafkaMetricsConf = kubeClient().getClient().configMaps().inNamespace(testStorage.getNamespaceName()).withName(metricsCMNameK).get().getData().get("metrics-config.yml");
-        String zkMetricsConf = kubeClient().getClient().configMaps().inNamespace(testStorage.getNamespaceName()).withName(metricsCMNameZk).get().getData().get("metrics-config.yml");
         Object kafkaMetricsJsonToYaml = yamlReader.readValue(kafkaMetricsConf, Object.class);
-        Object zkMetricsJsonToYaml = yamlReader.readValue(zkMetricsConf, Object.class);
         ObjectMapper jsonWriter = new ObjectMapper();
         for (String cmName : StUtils.getKafkaConfigurationConfigMaps(testStorage.getNamespaceName(), testStorage.getClusterName())) {
             assertThat(kubeClient().getClient().configMaps().inNamespace(testStorage.getNamespaceName()).withName(cmName).get().getData().get(
                     TestConstants.METRICS_CONFIG_JSON_NAME),
                 is(jsonWriter.writeValueAsString(kafkaMetricsJsonToYaml)));
         }
-        assertThat(kubeClient().getClient().configMaps().inNamespace(testStorage.getNamespaceName()).withName(KafkaResources.zookeeperMetricsAndLogConfigMapName(testStorage.getClusterName())).get().getData().get(
-                TestConstants.METRICS_CONFIG_JSON_NAME),
-            is(jsonWriter.writeValueAsString(zkMetricsJsonToYaml)));
 
-        LOGGER.info("Check if metrics are present in Pod of Kafka and ZooKeeper");
+        LOGGER.info("Check if metrics are present in Pod of Kafka");
 
         kafkaCollector.collectMetricsFromPods(TestConstants.METRICS_COLLECT_TIMEOUT);
-        zkCollector.collectMetricsFromPods(TestConstants.METRICS_COLLECT_TIMEOUT);
 
         assertThat(kafkaCollector.getCollectedData().values().toString().contains("kafka_"), is(true));
-        assertThat(zkCollector.getCollectedData().values().toString().contains("replicaId"), is(true));
 
-        LOGGER.info("Removing metrics from Kafka and ZooKeeper and setting them to null");
+        LOGGER.info("Removing metrics from Kafka and setting them to null");
         KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), kafka -> {
             kafka.getSpec().getKafka().setMetricsConfig(null);
-            kafka.getSpec().getZookeeper().setMetricsConfig(null);
         });
 
-        LOGGER.info("Waiting for Kafka and ZooKeeper Pods to roll");
+        LOGGER.info("Waiting for Kafka Pods to roll");
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
 
         LOGGER.info("Check if metrics do not exist in Pods");
         kafkaCollector.collectMetricsFromPodsWithoutWait().values().forEach(value -> assertThat(value.isEmpty(), is(true)));
-        zkCollector.collectMetricsFromPodsWithoutWait().values().forEach(value -> assertThat(value.isEmpty(), is(true)));
     }
 
     /**
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/security/NetworkPoliciesST.java b/systemtest/src/test/java/io/strimzi/systemtest/security/NetworkPoliciesST.java
index bd7548ba7db..14676f461bb 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/security/NetworkPoliciesST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/security/NetworkPoliciesST.java
@@ -346,7 +346,6 @@ void checkNetworkPoliciesInNamespace(String namespaceName, String clusterName) {
         List<NetworkPolicy> networkPolicyList = new ArrayList<>(kubeClient().getClient().network().networkPolicies().inNamespace(namespaceName).list().getItems());
 
         assertNotNull(networkPolicyList.stream().filter(networkPolicy ->  networkPolicy.getMetadata().getName().contains(KafkaResources.kafkaNetworkPolicyName(clusterName))).findFirst());
-        assertNotNull(networkPolicyList.stream().filter(networkPolicy ->  networkPolicy.getMetadata().getName().contains(KafkaResources.zookeeperNetworkPolicyName(clusterName))).findFirst());
         assertNotNull(networkPolicyList.stream().filter(networkPolicy ->  networkPolicy.getMetadata().getName().contains(KafkaResources.entityOperatorDeploymentName(clusterName))).findFirst());
 
         // if KE is enabled
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/security/PodSecurityProfilesST.java b/systemtest/src/test/java/io/strimzi/systemtest/security/PodSecurityProfilesST.java
index 3422474a5cd..06e91734cc7 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/security/PodSecurityProfilesST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/security/PodSecurityProfilesST.java
@@ -55,7 +55,7 @@
  * only some Volume types etc.).
  *
  * Test cases are design to verify common behaviour of Pod Security profiles. Specifically, (i.) we check if containers such
- * as Kafka, ZooKeeper, Entity Operator, KafkaBridge has properly set .securityContext (ii.) then we check if these
+ * as Kafka, Entity Operator, KafkaBridge has properly set .securityContext (ii.) then we check if these
  * resources working and are stable with exchanging messages.
  */
 @Tag(REGRESSION)
@@ -75,7 +75,7 @@ public class PodSecurityProfilesST extends AbstractST {
      *     - All components are deployed targeting respective Kafka Clusters
      *  4. - Deploy producer which will produce data into Topic residing in Kafka Cluster serving as Source for KafkaMirrorMakers and is targeted by other Operands
      *     - Messages are sent into KafkaTopic
-     *  5. - Verify that containers such as Kafka, ZooKeeper, Entity Operator, KafkaBridge has properly set .securityContext
+     *  5. - Verify that containers such as Kafka, Entity Operator, KafkaBridge has properly set .securityContext
      *     - All containers and Pods have expected properties
      *  6. - Verify KafkaConnect and KafkaConnector are working by checking presence of Data in file targeted by FileSink KafkaConnector
      *     - Data are present here
@@ -208,7 +208,7 @@ void testOperandsWithRestrictedSecurityProfile() {
 
     @BeforeAll
     void beforeAll() {
-        // we configure Pod Security via provider class, which sets SecurityContext to all containers (e.g., Kafka, ZooKeeper,
+        // we configure Pod Security via provider class, which sets SecurityContext to all containers (e.g., Kafka,
         // Entity Operator, Bridge). Another alternative but more complicated is to set it via .template section inside each CR.
         clusterOperator = clusterOperator
             .defaultInstallation()
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/security/SecurityST.java b/systemtest/src/test/java/io/strimzi/systemtest/security/SecurityST.java
index 393abf9de65..506a5feb4a2 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/security/SecurityST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/security/SecurityST.java
@@ -109,8 +109,6 @@ class SecurityST extends AbstractST {
     @Tag("ClusterCaCerts")
     void testAutoRenewClusterCaCertsTriggeredByAnno() {
         autoRenewSomeCaCertsTriggeredByAnno(
-                /* ZK node need new certs */
-                true,
                 /* brokers need new certs */
                 true,
                 /* eo needs new cert */
@@ -124,8 +122,6 @@ void testAutoRenewClusterCaCertsTriggeredByAnno() {
     @Tag("ClientsCaCerts")
     void testAutoRenewClientsCaCertsTriggeredByAnno() {
         autoRenewSomeCaCertsTriggeredByAnno(
-                /* no communication between clients and zk, so no need to roll */
-                false,
                 /* brokers need to trust client certs with new cert */
                 true,
                 /* eo needs to generate new client certs */
@@ -141,7 +137,6 @@ void testAutoRenewClientsCaCertsTriggeredByAnno() {
     @Tag("AllCaCerts")
     void testAutoRenewAllCaCertsTriggeredByAnno() {
         autoRenewSomeCaCertsTriggeredByAnno(
-                true,
                 true,
                 true,
                 true);
@@ -149,7 +144,6 @@ void testAutoRenewAllCaCertsTriggeredByAnno() {
 
     @SuppressWarnings({"checkstyle:MethodLength", "checkstyle:NPathComplexity"})
     void autoRenewSomeCaCertsTriggeredByAnno(
-            boolean zkShouldRoll,
             boolean kafkaShouldRoll,
             boolean eoShouldRoll,
             boolean keAndCCShouldRoll) {
@@ -202,14 +196,9 @@ void autoRenewSomeCaCertsTriggeredByAnno(
             kubeClient().patchSecret(testStorage.getNamespaceName(), secretName, annotated);
         }
 
-        if (!Environment.isKRaftModeEnabled()) {
-            if (zkShouldRoll) {
-                LOGGER.info("Waiting for ZK rolling restart");
-                RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
-            }
-        }
         if (kafkaShouldRoll) {
             LOGGER.info("Waiting for Kafka rolling restart");
+            RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, brokerPods);
             RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
         }
         if (eoShouldRoll) {
@@ -249,13 +238,9 @@ void autoRenewSomeCaCertsTriggeredByAnno(
         resourceManager.createResourceWithWait(kafkaClients.consumerTlsStrimzi(testStorage.getClusterName()));
         ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
 
-        if (!Environment.isKRaftModeEnabled()) {
-            if (!zkShouldRoll) {
-                assertThat("ZK Pods should not roll, but did.", PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector()), is(controllerPods));
-            }
-        }
         if (!kafkaShouldRoll) {
             assertThat("Kafka Pods should not roll, but did.", PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getBrokerSelector()), is(brokerPods));
+            assertThat("Kafka Pods should not roll, but did.", PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getBrokerSelector()), is(brokerPods));
         }
         if (!eoShouldRoll) {
             assertThat("EO Pod should not roll, but did.", DeploymentUtils.depSnapshot(testStorage.getNamespaceName(), testStorage.getEoDeploymentName()), is(eoPod));
@@ -275,7 +260,6 @@ void testAutoReplaceClusterCaKeysTriggeredByAnno() {
                 3, // additional third rolling due to the removal of the older cluster CA certificate
                 true,
                 true,
-                true,
                 true);
     }
 
@@ -286,7 +270,6 @@ void testAutoReplaceClusterCaKeysTriggeredByAnno() {
     void testAutoReplaceClientsCaKeysTriggeredByAnno() {
         autoReplaceSomeKeysTriggeredByAnno(
                 1,
-                false,
                 true,
                 false,
                 false);
@@ -301,13 +284,11 @@ void testAutoReplaceAllCaKeysTriggeredByAnno() {
                 3, // additional third rolling due to the removal of the older cluster CA certificate
                 true,
                 true,
-                true,
                 true);
     }
 
     @SuppressWarnings({"checkstyle:MethodLength", "checkstyle:NPathComplexity", "checkstyle:CyclomaticComplexity"})
     void autoReplaceSomeKeysTriggeredByAnno(int expectedRolls,
-                                            boolean zkShouldRoll,
                                             boolean kafkaShouldRoll,
                                             boolean eoShouldRoll,
                                             boolean keAndCCShouldRoll) {
@@ -367,17 +348,13 @@ void autoReplaceSomeKeysTriggeredByAnno(int expectedRolls,
                 Pod coPod = kubeClient().listPodsByPrefixInName(clusterOperator.getDeploymentNamespace(), clusterOperator.getClusterOperatorName()).get(0);
                 kubeClient().deletePod(clusterOperator.getDeploymentNamespace(), coPod);
             }
-            if (!Environment.isKRaftModeEnabled()) {
-                if (zkShouldRoll) {
-                    LOGGER.info("Waiting for ZK rolling restart ({})", i);
-                    controllerPods = i < expectedRolls ?
-                            RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), controllerPods) :
-                            RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
-                }
-            }
 
             if (kafkaShouldRoll) {
                 LOGGER.info("Waiting for Kafka rolling restart ({})", i);
+                controllerPods = i < expectedRolls ?
+                    RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), controllerPods) :
+                    RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
+
                 brokerPods = i < expectedRolls ?
                         RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), brokerPods) :
                         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
@@ -431,14 +408,9 @@ void autoReplaceSomeKeysTriggeredByAnno(int expectedRolls,
         resourceManager.createResourceWithWait(kafkaClients.consumerTlsStrimzi(testStorage.getClusterName()));
         ClientUtils.waitForInstantConsumerClientSuccess(testStorage);
 
-        if (!Environment.isKRaftModeEnabled()) {
-            if (!zkShouldRoll) {
-                assertThat("ZK Pods should not roll, but did.", PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector()), is(controllerPods));
-            }
-        }
-
         if (!kafkaShouldRoll) {
-            assertThat("Kafka Pods should not roll, but did.", PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getBrokerSelector()), is(brokerPods));
+            assertThat("Controller Pods should not roll, but did.", PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getControllerSelector()), is(controllerPods));
+            assertThat("Broker Pods should not roll, but did.", PodUtils.podSnapshot(testStorage.getNamespaceName(), testStorage.getBrokerSelector()), is(brokerPods));
         }
 
         if (!eoShouldRoll) {
@@ -451,7 +423,6 @@ void autoReplaceSomeKeysTriggeredByAnno(int expectedRolls,
         }
     }
 
-    @SuppressWarnings("deprecation") // ZooKeeper is deprecated, but some APi methods are still called here
     private void createKafkaCluster(TestStorage testStorage) {
         LOGGER.info("Creating a cluster");
 
@@ -499,12 +470,6 @@ private void createKafkaCluster(TestStorage testStorage) {
                         .withDeleteClaim(true)
                     .endPersistentClaimStorage()
                 .endKafka()
-                .editZookeeper()
-                    .withNewPersistentClaimStorage()
-                        .withSize("2Gi")
-                        .withDeleteClaim(true)
-                    .endPersistentClaimStorage()
-                .endZookeeper()
                 .withNewCruiseControl()
                 .endCruiseControl()
                 .withNewKafkaExporter()
@@ -512,10 +477,6 @@ private void createKafkaCluster(TestStorage testStorage) {
             .endSpec()
             .build();
 
-        if (Environment.isKRaftModeEnabled()) {
-            kafka.getSpec().setZookeeper(null);
-        }
-
         resourceManager.createResourceWithWait(kafka);
     }
 
@@ -1042,9 +1003,7 @@ void testCaRenewalBreakInMiddle() {
 
         // Wait for the certificates to get replaced
         SecretUtils.waitForCertToChange(testStorage.getNamespaceName(), clusterCaCert, KafkaResources.clusterCaCertificateSecretName(testStorage.getClusterName()));
-        if (!Environment.isKRaftModeEnabled()) {
-            RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
-        }
+        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
         DeploymentUtils.waitTillDepHasRolled(testStorage.getNamespaceName(), testStorage.getEoDeploymentName(), 1, eoPods);
 
@@ -1243,12 +1202,8 @@ void testOwnerReferenceOfCASecrets() {
 
         LOGGER.info("Deleting Kafka: {}/{}", testStorage.getNamespaceName(), testStorage.getClusterName());
         KafkaResource.kafkaClient().inNamespace(testStorage.getNamespaceName()).withName(testStorage.getClusterName()).withPropagationPolicy(DeletionPropagation.FOREGROUND).delete();
-        if (Environment.isKafkaNodePoolsEnabled()) {
-            KafkaNodePoolUtils.deleteKafkaNodePoolWithPodSetAndWait(testStorage.getNamespaceName(), testStorage.getClusterName(), testStorage.getBrokerPoolName());
-            if (Environment.isKRaftModeEnabled()) {
-                KafkaNodePoolUtils.deleteKafkaNodePoolWithPodSetAndWait(testStorage.getNamespaceName(), testStorage.getClusterName(), testStorage.getControllerPoolName());
-            }
-        }
+        KafkaNodePoolUtils.deleteKafkaNodePoolWithPodSetAndWait(testStorage.getNamespaceName(), testStorage.getClusterName(), testStorage.getBrokerPoolName());
+        KafkaNodePoolUtils.deleteKafkaNodePoolWithPodSetAndWait(testStorage.getNamespaceName(), testStorage.getClusterName(), testStorage.getControllerPoolName());
         KafkaUtils.waitForKafkaDeletion(testStorage.getNamespaceName(), testStorage.getClusterName());
 
         LOGGER.info("Checking actual Secrets after Kafka deletion");
@@ -1331,18 +1286,6 @@ void testClusterCACertRenew() {
         Date initialKafkaBrokerCertStartTime = kafkaBrokerCert.getNotBefore();
         Date initialKafkaBrokerCertEndTime = kafkaBrokerCert.getNotAfter();
 
-        Date initialZkCertStartTime = null;
-        Date initialZkCertEndTime = null;
-        Secret zkCertCreationSecret = null;
-        X509Certificate zkBrokerCert = null;
-        if (!Environment.isKRaftModeEnabled()) {
-            // Check Zookeeper certificate dates
-            zkCertCreationSecret = kubeClient(testStorage.getNamespaceName()).getSecret(testStorage.getNamespaceName(), testStorage.getClusterName() + "-zookeeper-nodes");
-            zkBrokerCert = SecretUtils.getCertificateFromSecret(zkCertCreationSecret, testStorage.getClusterName() + "-zookeeper-0.crt");
-            initialZkCertStartTime = zkBrokerCert.getNotBefore();
-            initialZkCertEndTime = zkBrokerCert.getNotAfter();
-        }
-
         LOGGER.info("Change of Kafka validity and renewal days - reconciliation should start");
         CertificateAuthority newClusterCA = new CertificateAuthority();
         newClusterCA.setRenewalDays(150);
@@ -1351,12 +1294,10 @@ void testClusterCACertRenew() {
         KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> k.getSpec().setClusterCa(newClusterCA));
 
         // On the next reconciliation, the Cluster Operator performs a `rolling update`:
-        //   a) ZooKeeper
-        //   b) Kafka
+        //   a) Controller pods
+        //   b) Broker pods
         //   c) and other components to trust the new Cluster CA certificate. (i.e., Entity Operator)
-        if (!Environment.isKRaftModeEnabled()) {
-            RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
-        }
+        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
         DeploymentUtils.waitTillDepHasRolled(testStorage.getNamespaceName(), testStorage.getEoDeploymentName(), 1, eoPod);
 
@@ -1384,22 +1325,6 @@ void testClusterCACertRenew() {
                 initialKafkaBrokerCertStartTime.compareTo(changedKafkaBrokerCertStartTime) < 0);
         assertThat("Broker certificates end dates have not been renewed.",
                 initialKafkaBrokerCertEndTime.compareTo(changedKafkaBrokerCertEndTime) < 0);
-
-        if (!Environment.isKRaftModeEnabled()) {
-            // Check renewed Zookeeper certificate dates
-            zkCertCreationSecret = kubeClient(testStorage.getNamespaceName()).getSecret(testStorage.getNamespaceName(), testStorage.getClusterName() + "-zookeeper-nodes");
-            zkBrokerCert = SecretUtils.getCertificateFromSecret(zkCertCreationSecret, testStorage.getClusterName() + "-zookeeper-0.crt");
-            Date changedZkCertStartTime = zkBrokerCert.getNotBefore();
-            Date changedZkCertEndTime = zkBrokerCert.getNotAfter();
-
-            LOGGER.info("ZooKeeper cert creation dates: " + initialZkCertStartTime + " --> " + initialZkCertEndTime);
-            LOGGER.info("ZooKeeper cert changed dates:  " + changedZkCertStartTime + " --> " + changedZkCertEndTime);
-
-            assertThat("ZooKeeper certificates start dates have not been renewed.",
-                    initialZkCertStartTime.compareTo(changedZkCertStartTime) < 0);
-            assertThat("ZooKeeper certificates end dates have not been renewed.",
-                    initialZkCertEndTime.compareTo(changedZkCertEndTime) < 0);
-        }
     }
 
     @ParallelNamespaceTest
@@ -1446,14 +1371,11 @@ void testClientsCACertRenew() {
 
         KafkaResource.replaceKafkaResourceInSpecificNamespace(testStorage.getNamespaceName(), testStorage.getClusterName(), k -> k.getSpec().setClientsCa(newClientsCA));
 
-        // On the next reconciliation, the Cluster Operator performs a `rolling update` only for the
-        // `Kafka pods`.
-        // a) ZooKeeper must not roll
-        if (!Environment.isKRaftModeEnabled()) {
-            RollingUpdateUtils.waitForNoRollingUpdate(testStorage.getNamespaceName(), testStorage.getControllerSelector(), controllerPods);
-        }
+        // On the next reconciliation, the Cluster Operator performs a `rolling update`
+        // a) controllers have to roll
+        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, controllerPods);
 
-        // b) Kafka has to roll
+        // b) brokers have to roll
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
 
         // c) EO must roll (because User Operator uses Clients CA for issuing user certificates)
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/security/custom/CustomAuthorizerST.java b/systemtest/src/test/java/io/strimzi/systemtest/security/custom/CustomAuthorizerST.java
index 1d7230d1d99..50549c9f589 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/security/custom/CustomAuthorizerST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/security/custom/CustomAuthorizerST.java
@@ -180,7 +180,7 @@ public void setup() {
                 .editKafka()
                     .addToConfig("auto.create.topics.enable", "true")
                     .withNewKafkaAuthorizationCustom()
-                        .withAuthorizerClass(Environment.isKRaftModeEnabled() ? KafkaAuthorizationSimple.KRAFT_AUTHORIZER_CLASS_NAME : "kafka.security.authorizer.AclAuthorizer")
+                        .withAuthorizerClass(KafkaAuthorizationSimple.KRAFT_AUTHORIZER_CLASS_NAME)
                         .withSupportsAdminApi(true)
                         .withSuperUsers("CN=" + ADMIN)
                     .endKafkaAuthorizationCustom()
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/security/custom/CustomCaST.java b/systemtest/src/test/java/io/strimzi/systemtest/security/custom/CustomCaST.java
index 9b8449e3a13..915af375d10 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/security/custom/CustomCaST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/security/custom/CustomCaST.java
@@ -11,7 +11,6 @@
 import io.strimzi.operator.common.Annotations;
 import io.strimzi.operator.common.model.Ca;
 import io.strimzi.systemtest.AbstractST;
-import io.strimzi.systemtest.Environment;
 import io.strimzi.systemtest.annotations.ParallelNamespaceTest;
 import io.strimzi.systemtest.kafkaclients.internalClients.KafkaClients;
 import io.strimzi.systemtest.resources.NodePoolsConverter;
@@ -51,7 +50,6 @@
 
 import static io.strimzi.systemtest.TestTags.REGRESSION;
 import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;
-import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.jupiter.api.Assertions.assertNotNull;
 
@@ -107,8 +105,8 @@ void testReplacingCustomClusterKeyPairToInvokeRenewalProcess() {
         manuallyRenewCa(testStorage, clusterCa, newClusterCa);
 
         // On the next reconciliation, the Cluster Operator performs a `rolling update`:
-        //      a) ZooKeeper
-        //      b) Kafka
+        //      a) Kafka controllers
+        //      b) Kafka brokers
         //      c) and other components to trust the new CA certificate. (i.e., Entity Operator)
         // When the rolling update is complete, the Cluster Operator
         // will start a new one to generate new server certificates signed by the new CA key.
@@ -153,7 +151,7 @@ void testReplacingCustomClusterKeyPairToInvokeRenewalProcess() {
      * When annotation is removed, kafka resumes and tries to renew clients certificates using the new CA keypair.
      * Note: There is a need to keep an old CA key in the secret and remove it only after all components successfully
      * roll several times (due to the fact that it takes multiple rolling updates to finally update from old keypair to the new one).
-     * Because this test specifically targets clients certificates, EO and ZK must NOT roll, only broker pods should.
+     * Because this test specifically targets clients certificates, EO must NOT roll, only Kafka pods should.
      * Test also verifies communication by producing and consuming messages.
      */
     @ParallelNamespaceTest
@@ -184,13 +182,8 @@ void testReplacingCustomClientsKeyPairToInvokeRenewalProcess() {
         // `Kafka pods`. When the rolling update is complete, the Cluster Operator will start a new one to
         // generate new server certificates signed by the new CA key.
 
-        // Zookeper must not roll
-        if (!Environment.isKRaftModeEnabled()) {
-            RollingUpdateUtils.waitForNoRollingUpdate(testStorage.getNamespaceName(), testStorage.getControllerSelector(), controllerPods);
-            assertThat(RollingUpdateUtils.componentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), controllerPods), is(Boolean.FALSE));
-        }
-
         // Kafka has to roll
+        RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, brokerPods);
         RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
 
         // EO must not roll
@@ -207,7 +200,7 @@ void testReplacingCustomClientsKeyPairToInvokeRenewalProcess() {
      * This tests focuses on verifying the functionality of custom cluster and clients CAs.
      * Steps are following. Before deploying kafka a clients and cluster CAs are created and deployed as secrets.
      * Kafka is then deployed with those, forcing it NOT to generate own certificate authority.
-     * After verification of correct certificates on zookeeper, user certificates are checked for correctness as well.
+     * After verification of correct certificates on user certificates are checked for correctness as well.
      * Then a producer / consumer jos are deployed to verify communication.
      */
     @ParallelNamespaceTest
@@ -254,20 +247,13 @@ void testCustomClusterCaAndClientsCaCertificates() {
             .build()
         );
 
-        LOGGER.info("Check Kafka(s) and ZooKeeper(s) certificates");
+        LOGGER.info("Check Kafka(s) certificates");
         String brokerPodName = kubeClient().listPods(testStorage.getNamespaceName(), testStorage.getBrokerSelector()).get(0).getMetadata().getName();
         final X509Certificate kafkaCert = SecretUtils.getCertificateFromSecret(kubeClient(testStorage.getNamespaceName()).getSecret(testStorage.getNamespaceName(),
             testStorage.getClusterName() + "-kafka-brokers"), brokerPodName + ".crt");
         assertThat("KafkaCert does not have expected test Issuer: " + kafkaCert.getIssuerDN(),
                 SystemTestCertManager.containsAllDN(kafkaCert.getIssuerX500Principal().getName(), clusterCa.getSubjectDn()));
 
-        if (!Environment.isKRaftModeEnabled()) {
-            X509Certificate zookeeperCert = SecretUtils.getCertificateFromSecret(kubeClient(testStorage.getNamespaceName()).getSecret(testStorage.getNamespaceName(),
-                    testStorage.getClusterName() + "-zookeeper-nodes"), testStorage.getClusterName() + "-zookeeper-0.crt");
-            assertThat("ZookeeperCert does not have expected test Subject: " + zookeeperCert.getIssuerDN(),
-                    SystemTestCertManager.containsAllDN(zookeeperCert.getIssuerX500Principal().getName(), clusterCa.getSubjectDn()));
-        }
-
         resourceManager.createResourceWithWait(KafkaTopicTemplates.topic(testStorage).build());
 
         LOGGER.info("Check KafkaUser certificate");
@@ -323,19 +309,6 @@ void testReplaceCustomClusterCACertificateValidityToInvokeRenewalProcess() {
         final Date initialKafkaBrokerCertStartTime = kafkaBrokerCert.getNotBefore();
         final Date initialKafkaBrokerCertEndTime = kafkaBrokerCert.getNotAfter();
 
-        // Check Zookeeper certificate dates
-        Secret zkCertCreationSecret;
-        X509Certificate zkBrokerCert;
-        Date initialZkCertStartTime = null;
-        Date initialZkCertEndTime = null;
-
-        if (!Environment.isKRaftModeEnabled()) {
-            zkCertCreationSecret = kubeClient(testStorage.getNamespaceName()).getSecret(testStorage.getNamespaceName(), testStorage.getClusterName() + "-zookeeper-nodes");
-            zkBrokerCert = SecretUtils.getCertificateFromSecret(zkCertCreationSecret, testStorage.getClusterName() + "-zookeeper-0.crt");
-            initialZkCertStartTime = zkBrokerCert.getNotBefore();
-            initialZkCertEndTime = zkBrokerCert.getNotAfter();
-        }
-
         // Pause Kafka reconciliation
         LOGGER.info("Pause the reconciliation of the Kafka CustomResource ({})", StrimziPodSetResource.getBrokerComponentName(testStorage.getClusterName()));
         KafkaUtils.annotateKafka(testStorage.getNamespaceName(), testStorage.getClusterName(), Collections.singletonMap(Annotations.ANNO_STRIMZI_IO_PAUSE_RECONCILIATION, "true"));
@@ -356,12 +329,10 @@ void testReplaceCustomClusterCACertificateValidityToInvokeRenewalProcess() {
         KafkaUtils.removeAnnotation(testStorage.getNamespaceName(), testStorage.getClusterName(), Annotations.ANNO_STRIMZI_IO_PAUSE_RECONCILIATION);
 
         // On the next reconciliation, the Cluster Operator performs a `rolling update`:
-        //   a) ZooKeeper
-        //   b) Kafka
+        //   a) controllers
+        //   b) brokers
         //   c) and other components to trust the new Cluster CA certificate. (i.e., Entity Operator)
-        if (!Environment.isKRaftModeEnabled()) {
-            RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
-        }
+        RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getControllerSelector(), 3, controllerPods);
         RollingUpdateUtils.waitTillComponentHasRolled(testStorage.getNamespaceName(), testStorage.getBrokerSelector(), 3, brokerPods);
         DeploymentUtils.waitTillDepHasRolled(testStorage.getNamespaceName(), testStorage.getEoDeploymentName(), 1, eoPod);
 
@@ -377,34 +348,16 @@ void testReplaceCustomClusterCACertificateValidityToInvokeRenewalProcess() {
         final Date changedKafkaBrokerCertStartTime = kafkaBrokerCert.getNotBefore();
         final Date changedKafkaBrokerCertEndTime = kafkaBrokerCert.getNotAfter();
 
-        // Check renewed Zookeeper certificate dates
-        Date changedZkCertStartTime = null;
-        Date changedZkCertEndTime = null;
-        if (!Environment.isKRaftModeEnabled()) {
-            zkCertCreationSecret = kubeClient(testStorage.getNamespaceName()).getSecret(testStorage.getNamespaceName(), testStorage.getClusterName() + "-zookeeper-nodes");
-            zkBrokerCert = SecretUtils.getCertificateFromSecret(zkCertCreationSecret, testStorage.getClusterName() + "-zookeeper-0.crt");
-            changedZkCertStartTime = zkBrokerCert.getNotBefore();
-            changedZkCertEndTime = zkBrokerCert.getNotAfter();
-        }
-
         // Print out certificate dates for debug
         LOGGER.info("Initial ClusterCA cert dates: " + initialCertStartTime + " --> " + initialCertEndTime);
         LOGGER.info("Changed ClusterCA cert dates: " + changedCertStartTime + " --> " + changedCertEndTime);
         LOGGER.info("Kafka Broker cert creation dates: " + initialKafkaBrokerCertStartTime + " --> " + initialKafkaBrokerCertEndTime);
         LOGGER.info("Kafka Broker cert changed dates:  " + changedKafkaBrokerCertStartTime + " --> " + changedKafkaBrokerCertEndTime);
-        if (!Environment.isKRaftModeEnabled()) {
-            LOGGER.info("ZooKeeper cert creation dates: " + initialZkCertStartTime + " --> " + initialZkCertEndTime);
-            LOGGER.info("ZooKeeper cert changed dates:  " + changedZkCertStartTime + " --> " + changedZkCertEndTime);
-        }
 
         // Verify renewal result
         assertThat("ClusterCA cert should not have changed start date.", initialCertEndTime.compareTo(changedCertEndTime) == 0);
         assertThat("Broker certificates start dates should have been renewed.", initialKafkaBrokerCertStartTime.compareTo(changedKafkaBrokerCertStartTime) < 0);
         assertThat("Broker certificates end dates should have been renewed.", initialKafkaBrokerCertEndTime.compareTo(changedKafkaBrokerCertEndTime) < 0);
-        if (!Environment.isKRaftModeEnabled()) {
-            assertThat("Zookeeper certificates start dates should have been renewed.", initialZkCertStartTime.compareTo(changedZkCertStartTime) < 0);
-            assertThat("Zookeeper certificates end dates should have been renewed.", initialZkCertEndTime.compareTo(changedZkCertEndTime) < 0);
-        }
     }
 
     /**
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/specific/DrainCleanerST.java b/systemtest/src/test/java/io/strimzi/systemtest/specific/DrainCleanerST.java
index 5cb40e0ef46..2450c85eb63 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/specific/DrainCleanerST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/specific/DrainCleanerST.java
@@ -5,9 +5,7 @@
 package io.strimzi.systemtest.specific;
 
 import io.fabric8.kubernetes.client.KubernetesClientException;
-import io.strimzi.api.kafka.model.kafka.KafkaResources;
 import io.strimzi.systemtest.AbstractST;
-import io.strimzi.systemtest.Environment;
 import io.strimzi.systemtest.TestConstants;
 import io.strimzi.systemtest.annotations.IsolatedTest;
 import io.strimzi.systemtest.annotations.MicroShiftNotSupported;
@@ -74,16 +72,6 @@ void testDrainCleanerWithComponents() {
 
         List<String> brokerPods = kubeClient().listPodNames(TestConstants.DRAIN_CLEANER_NAMESPACE, testStorage.getBrokerSelector());
 
-        if (!Environment.isKRaftModeEnabled()) {
-            String zkPodName = KafkaResources.zookeeperPodName(testStorage.getClusterName(), 0);
-
-            Map<String, String> zkPod = PodUtils.podSnapshot(TestConstants.DRAIN_CLEANER_NAMESPACE, testStorage.getControllerSelector()).entrySet()
-                    .stream().filter(snapshot -> snapshot.getKey().equals(zkPodName)).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
-
-            evictPodWithName(zkPodName);
-            RollingUpdateUtils.waitTillComponentHasRolledAndPodsReady(TestConstants.DRAIN_CLEANER_NAMESPACE, testStorage.getControllerSelector(), replicas, zkPod);
-        }
-
         String kafkaPodName = brokerPods.get(0);
 
         Map<String, String> kafkaPod = PodUtils.podSnapshot(TestConstants.DRAIN_CLEANER_NAMESPACE, testStorage.getBrokerSelector()).entrySet()
diff --git a/systemtest/src/test/java/io/strimzi/systemtest/specific/RackAwarenessST.java b/systemtest/src/test/java/io/strimzi/systemtest/specific/RackAwarenessST.java
index a03cdb9fce0..4beab3df396 100644
--- a/systemtest/src/test/java/io/strimzi/systemtest/specific/RackAwarenessST.java
+++ b/systemtest/src/test/java/io/strimzi/systemtest/specific/RackAwarenessST.java
@@ -238,7 +238,7 @@ void testConnectRackAwareness() {
      * @description This test case verifies that Rack awareness configuration works as expected in KafkaMirrorMaker2 by configuring it and also using given CLuster.
      *
      * @steps
-     *  1. - Deploy target and source Kafka Clusters, both with 1 Kafka and 1 Zookeeper replica.
+     *  1. - Deploy target and source Kafka Clusters, both with 1 broker and 1 controller replica.
      *     - Kafka Clusters and its components are deployed.
      *  2. - Deploy KafkaMirrorMaker2 Cluster with rack configuration set to 'kubernetes.io/hostname'.
      *     - KafkaMirrorMaker2 Cluster is deployed with according configuration in pod affinity, and consumer client rack on a given node.
