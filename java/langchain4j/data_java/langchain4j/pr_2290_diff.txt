diff --git a/docs/docs/integrations/language-models/ollama.md b/docs/docs/integrations/language-models/ollama.md
index 1a8a3daaa8c..039714f589c 100644
--- a/docs/docs/integrations/language-models/ollama.md
+++ b/docs/docs/integrations/language-models/ollama.md
@@ -268,20 +268,22 @@ class OllamaStreamingChatLocalModelTest {
 `OllamaChatModel` and `OllamaStreamingChatModel` classes can be instantiated with the following
 params with the builder pattern:
 
-| Parameter       | Description                                                                                                                                                                       | Type           | Example                |
-|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|------------------------|
-| `baseUrl`       | The base URL of Ollama server.                                                                                                                                                    | `String`       | http://localhost:11434 |
-| `modelName`     | The name of the model to use from Ollama server.                                                                                                                                  | `String`       |                        |
-| `temperature`   | Controls the randomness of the generated responses. Higher values (e.g., 1.0) result in more diverse output, while lower values (e.g., 0.2) produce more deterministic responses. | `Double`       |                        |
-| `topK`          | Specifies the number of highest probability tokens to consider for each step during generation.                                                                                   | `Integer`      |                        |
-| `topP`          | Controls the diversity of the generated responses by setting a threshold for the cumulative probability of top tokens.                                                            | `Double`       |                        |
-| `repeatPenalty` | Penalizes the model for repeating similar tokens in the generated output.                                                                                                         | `Double`       |                        |
-| `seed`          | Sets the random seed for reproducibility of generated responses.                                                                                                                  | `Integer`      |                        |
-| `numPredict`    | The number of predictions to generate for each input prompt.                                                                                                                      | `Integer`      |                        |
-| `stop`          | A list of strings that, if generated, will mark the end of the response.                                                                                                          | `List<String>` |                        |
-| `format`        | The desired format for the generated output.                                                                                                                                      | `String`       |                        |
-| `timeout`       | The maximum time allowed for the API call to complete.                                                                                                                            | `Duration`     | PT60S                  |
-| `maxRetries`    | The maximum number of retries in case of API call failure.                                                                                                                        | `Integer`      |                        |
+| Parameter        | Description                                                                                                                                                                       | Type             | Example                |
+|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|------------------------|
+| `baseUrl`        | The base URL of Ollama server.                                                                                                                                                    | `String`         | http://localhost:11434 |
+| `modelName`      | The name of the model to use from Ollama server.                                                                                                                                  | `String`         |                        |
+| `temperature`    | Controls the randomness of the generated responses. Higher values (e.g., 1.0) result in more diverse output, while lower values (e.g., 0.2) produce more deterministic responses. | `Double`         |                        |
+| `topK`           | Specifies the number of highest probability tokens to consider for each step during generation.                                                                                   | `Integer`        |                        |
+| `topP`           | Controls the diversity of the generated responses by setting a threshold for the cumulative probability of top tokens.                                                            | `Double`         |                        |
+| `repeatPenalty`  | Penalizes the model for repeating similar tokens in the generated output.                                                                                                         | `Double`         |                        |
+| `seed`           | Sets the random seed for reproducibility of generated responses.                                                                                                                  | `Integer`        |                        |
+| `numPredict`     | The number of predictions to generate for each input prompt.                                                                                                                      | `Integer`        |                        |
+| `stop`           | A list of strings that, if generated, will mark the end of the response.                                                                                                          | `List<String>`   |                        |
+| `format`         | The desired format for the generated output. (**Depracated** see **responseFormat**)                                                                                              | `String`         |                        |
+| `responseFormat` | The desired format for the generated output. TEXT or JSON with optional JSON Schema definition                                                                                    | `ResponseFormat` |                        |
+| `supportedCapabilities` | Set of model capabilities used by `AiServices` API (only `OllamaChatModel` supported)                                                                                             | `Capability` | RESPONSE_FORMAT_JSON_SCHEMA |
+| `timeout`        | The maximum time allowed for the API call to complete.                                                                                                                            | `Duration`       | PT60S                  |
+| `maxRetries`     | The maximum number of retries in case of API call failure.                                                                                                                        | `Integer`        |                        |
 
 #### Usage Example
 ```java
@@ -300,3 +302,112 @@ langchain4j.ollama.chat-model.model-name=llama3.1
 langchain4j.ollama.chat-model.temperature=0.8
 langchain4j.ollama.chat-model.timeout=PT60S
 ```
+
+### JSON mode
+
+#### JSON mode using builder
+
+```java
+OllamaChatModel ollamaChatModel = OllamaChatModel.builder()
+    .baseUrl("http://localhost:11434")
+    .modelName("llama3.1")
+    .responseFormat(ResponseFormat.JSON)    
+    .temperature(0.8)
+    .timeout(Duration.ofSeconds(60))
+    .build();
+```
+
+#### JSON mode using builder *deprecated*
+
+```java
+OllamaChatModel ollamaChatModel = OllamaChatModel.builder()
+    .baseUrl("http://localhost:11434")
+    .modelName("llama3.1")
+    .format("json")    
+    .temperature(0.8)
+    .timeout(Duration.ofSeconds(60))
+    .build();
+```
+
+### Structured outputs
+
+#### JSON schema definition using builder
+
+```java
+OllamaChatModel ollamaChatModel = OllamaChatModel.builder()
+    .baseUrl("http://localhost:11434")
+    .modelName("llama3.1")
+    .responseFormat(ResponseFormat.builder()
+            .type(ResponseFormatType.JSON)
+            .jsonSchema(JsonSchema.builder().rootElement(JsonObjectSchema.builder()
+                            .addProperty("name", JsonStringSchema.builder().build())
+                            .addProperty("capital", JsonStringSchema.builder().build())
+                            .addProperty(
+                                    "languages",
+                                    JsonArraySchema.builder()
+                                            .items(JsonStringSchema.builder().build())
+                                            .build())
+                            .required("name", "capital", "languages")
+                            .build())
+                    .build())
+            .build())
+    .temperature(0.8)
+    .timeout(Duration.ofSeconds(60))
+    .build();
+```
+
+#### JSON Schema using ChatRequest API
+
+```java
+OllamaChatModel ollamaChatModel = OllamaChatModel.builder()
+    .baseUrl("http://localhost:11434")
+    .modelName("llama3.1")
+    .build();
+
+ChatResponse chatResponse = ollamaChatModel.chat(ChatRequest.builder()
+        .messages(userMessage("Tell me about Canada."))
+        .responseFormat(ResponseFormat.builder()
+                .type(ResponseFormatType.JSON)
+                .jsonSchema(JsonSchema.builder().rootElement(JsonObjectSchema.builder()
+                                .addProperty("name", JsonStringSchema.builder().build())
+                                .addProperty("capital", JsonStringSchema.builder().build())
+                                .addProperty(
+                                        "languages",
+                                        JsonArraySchema.builder()
+                                                .items(JsonStringSchema.builder().build())
+                                                .build())
+                                .required("name", "capital", "languages")
+                                .build())
+                        .build())
+                .build())
+        .build());
+
+String jsonFormattedResponse = chatResponse.aiMessage().text();
+
+/* jsonFormattedResponse value:
+
+  {
+    "capital" : "Ottawa",
+    "languages" : [ "English", "French" ],
+    "name" : "Canada"
+  }
+
+ */
+
+
+```
+
+
+### Json Schema with AiServices
+
+When `OllamaChatModel` is created with supported capability `RESPONSE_FORMAT_JSON_SCHEMA`, `AIService` will automatically generate schema from interface return value. More about it in [Structured Outputs](/tutorials/structured-outputs.md#using-json-schema-with-ai-services)
+
+```java
+OllamaChatModel ollamaChatModel = OllamaChatModel.builder()
+    .baseUrl("...")
+    .modelName("...")
+    .supportedCapabilities(RESPONSE_FORMAT_JSON_SCHEMA)    
+    .build();
+```
+
+
