**Fixes for OpenAI Streaming Token Counting:**

1. **Bug Description:**
   - The token usage logged in callbacks for OpenAI streaming was not matching the usage reported by the OpenAI API.

2. **Key Changes:**
   - Ensure that logging callbacks use the `usage` data returned from the OpenAI API during streaming.
   - Always include the `include_usage` parameter in requests to the OpenAI API to ensure accurate token usage tracking, even if not explicitly set by users.

3. **Testing:**
   - Implement unit tests to verify the correct tracking of token usage in various scenarios:
     - When `stream_options={"include_usage": True}`.
     - Without setting `include_usage`.
     - When message logging is disabled (`litellm.turn_off_message_logging=True`).

4. **Type:**
   - New Feature
   - Test

5. **Action Required:**
   - Verify that the new tests cover all pathways and scenarios related to token usage tracking to confirm accuracy and reliability of the logging callbacks.