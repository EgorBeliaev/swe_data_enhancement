[
    {
        "title": "Fix passing top_k parameter for Bedrock Anthropic models"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "## Title\r\n\r\n<!-- e.g. \"Implement user authentication feature\" -->\r\nFix passing top_k parameter for Bedrock Anthropic models\r\n\r\n## Relevant issues\r\n\r\n<!-- e.g. \"Fixes #000\" -->\r\nFixes #7782 \r\n\r\n## Type\r\n\r\n<!-- Select the type of Pull Request -->\r\n<!-- Keep only the necessary ones -->\r\n\r\n\ud83d\udc1b Bug Fix\r\n\u2705 Test\r\n\r\n## Changes\r\n\r\n<!-- List of changes -->\r\nThe bug arose from the fact that different bedrock models pass in the top_k parameter in different ways.\r\n\r\nSpecifically the nova model passes in the parameter through \r\n```\r\nadditionalModelRequestFields = {\r\n    \"inferenceConfig\": {\r\n         \"topK\": 20\r\n    }\r\n}\r\n```\r\n\r\nand the anthropic model passes it in through\r\n\r\n```\r\nadditional_model_fields = {\"top_k\": top_k}\r\n```\r\n\r\nThis PR checks the model types and sets the parameter based on that. Right now, this is a simple if statement, but a long term fix might be to create a new class for each model, and create handling logic for supported / model-specific param within each class. This way the overall converse handler does not need to know about these specifics.\r\n\r\nThis PR also simplifies the additional_model_fields creation logic \r\n\r\n## [REQUIRED] Testing - Attach a screenshot of any new tests passing locally\r\nIf UI changes, send a screenshot/GIF of working UI fixes\r\n\r\n<!-- Test procedure -->\r\nTested the top_k param for 4 different models w/ real API calls:\r\n![image](https://github.com/user-attachments/assets/59ec88da-501a-4367-9526-dd091b74ead1)\r\n\r\nTests still passed w/ mocks:\r\n![image](https://github.com/user-attachments/assets/a4194ed3-f225-4e38-9e18-a7e6f9902939)\r\n"
    },
    {
        "author": {
            "login": "ishaan-jaff"
        },
        "body": "LGTM overall, just a couple minor points of feedback "
    },
    {
        "author": {
            "login": "krrishdholakia"
        },
        "body": ""
    },
    {
        "author": {
            "login": "vibhavbhat"
        },
        "body": ""
    },
    {
        "author": {
            "login": "vibhavbhat"
        },
        "body": ""
    },
    {
        "author": {
            "login": "spacesphere"
        },
        "body": ""
    },
    {
        "author": {
            "login": "spacesphere"
        },
        "body": ""
    },
    {
        "author": {
            "login": "spacesphere"
        },
        "body": ""
    },
    {
        "author": {
            "login": "krrishdholakia"
        },
        "body": ""
    },
    {
        "author": {
            "login": "ishaan-jaff"
        },
        "body": "lgtm, merging into staging branch to run through testing "
    },
    {
        "author": {
            "login": "vercel"
        },
        "body": "[vc]: #nen4oIIe3es2BD41dVQDcp8k2IrdKJvHdCpMUFYZSAw=:eyJpc01vbm9yZXBvIjp0cnVlLCJ0eXBlIjoiZ2l0aHViIiwicHJvamVjdHMiOlt7Im5hbWUiOiJsaXRlbGxtIiwicm9vdERpcmVjdG9yeSI6ImRvY3MvbXktd2Vic2l0ZSIsImluc3BlY3RvclVybCI6Imh0dHBzOi8vdmVyY2VsLmNvbS9jbGVya2llYWkvbGl0ZWxsbS9FVG4ybkdYZHpuZUg3THVSQ2daTUNrYll4N1ZhIiwicHJldmlld1VybCI6ImxpdGVsbG0tZ2l0LWZvcmstdmliaGF2YmhhdC1maXhhbnRocm9waWN0b3BrYnVnLWNsZXJraWVhaS52ZXJjZWwuYXBwIiwibmV4dENvbW1pdFN0YXR1cyI6IkRFUExPWUVEIiwibGl2ZUZlZWRiYWNrIjp7InJlc29sdmVkIjowLCJ1bnJlc29sdmVkIjowLCJ0b3RhbCI6MCwibGluayI6ImxpdGVsbG0tZ2l0LWZvcmstdmliaGF2YmhhdC1maXhhbnRocm9waWN0b3BrYnVnLWNsZXJraWVhaS52ZXJjZWwuYXBwIn19XX0=\n**The latest updates on your projects**. Learn more about [Vercel for Git \u2197\ufe0e](https://vercel.link/github-learn-more)\n\n| Name | Status | Preview | Comments | Updated (UTC) |\n| :--- | :----- | :------ | :------- | :------ |\n| **litellm** | \u2705 Ready ([Inspect](https://vercel.com/clerkieai/litellm/ETn2nGXdzneH7LuRCgZMCkbYx7Va)) | [Visit Preview](https://litellm-git-fork-vibhavbhat-fixanthropictopkbug-clerkieai.vercel.app) | \ud83d\udcac [**Add feedback**](https://vercel.live/open-feedback/litellm-git-fork-vibhavbhat-fixanthropictopkbug-clerkieai.vercel.app?via=pr-comment-feedback-link) | Feb 4, 2025 4:36am |\n\n"
    },
    {
        "author": {
            "login": "krrishdholakia"
        },
        "body": "Is this okay to merge? @ishaan-jaff \r\n"
    },
    {
        "author": {
            "login": "vibhavbhat"
        },
        "body": "@ishaan-jaff Just following up to see if this is fine to merge"
    },
    {
        "data": {
            "repository": {
                "issue": null
            }
        },
        "errors": [
            {
                "type": "NOT_FOUND",
                "path": [
                    "repository",
                    "issue"
                ],
                "locations": [
                    {
                        "line": 4,
                        "column": 9
                    }
                ],
                "message": "Could not resolve to an Issue with the number of 0."
            }
        ]
    },
    {
        "data": {
            "repository": {
                "issue": {
                    "title": "[Bug]: Cannot pass provider-specific parameters to Bedrock Anthropic models",
                    "body": "### What happened?\n\nPassing the `top_k` parameter to an Anthropic model in the Bedrock platform throws this error:\r\n```BadRequestError: litellm.BadRequestError: BedrockException - {\"message\":\"The model returned the following errors: Malformed input request: #: extraneous key [inferenceConfig] is not permitted, please reformat your input and try again.\"}```\r\n\r\nI think the issue is with the Bedrock Converse API, sometimes the additional parameters are passed in this way ([from the API docs](https://docs.aws.amazon.com/nova/latest/userguide/using-converse-api.html)):\r\n```\r\nadditionalModelRequestFields = {\r\n    \"inferenceConfig\": {\r\n         \"topK\": 20\r\n    }\r\n}\r\n\r\nmodel_response = client.converse(\r\n    modelId=\"us.amazon.nova-lite-v1:0\", \r\n    messages=messages, \r\n    system=system, \r\n    inferenceConfig=inf_params,\r\n    additionalModelRequestFields=additionalModelRequestFields\r\n)\r\n```\r\n\r\nSometimes it's passed without the `inferenceConfig` key, such as this example in the [API docs](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-examples.html):\r\n```    \r\nadditional_model_fields = {\"top_k\": top_k}\r\n\r\n    # Send the message.\r\n    response = bedrock_client.converse(\r\n        modelId=model_id,\r\n        messages=messages,\r\n        system=system_prompts,\r\n        inferenceConfig=inference_config,\r\n        additionalModelRequestFields=additional_model_fields\r\n    )\r\n```\r\n\r\nI managed to fix the error for Anthropic by changing the litellm code [here](https://github.com/BerriAI/litellm/blob/fe60a38c8e43e908f44d8c668a5ba9fae1dca762/litellm/llms/bedrock/chat/converse_transformation.py#L381) to this:\r\n```\r\n      if \"topK\" in inference_params:\r\n            additional_request_params = {\r\n                \"top_k\": inference_params.pop(\"topK\")\r\n            }\r\n        elif \"top_k\" in inference_params:\r\n            additional_request_params = {\r\n                \"top_k\": inference_params.pop(\"top_k\")\r\n            }\r\n```\r\nHowever if you do that, then it stops working for the Nova model. So I'm not sure what the solution is here!\r\n\r\n\n\n### Relevant log output\n\n_No response_\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.58.2\n\n### Twitter / LinkedIn details\n\n_No response_",
                    "state": "CLOSED",
                    "comments": {
                        "nodes": [
                            {
                                "author": {
                                    "login": "krrishdholakia"
                                },
                                "body": "> However if you do that, then it stops working for the Nova model. So I'm not sure what the solution is here!\n\nThank you for this ticket @mrm1001  - what's the error thrown by nova? @mrm1001 "
                            },
                            {
                                "author": {
                                    "login": "ishaan-jaff"
                                },
                                "body": "fixed in #8131  by @vibhavbhat "
                            }
                        ]
                    }
                }
            }
        }
    }
]