[
    {
        "title": "(Fixes) OpenAI Streaming Token Counting + Fixes usage track when `litellm.turn_off_message_logging=True`"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "## (Fixes) OpenAI Streaming Token Counting + Fixes usage track when `litellm.turn_off_message_logging=True`\r\n\r\n## Bug Description\r\n- For OpenAI streaming the Usage tracked in callbacks was != Usage from OpenAI API \r\n\r\n## Key changes made\r\n- For streaming ensure logging callbacks use `usage` from OpenAI api response \r\n- When users don't send `include_usage` to OpenAI API, litellm will still pass `include_usage` to OpenAI APIs to guarantee accurate token counting \r\n- Unit testing for both scenarios \r\n\r\n\r\n### Test Cases\r\n\r\n1. `test_stream_token_counting_gpt_4o`\r\n   - Tests if token usage tracking works correctly when `stream_options={\"include_usage\": True}`\r\n   - Verifies that the logging callback records the same usage metrics as returned by OpenAI API\r\n\r\n2. `test_stream_token_counting_without_include_usage`\r\n   - Tests token usage tracking without explicitly setting `include_usage`\r\n   - Verifies that the logging callback records the same usage metrics as returned by OpenAI API\r\n\r\n3. `test_stream_token_counting_with_redaction`\r\n   - Tests token usage tracking when message logging is disabled (`litellm.turn_off_message_logging=True`)\r\n   -  Verifies that the logging callback records the same usage metrics as returned by OpenAI API\r\n<!-- e.g. \"Implement user authentication feature\" -->\r\n\r\n## Relevant issues\r\n\r\n<!-- e.g. \"Fixes #000\" -->\r\n\r\n## Type\r\n\r\n<!-- Select the type of Pull Request -->\r\n<!-- Keep only the necessary ones -->\r\n\r\n\ud83c\udd95 New Feature\r\n\u2705 Test\r\n\r\n## Changes\r\n\r\n<!-- List of changes -->\r\n\r\n## [REQUIRED] Testing - Attach a screenshot of any new tests passing locally\r\nIf UI changes, send a screenshot/GIF of working UI fixes\r\n\r\n<!-- Test procedure -->\r\n\r\n"
    },
    {
        "author": {
            "login": "github-advanced-security"
        },
        "body": ""
    },
    {
        "author": {
            "login": "vercel"
        },
        "body": "[vc]: #i9n77mCje7cYwWjuI3+Jl93n1PBBDPIj5dHku8JR/Q4=:eyJpc01vbm9yZXBvIjp0cnVlLCJ0eXBlIjoiZ2l0aHViIiwicHJvamVjdHMiOlt7Im5hbWUiOiJsaXRlbGxtIiwiaW5zcGVjdG9yVXJsIjoiaHR0cHM6Ly92ZXJjZWwuY29tL2NsZXJraWVhaS9saXRlbGxtLzdjSlBta250YzR4TkdYcnlYNDhEdVRIZHp2aGMiLCJwcmV2aWV3VXJsIjoibGl0ZWxsbS1naXQtbGl0ZWxsbXN0cmVhbWluZ3VzYWdlYXBwcm9hY2gyLWNsZXJraWVhaS52ZXJjZWwuYXBwIiwibmV4dENvbW1pdFN0YXR1cyI6IkRFUExPWUVEIiwibGl2ZUZlZWRiYWNrIjp7InJlc29sdmVkIjowLCJ1bnJlc29sdmVkIjowLCJ0b3RhbCI6MCwibGluayI6ImxpdGVsbG0tZ2l0LWxpdGVsbG1zdHJlYW1pbmd1c2FnZWFwcHJvYWNoMi1jbGVya2llYWkudmVyY2VsLmFwcCJ9LCJyb290RGlyZWN0b3J5IjoiZG9jcy9teS13ZWJzaXRlIn1dfQ==\n**The latest updates on your projects**. Learn more about [Vercel for Git \u2197\ufe0e](https://vercel.link/github-learn-more)\n\n| Name | Status | Preview | Comments | Updated (UTC) |\n| :--- | :----- | :------ | :------- | :------ |\n| **litellm** | \u2705 Ready ([Inspect](https://vercel.com/clerkieai/litellm/7cJPmkntc4xNGXryX48DuTHdzvhc)) | [Visit Preview](https://litellm-git-litellmstreamingusageapproach2-clerkieai.vercel.app) | \ud83d\udcac [**Add feedback**](https://vercel.live/open-feedback/litellm-git-litellmstreamingusageapproach2-clerkieai.vercel.app?via=pr-comment-feedback-link) | Jan 31, 2025 10:55pm |\n\n"
    },
    {
        "author": {
            "login": "codecov"
        },
        "body": "## [Codecov](https://app.codecov.io/gh/BerriAI/litellm/pull/8156?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=BerriAI) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n\n:loudspeaker: Thoughts on this report? [Let us know!](https://github.com/codecov/feedback/issues/255)"
    },
    {
        "data": {
            "repository": {
                "issue": null
            }
        },
        "errors": [
            {
                "type": "NOT_FOUND",
                "path": [
                    "repository",
                    "issue"
                ],
                "locations": [
                    {
                        "line": 4,
                        "column": 9
                    }
                ],
                "message": "Could not resolve to an Issue with the number of 0."
            }
        ]
    }
]