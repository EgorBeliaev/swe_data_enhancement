[
    {
        "title": "fix sd2 switching"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "Closes: https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/13763\r\nI start webui with not sd 2.1 model, then I try to load sd 2.1 checkpoint, and I get \"NotImplementedError\"\r\nIf I've started webui with sd2 model in `--ckpt` flag, it works, even if I switch to other and back to sd2\r\n\r\nThis bug appeared in October, now I think it's connected with `device = devices.cpu` in `is_using_v_parameterization_for_sd2` inside this commit https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/d04e3e921e8ee71442a1f4a1d6e91c05b8238007#diff-b710a9b8e9fbcc5bc5a014f938c9c74564c1dcfc86929f0dc9ff643ba3fe7873R30\r\n\r\n\r\n> NotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs:\r\n>      query       : shape=(1, 64, 5, 64) (torch.float32)\r\n>      key         : shape=(1, 64, 5, 64) (torch.float32)\r\n>      value       : shape=(1, 64, 5, 64) (torch.float32)\r\n>      attn_bias   : <class 'NoneType'>\r\n>      p           : 0.0\r\n>\r\n> `decoderF` is not supported because:\r\n>     *device=cpu (supported: {'cuda'})*\r\n>     attn_bias type is <class 'NoneType'>\r\n>\r\n> `flshattF@v2.3.6` is not supported because:\r\n>     *device=cpu (supported: {'cuda'})*\r\n>     dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\r\n>\r\n> `tritonflashattF` is not supported because:\r\n>     *device=cpu (supported: {'cuda'})*\r\n>     dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\r\n>     operator wasn't built - see `python -m xformers.info` for more info\r\n>     triton is not available\r\n>     Only work on pre-MLIR triton for now\r\n>\r\n> `cutlassF` is not supported because:\r\n>     *device=cpu (supported: {'cuda'})*\r\n>\r\n> `smallkF` is not supported because:\r\n>     max(query.shape[-1] != value.shape[-1]) > 32\r\n>     *device=cpu (supported: {'cuda'})*\r\n>     unsupported embed per head: 64\r\n\r\nAfter this patch the bug is gone for me\r\n\r\n## Checklist:\r\n\r\n- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)\r\n- [x] I have performed a self-review of my own code\r\n- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)\r\n- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)\r\n"
    },
    {
        "author": {
            "login": "jetjodh"
        },
        "body": ""
    },
    {
        "author": {
            "login": "AUTOMATIC1111"
        },
        "body": ""
    },
    {
        "author": {
            "login": "AUTOMATIC1111"
        },
        "body": "The reason I didn't want to run the calculation on GPU, is that it takes substantially longer to transfer the model to GPU than to just do one inference on CPU. I merged the other one."
    },
    {
        "author": {
            "login": "light-and-ray"
        },
        "body": "> I merged the other one.\n\nWhich? The hcl's solves the other issue"
    },
    {
        "author": {
            "login": "light-and-ray"
        },
        "body": "> The reason I didn't want to run the calculation on GPU\n\nBut it doesn't work, at least not for everyone. Read the description "
    }
]