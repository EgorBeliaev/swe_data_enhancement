**Instructions for Programmer:**

1. **Timestep Embedding Optimization:**
   - Remove the blocking operation by avoiding the creation of a tensor on CPU and subsequent transfer to GPU. This change should eliminate GPU sync per step.

2. **SpatialTransformer.forward Optimization:**
   - Replace `einops` operations with native PyTorch `reshape`, `view`, and `permute` operations.
   - Remove the `.contiguous()` call.
   - This update is expected to reduce calls to `aten::copy_` and enhance performance by approximately 6-8 ms per forward pass, based on profiling (512x512, batch size 4, using an overclocked RTX 3090).

**Ensure that:**
- Changes adopted align with the repositoryâ€™s contributing guidelines and have passed all tests.