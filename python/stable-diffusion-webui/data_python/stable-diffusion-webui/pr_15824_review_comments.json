[
    {
        "title": "[Performance] LDM optimization patches"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "## Description\r\n\r\nChange 1: Timestep Embedding Patch\r\n* Fixes a blocking op in the timestep embedding.  It was creating a tensor on CPU and then moving it to GPU, which would force a sync every step.\r\n* Combined with the other performance PRs (mine and HCL's), Torch's dispatch queue should be completely unblocked (until extensions with similar problems mess it up).  This will allow near constant 100% GPU usage.\r\n\r\nChange 2: SpatialTransformer.forward einops removal\r\n* Changes the function to use native torch reshape/view/permute ops and removes the `.contiguous()` call.\r\n* Prevents 32 calls to `aten::copy_` and `void at::native::elementwise_kernel<128, 4, at::nati...` per forward pass (SD 1.5).  Speedup seems to be around 6-8 ms per forward, but my profiler is being a little inconsistent with the timing (512x512, batch 4, overclocked 3090)\r\n\r\n## Checklist:\r\n\r\n- [x] I have read [contributing wiki page](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)\r\n- [x] I have performed a self-review of my own code\r\n- [x] My code follows the [style guidelines](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing#code-style)\r\n- [x] My code passes [tests](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Tests)\r\n"
    },
    {
        "author": {
            "login": "AUTOMATIC1111"
        },
        "body": ""
    },
    {
        "author": {
            "login": "drhead"
        },
        "body": "I think #18620 might need to be merged before tests will pass on this."
    },
    {
        "author": {
            "login": "w-e-w"
        },
        "body": "- we are currently on #15824\r\n\r\nso we need to wait 2769 new posts to merge this \ud83d\ude43"
    },
    {
        "author": {
            "login": "drhead"
        },
        "body": "Upon further review I think it would be sufficient for #15820 to be merged first lol"
    },
    {
        "author": {
            "login": "drhead"
        },
        "body": "Added another patch, and it passes tests now."
    }
]