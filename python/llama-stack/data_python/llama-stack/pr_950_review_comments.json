[
    {
        "title": "fix: Ensure a better error stack trace when llama-stack is not built"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "# What does this PR do?\r\n\r\ncurrently this is the output when you run a distribution locally without running `llama stack build`:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/charliedoern/Documents/llama-sdk.py\", line 25, in <module>\r\n    models = client.models.list()\r\n             ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/charliedoern/Documents/llama-stack-client-python/src/llama_stack_client/resources/models.py\", line 107, in list\r\n    raise exc\r\n  File \"/Users/charliedoern/Documents/llama-stack-client-python/src/llama_stack_client/resources/models.py\", line 95, in list\r\n    return self._get(\r\n           ^^^^^^^^^^\r\n  File \"/Users/charliedoern/Documents/llama-stack-client-python/src/llama_stack_client/_base_client.py\", line 1212, in get\r\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/charliedoern/Documents/llama-stack/llama_stack/distribution/library_client.py\", line 168, in request\r\n    return asyncio.run(self.async_client.request(*args, **kwargs))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 190, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/Users/charliedoern/Documents/llama-stack/llama_stack/distribution/library_client.py\", line 258, in request\r\n    if not self.endpoint_impls:\r\n           ^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'AsyncLlamaStackAsLibraryClient' object has no attribute 'endpoint_impls'\r\n```\r\n\r\nthe intended exception is never raised, add an except for an AttributeError so users can catch when they call things like `models.list()` and so that a more useful error telling them that the client is not properly initialized is printed. \r\n\r\n## Test Plan\r\n\r\nPlease describe:\r\n - I ran the script found here: https://llama-stack.readthedocs.io/en/latest/getting_started/index.html#run-inference-with-python-sdk locally with the changes in this PR and the exception was caught successfully. \r\n\r\n## Before submitting\r\n\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Ran pre-commit to handle lint / formatting issues.\r\n- [ ] Read the [contributor guideline](https://github.com/meta-llama/llama-stack/blob/main/CONTRIBUTING.md),\r\n      Pull Request section?\r\n- [ ] Updated relevant documentation.\r\n- [ ] Wrote necessary unit or integration tests.\r\n"
    },
    {
        "author": {
            "login": "terrytangyuan"
        },
        "body": "Thanks! Could you sign the individual CLA?"
    },
    {
        "author": {
            "login": "nathan-weinberg"
        },
        "body": "LGTM"
    },
    {
        "author": {
            "login": "raghotham"
        },
        "body": ""
    },
    {
        "author": {
            "login": "ashwinb"
        },
        "body": ""
    },
    {
        "author": {
            "login": "ashwinb"
        },
        "body": ""
    },
    {
        "author": {
            "login": "ashwinb"
        },
        "body": ""
    },
    {
        "author": {
            "login": "facebook-github-bot"
        },
        "body": "Hi @cdoern! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20meta-llama%2Fllama-stack%20%23950). Thanks!"
    },
    {
        "author": {
            "login": "facebook-github-bot"
        },
        "body": "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    },
    {
        "author": {
            "login": "cdoern"
        },
        "body": "@terrytangyuan @ashwinb @raghotham is this good to go?"
    }
]