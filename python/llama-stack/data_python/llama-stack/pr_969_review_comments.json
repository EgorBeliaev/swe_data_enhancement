[
    {
        "title": "fix: List providers command prints out non-existing APIs from registry. Fixes #966"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "Fixes #966.\r\n\r\nVerified that:\r\n1. Correct list of APIs are printed out when running `llama stack list-providers`\r\n2. `llama stack list-providers <api>` works as expected."
    },
    {
        "author": {
            "login": "ashwinb"
        },
        "body": ""
    },
    {
        "author": {
            "login": "terrytangyuan"
        },
        "body": ""
    },
    {
        "author": {
            "login": "ashwinb"
        },
        "body": "See comment inline"
    },
    {
        "author": {
            "login": "terrytangyuan"
        },
        "body": ""
    },
    {
        "author": {
            "login": "ashwinb"
        },
        "body": "yay nice I didn't realize we already _had_ a `providable_apis` function hoho"
    },
    {
        "author": {
            "login": "raghotham"
        },
        "body": "Can you share sample outputs for the commands?"
    },
    {
        "author": {
            "login": "terrytangyuan"
        },
        "body": "```\r\n> llama stack list-providers \r\nusage: llama stack list-providers [-h] {agents,datasetio,eval,inference,post_training,safety,scoring,telemetry,tool_runtime,vector_io}\r\nllama stack list-providers: error: the following arguments are required: api\r\n```\r\n\r\n```\r\n> llama stack list-providers inference\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| Provider Type                    | PIP Package Dependencies                                                         |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| inline::meta-reference           | accelerate,blobfile,fairscale,torch,torchvision,transformers,zmq,lm-format-      |\r\n|                                  | enforcer,sentence-transformers                                                   |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| inline::meta-reference-quantized | accelerate,blobfile,fairscale,torch,torchvision,transformers,zmq,lm-format-      |\r\n|                                  | enforcer,sentence-transformers,fbgemm-gpu,torchao==0.5.0                         |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| inline::vllm                     | vllm                                                                             |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| inline::sentence-transformers    | sentence-transformers                                                            |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::sample                   |                                                                                  |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::cerebras                 | cerebras_cloud_sdk                                                               |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::ollama                   | ollama,aiohttp                                                                   |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::vllm                     | openai                                                                           |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::tgi                      | huggingface_hub,aiohttp                                                          |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::hf::serverless           | huggingface_hub,aiohttp                                                          |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::hf::endpoint             | huggingface_hub,aiohttp                                                          |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::fireworks                | fireworks-ai                                                                     |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::together                 | together                                                                         |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::groq                     | groq                                                                             |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::bedrock                  | boto3                                                                            |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::databricks               | openai                                                                           |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::nvidia                   | openai                                                                           |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::runpod                   | openai                                                                           |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n| remote::sambanova                | openai                                                                           |\r\n+----------------------------------+----------------------------------------------------------------------------------+\r\n\r\n```\r\n\r\n```\r\n> llama stack list-providers telemetry\r\n+------------------------+----------------------------------------------------------+\r\n| Provider Type          | PIP Package Dependencies                                 |\r\n+------------------------+----------------------------------------------------------+\r\n| inline::meta-reference | opentelemetry-sdk,opentelemetry-exporter-otlp-proto-http |\r\n+------------------------+----------------------------------------------------------+\r\n| remote::sample         |                                                          |\r\n+------------------------+----------------------------------------------------------+\r\n\r\n```"
    },
    {
        "data": {
            "repository": {
                "issue": {
                    "title": "list-providers does not support all the APIs",
                    "body": "### System Info\n\nOutput of `python -m \"torch.utils.collect_env\"`:\n```\nPyTorch version: 2.6.0\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: macOS 15.3 (arm64)\nGCC version: Could not collect\nClang version: 16.0.0 (clang-1600.0.26.6)\nCMake version: Could not collect\nLibc version: N/A\n\nPython version: 3.10.16 (main, Dec 11 2024, 10:22:29) [Clang 14.0.6 ] (64-bit runtime)\nPython platform: macOS-15.3-arm64-arm-64bit\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nApple M3 Pro\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.6.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n```\nOther info:\n```\n% llama-stack-client --version\nllama-stack-client, version 0.1.1\n```\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### \ud83d\udc1b Describe the bug\n\n**Setup:**\nFresh `llama` install, using the `ollama` template with:\n```console\nllama stack build --template ollama --image-type venv\n```\n\n**Steps:**\nFirst verify `list-providers` CLI with no `api` parameter:\n```console\n% llama stack list-providers\nusage: llama stack list-providers [-h]\n                                  {inference,safety,agents,vector_io,datasetio,scoring,eval,post_training,tool_runtime,telemetry,models,shields,vector_dbs,datasets,scoring_functions,eval_tasks,tool_groups,inspect}\nllama stack list-providers: error: the following arguments are required: api\n```\nThen, use some of the api values proposed in the help message, like `models` and `vector_dbs`, an error is returned.\n\n**Notes**: Tried the same commands with the `together.ai` provider, using the [Llama 3.2 course on Deeplearning.ai](https://learn.deeplearning.ai/courses/introducing-multimodal-llama-3-2/lesson/8/llama-stack), which traised the same error.\n\n### Error logs\n\n```console\n % llama stack list-providers models\nTraceback (most recent call last):\n  File \"/Users/dmartino/miniconda3/envs/stack/bin/llama\", line 8, in <module>\n    sys.exit(main())\n  File \"/Users/dmartino/miniconda3/envs/stack/lib/python3.10/site-packages/llama_stack/cli/llama.py\", line 46, in main\n    parser.run(args)\n  File \"/Users/dmartino/miniconda3/envs/stack/lib/python3.10/site-packages/llama_stack/cli/llama.py\", line 40, in run\n    args.func(args)\n  File \"/Users/dmartino/miniconda3/envs/stack/lib/python3.10/site-packages/llama_stack/cli/stack/list_providers.py\", line 40, in _run_providers_list_cmd\n    providers_for_api = all_providers[Api(args.api)]\nKeyError: <Api.models: 'models'>\n```\n\n### Expected behavior\n\nThe list of providers for the given api should be displayed.\nIn alternative, consider adding a proper error message and remove the stack trace, by adding a check [here](https://github.com/meta-llama/llama-stack/blob/529708215c5ad54e1ef41ba3e68d3a2af8d563b0/llama_stack/cli/stack/list_providers.py#L40):\n```py\n        all_providers = get_provider_registry()\n        if Api(args.api) not in all_providers:\n            print(f\"Missing requested provider {Api(args.api)}\")\n```\n\n**Troubleshooting:**\nThe definition of the related module under [llama_stack.providers.registry](https://github.com/meta-llama/llama-stack/blob/529708215c5ad54e1ef41ba3e68d3a2af8d563b0/llama_stack/distribution/distribution.py#L66) is needed.\n\n\n",
                    "state": "CLOSED",
                    "comments": {
                        "nodes": [
                            {
                                "author": {
                                    "login": "terrytangyuan"
                                },
                                "body": "Fixing in https://github.com/meta-llama/llama-stack/pull/969"
                            }
                        ]
                    }
                }
            }
        }
    }
]