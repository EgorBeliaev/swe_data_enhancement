[
    {
        "title": "refactor(ollama): model availability check"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "# What does this PR do?\r\n\r\nMoved model availability check logic into a dedicated check_model_availability function. Eliminated redundant code by reusing the helper function in both embedding and non-embedding model registration.\r\n\r\nSigned-off-by: S\u00e9bastien Han <seb@redhat.com>\r\n\r\n## Test Plan\r\n\r\nRun Ollama and serve 2 models to get most the unit test pass:\r\n\r\n```\r\nollama run llama3.2:3b-instruct-fp16 --keepalive 2m &\r\nollama run llama3.1:8b  --keepalive 2m &\r\n```\r\n\r\nRun the unit test:\r\n\r\n```\r\nuv run pytest -v -k \"ollama\" --inference-model=llama3.2:3b-instruct-fp16 llama_stack/providers/tests/inference/test_model_registration.py\r\n/Users/leseb/Documents/AI/llama-stack/.venv/lib/python3.13/site-packages/pytest_asyncio/plugin.py:207: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\r\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\r\n\r\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\r\n============================================ test session starts =============================================\r\nplatform darwin -- Python 3.13.1, pytest-8.3.4, pluggy-1.5.0 -- /Users/leseb/Documents/AI/llama-stack/.venv/bin/python3\r\ncachedir: .pytest_cache\r\nmetadata: {'Python': '3.13.1', 'Platform': 'macOS-15.3-arm64-arm-64bit-Mach-O', 'Packages': {'pytest': '8.3.4', 'pluggy': '1.5.0'}, 'Plugins': {'html': '4.1.1', 'metadata': '3.1.1', 'asyncio': '0.25.3', 'anyio': '4.8.0', 'nbval': '0.11.0'}}\r\nrootdir: /Users/leseb/Documents/AI/llama-stack\r\nconfigfile: pyproject.toml\r\nplugins: html-4.1.1, metadata-3.1.1, asyncio-0.25.3, anyio-4.8.0, nbval-0.11.0\r\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None\r\ncollected 65 items / 60 deselected / 5 selected                                                              \r\n\r\nllama_stack/providers/tests/inference/test_model_registration.py::TestModelRegistration::test_register_unsupported_model[-ollama] PASSED [ 20%]\r\nllama_stack/providers/tests/inference/test_model_registration.py::TestModelRegistration::test_register_nonexistent_model[-ollama] PASSED [ 40%]\r\nllama_stack/providers/tests/inference/test_model_registration.py::TestModelRegistration::test_register_with_llama_model[-ollama] FAILED [ 60%]\r\nllama_stack/providers/tests/inference/test_model_registration.py::TestModelRegistration::test_initialize_model_during_registering[-ollama] FAILED [ 80%]\r\nllama_stack/providers/tests/inference/test_model_registration.py::TestModelRegistration::test_register_with_invalid_llama_model[-ollama] PASSED [100%]\r\n\r\n================================================== FAILURES ==================================================\r\n_______________________ TestModelRegistration.test_register_with_llama_model[-ollama] ________________________\r\nllama_stack/providers/tests/inference/test_model_registration.py:54: in test_register_with_llama_model\r\n    _ = await models_impl.register_model(\r\nllama_stack/providers/utils/telemetry/trace_protocol.py:91: in async_wrapper\r\n    result = await method(self, *args, **kwargs)\r\nllama_stack/distribution/routers/routing_tables.py:245: in register_model\r\n    registered_model = await self.register_object(model)\r\nllama_stack/distribution/routers/routing_tables.py:192: in register_object\r\n    registered_obj = await register_object_with_provider(obj, p)\r\nllama_stack/distribution/routers/routing_tables.py:53: in register_object_with_provider\r\n    return await p.register_model(obj)\r\nllama_stack/providers/utils/telemetry/trace_protocol.py:91: in async_wrapper\r\n    result = await method(self, *args, **kwargs)\r\nllama_stack/providers/remote/inference/ollama/ollama.py:368: in register_model\r\n    await check_model_availability(model.provider_resource_id)\r\nllama_stack/providers/remote/inference/ollama/ollama.py:359: in check_model_availability\r\n    raise ValueError(\r\nE   ValueError: Model 'custom-model' is not available in Ollama. Available models: llama3.1:8b, llama3.2:3b-instruct-fp16\r\n__________________ TestModelRegistration.test_initialize_model_during_registering[-ollama] ___________________\r\nllama_stack/providers/tests/inference/test_model_registration.py:85: in test_initialize_model_during_registering\r\n    mock_load_model.assert_called_once()\r\n/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/unittest/mock.py:956: in assert_called_once\r\n    raise AssertionError(msg)\r\nE   AssertionError: Expected 'load_model' to have been called once. Called 0 times.\r\n-------------------------------------------- Captured stderr call --------------------------------------------\r\nW0207 11:55:26.777000 90854 .venv/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\r\n========================================== short test summary info ===========================================\r\nFAILED llama_stack/providers/tests/inference/test_model_registration.py::TestModelRegistration::test_register_with_llama_model[-ollama] - ValueError: Model 'custom-model' is not available in Ollama. Available models: llama3.1:8b, llama3.2:3b-i...\r\nFAILED llama_stack/providers/tests/inference/test_model_registration.py::TestModelRegistration::test_initialize_model_during_registering[-ollama] - AssertionError: Expected 'load_model' to have been called once. Called 0 times.\r\n=========================== 2 failed, 3 passed, 60 deselected, 2 warnings in 1.84s ===========================\r\n``` \r\n\r\nWe only \"care\" about the `test_register_nonexistent_model` for this code.\r\n\r\n\r\n## Sources\r\n\r\nPlease link relevant resources if necessary.\r\n\r\n\r\n## Before submitting\r\n\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Ran pre-commit to handle lint / formatting issues.\r\n- [ ] Read the [contributor guideline](https://github.com/meta-llama/llama-stack/blob/main/CONTRIBUTING.md),\r\n      Pull Request section?\r\n- [ ] Updated relevant documentation.\r\n- [ ] Wrote necessary unit or integration tests.\r\n"
    },
    {
        "author": {
            "login": "ashwinb"
        },
        "body": "Can you add a proper test plan which shows:\r\n\r\n- how you started ollama, and what models you had it load\r\n- running tests either from `providers/tests/inference` or `tests/client-sdk` which would exercise this code path?"
    },
    {
        "author": {
            "login": "ashwinb"
        },
        "body": "Thank you!"
    },
    {
        "author": {
            "login": "ashwinb"
        },
        "body": ""
    },
    {
        "author": {
            "login": "leseb"
        },
        "body": "> Can you add a proper test plan which shows:\r\n> \r\n> * how you started ollama, and what models you had it load\r\n> * running tests either from `providers/tests/inference` or `tests/client-sdk` which would exercise this code path?\r\n\r\nI updated the test plan, PTAL, thanks!"
    }
]