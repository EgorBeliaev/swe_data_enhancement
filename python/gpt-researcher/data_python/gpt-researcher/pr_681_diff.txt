diff --git a/gpt_researcher/master/actions.py b/gpt_researcher/master/actions.py
index 0d58a118c..fc17c6ded 100644
--- a/gpt_researcher/master/actions.py
+++ b/gpt_researcher/master/actions.py
@@ -343,12 +343,37 @@ async def generate_report(
     """
     generate_prompt = get_prompt_by_report_type(report_type)
     report = ""
-
+    
     if report_type == "subtopic_report":
-        content = f"{generate_prompt(query, existing_headers, main_topic, context, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words)}"
+        content = f"{generate_prompt(query, existing_headers, main_topic, context, report_format=cfg.report_format, total_words=cfg.total_words)}"
+        if tone:
+            content += f", tone={tone}"
+        summary = await create_chat_completion(
+            model=cfg.fast_llm_model,
+            messages=[
+                {"role": "system", "content": agent_role_prompt},
+                {"role": "user", "content": content}
+            ],
+            temperature=0,
+            llm_provider=cfg.llm_provider,
+            llm_kwargs=cfg.llm_kwargs,
+            cost_callback=cost_callback,
+        )
     else:
-        content = f"{generate_prompt(query, context, report_source, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words)}"
-
+        content = f"{generate_prompt(query, context, report_source, report_format=cfg.report_format, total_words=cfg.total_words)}"
+        if tone:
+            content += f", tone={tone}"
+        summary = await create_chat_completion(
+            model=cfg.fast_llm_model,
+            messages=[
+                {"role": "system", "content": agent_role_prompt},
+                {"role": "user", "content": content}
+            ],
+            temperature=0,
+            llm_provider=cfg.llm_provider,
+            llm_kwargs=cfg.llm_kwargs,
+            cost_callback=cost_callback,
+        )
     try:
         report = await create_chat_completion(
             model=cfg.smart_llm_model,
diff --git a/gpt_researcher/master/agent.py b/gpt_researcher/master/agent.py
index a8e1940f0..7d0b43f3f 100644
--- a/gpt_researcher/master/agent.py
+++ b/gpt_researcher/master/agent.py
@@ -59,9 +59,7 @@ def __init__(
         self.report_source: str = report_source
         self.research_costs: float = 0.0
         self.cfg = Config(config_path)
-        self.retriever = get_retriever(self.headers.get("retriever")) or get_retriever(
-            self.cfg.retriever
-        )
+        self.retriever = get_retriever( self.cfg.retriever)
         self.context = context
         self.source_urls = source_urls
         self.documents = documents
@@ -335,7 +333,7 @@ async def __scrape_data_by_query(self, sub_query):
             Summary
         """
         # Get Urls
-        retriever = self.retriever(sub_query, headers=self.headers)
+        retriever = self.retriever(sub_query)
         search_results = await asyncio.to_thread(
             retriever.search, max_results=self.cfg.max_search_results_per_query
         )
diff --git a/gpt_researcher/retrievers/searx/searx.py b/gpt_researcher/retrievers/searx/searx.py
index f64e26cff..5b96b3b34 100644
--- a/gpt_researcher/retrievers/searx/searx.py
+++ b/gpt_researcher/retrievers/searx/searx.py
@@ -18,7 +18,6 @@ def __init__(self, query):
         """
         self.query = query
         self.api_key = self.get_api_key()
-        self.client = TavilyClient(self.api_key)
 
     def get_api_key(self):
         """
diff --git a/gpt_researcher/utils/llm.py b/gpt_researcher/utils/llm.py
index 927141f85..d2dfeb1c8 100644
--- a/gpt_researcher/utils/llm.py
+++ b/gpt_researcher/utils/llm.py
@@ -85,7 +85,7 @@ async def create_chat_completion(
             f"Max tokens cannot be more than 8001, but got {max_tokens}")
 
     # Get the provider from supported providers
-    provider = get_llm(llm_provider, model=model, temperature=temperature, max_tokens=max_tokens, openai_api_key=openai_api_key, **(llm_kwargs or {}))
+    provider = get_llm(llm_provider, model=model, temperature=temperature, max_tokens=max_tokens, **(llm_kwargs or {}))
 
     response = ""
     # create response
