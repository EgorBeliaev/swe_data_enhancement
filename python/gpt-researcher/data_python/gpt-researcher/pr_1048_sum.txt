**Instruction for Programmer:**

Fix the issue where Gemini doesn't support `role=system` in report generation. When `"SMART_LLM": "google_vertexai:gemini-1.5-flash-002"` is configured, and the report generation fails with "Error in generate_report", implement the following logic:

1. First, attempt the original chat completion with the following structure:
   ```python
   messages = [
       {"role": "system", "content": f"{agent_role_prompt}"},
       {"role": "user", "content": content},
   ]
   ```
2. If the above attempt fails, retry using the modified chat completion:
   ```python
   messages = [
       {"role": "user", "content": f"{agent_role_prompt}\\n\\n{content}"},
   ]
   ```

This adjustment should allow for successful report generation with the specified SMART_LLM configuration.