diff --git a/README.md b/README.md
index 4396e08ad..665de1bc8 100644
--- a/README.md
+++ b/README.md
@@ -335,6 +335,7 @@ You can refine your search by selecting the task you're interested in (e.g., [te
 1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.
 1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) released with the paper [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.
 1. **[Granite](https://huggingface.co/docs/transformers/main/model_doc/granite)** (from IBM) released with the paper [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, Rameswar Panda.
+1. **[Grounding DINO](https://huggingface.co/docs/transformers/model_doc/grounding-dino)** (from IDEA-Research) released with the paper [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499) by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang.
 1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.
 1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.
 1. **[Hiera](https://huggingface.co/docs/transformers/model_doc/hiera)** (from Meta) released with the paper [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://arxiv.org/pdf/2306.00989) by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer.
diff --git a/docs/snippets/6_supported-models.snippet b/docs/snippets/6_supported-models.snippet
index 640d880e6..349238a08 100644
--- a/docs/snippets/6_supported-models.snippet
+++ b/docs/snippets/6_supported-models.snippet
@@ -50,6 +50,7 @@
 1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.
 1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) released with the paper [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.
 1. **[Granite](https://huggingface.co/docs/transformers/main/model_doc/granite)** (from IBM) released with the paper [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, Rameswar Panda.
+1. **[Grounding DINO](https://huggingface.co/docs/transformers/model_doc/grounding-dino)** (from IDEA-Research) released with the paper [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499) by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang.
 1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.
 1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.
 1. **[Hiera](https://huggingface.co/docs/transformers/model_doc/hiera)** (from Meta) released with the paper [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://arxiv.org/pdf/2306.00989) by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer.
diff --git a/src/base/image_processors_utils.js b/src/base/image_processors_utils.js
index 35df60ddd..685de28e5 100644
--- a/src/base/image_processors_utils.js
+++ b/src/base/image_processors_utils.js
@@ -68,7 +68,7 @@ function enforce_size_divisibility([width, height], divisor) {
  * @param {number[]} arr The coordinate for the center of the box and its width, height dimensions (center_x, center_y, width, height)
  * @returns {number[]} The coodinates for the top-left and bottom-right corners of the box (top_left_x, top_left_y, bottom_right_x, bottom_right_y)
  */
-function center_to_corners_format([centerX, centerY, width, height]) {
+export function center_to_corners_format([centerX, centerY, width, height]) {
     return [
         centerX - width / 2,
         centerY - height / 2,
diff --git a/src/base/processing_utils.js b/src/base/processing_utils.js
index 321f5cdb8..adf442f32 100644
--- a/src/base/processing_utils.js
+++ b/src/base/processing_utils.js
@@ -101,6 +101,17 @@ export class Processor extends Callable {
         return this.tokenizer.batch_decode(...args);
     }
 
+    /**
+     * @param {Parameters<PreTrainedTokenizer['decode']>} args
+     * @returns {ReturnType<PreTrainedTokenizer['decode']>}
+     */
+    decode(...args) {
+        if (!this.tokenizer) {
+            throw new Error('Unable to decode without a tokenizer.');
+        }
+        return this.tokenizer.decode(...args);
+    }
+
 
     /**
      * Calls the feature_extractor function with the given input.
diff --git a/src/models.js b/src/models.js
index d0bc2311b..091638418 100644
--- a/src/models.js
+++ b/src/models.js
@@ -532,14 +532,23 @@ async function encoderForward(self, model_inputs) {
         encoderFeeds.inputs_embeds = await self.encode_text({ input_ids: model_inputs.input_ids });
     }
     if (session.inputNames.includes('token_type_ids') && !encoderFeeds.token_type_ids) {
+        if (!encoderFeeds.input_ids) {
+            throw new Error('Both `input_ids` and `token_type_ids` are missing in the model inputs.');
+        }
         // Assign default `token_type_ids` (all zeroes) to the `encoderFeeds` if the model expects it,
         // but they weren't created by the tokenizer.
-        encoderFeeds.token_type_ids = new Tensor(
-            'int64',
-            new BigInt64Array(encoderFeeds.input_ids.data.length),
-            encoderFeeds.input_ids.dims
-        )
+        encoderFeeds.token_type_ids = zeros_like(encoderFeeds.input_ids);
+    }
+    if (session.inputNames.includes('pixel_mask') && !encoderFeeds.pixel_mask) {
+        if (!encoderFeeds.pixel_values) {
+            throw new Error('Both `pixel_values` and `pixel_mask` are missing in the model inputs.');
+        }
+        // Assign default `pixel_mask` (all ones) to the `encoderFeeds` if the model expects it,
+        // but they weren't created by the processor.
+        const dims = encoderFeeds.pixel_values.dims;
+        encoderFeeds.pixel_mask = ones([dims[0], dims[2], dims[3]]);
     }
+    
     return await sessionRun(session, encoderFeeds);
 }
 
@@ -5428,6 +5437,8 @@ export class Dinov2WithRegistersForImageClassification extends Dinov2WithRegiste
     }
 }
 //////////////////////////////////////////////////
+export class GroundingDinoPreTrainedModel extends PreTrainedModel { }
+export class GroundingDinoForObjectDetection extends GroundingDinoPreTrainedModel { }
 
 //////////////////////////////////////////////////
 export class YolosPreTrainedModel extends PreTrainedModel { }
@@ -7333,6 +7344,7 @@ const MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES = new Map([
 const MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES = new Map([
     ['owlvit', ['OwlViTForObjectDetection', OwlViTForObjectDetection]],
     ['owlv2', ['Owlv2ForObjectDetection', Owlv2ForObjectDetection]],
+    ['grounding-dino', ['GroundingDinoForObjectDetection', GroundingDinoForObjectDetection]],
 ]);
 
 const MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES = new Map([
diff --git a/src/models/grounding_dino/image_processing_grounding_dino.js b/src/models/grounding_dino/image_processing_grounding_dino.js
new file mode 100644
index 000000000..d4bb5bd8b
--- /dev/null
+++ b/src/models/grounding_dino/image_processing_grounding_dino.js
@@ -0,0 +1,29 @@
+
+import { 
+    ImageProcessor,
+} from "../../base/image_processors_utils.js";
+import { ones } from '../../utils/tensor.js';
+
+
+/**
+ * @typedef {object} GroundingDinoFeatureExtractorResultProps
+ * @property {import('../../utils/tensor.js').Tensor} pixel_mask
+ * @typedef {import('../../base/image_processors_utils.js').ImageProcessorResult & GroundingDinoFeatureExtractorResultProps} GroundingDinoFeatureExtractorResult
+ */
+
+export class GroundingDinoImageProcessor extends ImageProcessor {
+    /**
+     * Calls the feature extraction process on an array of images, preprocesses
+     * each image, and concatenates the resulting features into a single Tensor.
+     * @param {import('../../utils/image.js').RawImage[]} images The image(s) to extract features from.
+     * @returns {Promise<GroundingDinoFeatureExtractorResult>} An object containing the concatenated pixel values of the preprocessed images.
+     */
+    async _call(images) {
+        const result = await super._call(images);
+
+        const dims = result.pixel_values.dims;
+        const pixel_mask = ones([dims[0], dims[2], dims[3]]);
+
+        return { ...result, pixel_mask };
+    }
+}
diff --git a/src/models/grounding_dino/processing_grounding_dino.js b/src/models/grounding_dino/processing_grounding_dino.js
new file mode 100644
index 000000000..62d5387bf
--- /dev/null
+++ b/src/models/grounding_dino/processing_grounding_dino.js
@@ -0,0 +1,101 @@
+import { Processor } from "../../base/processing_utils.js";
+import { AutoImageProcessor } from "../auto/image_processing_auto.js";
+import { AutoTokenizer } from "../../tokenizers.js";
+import { center_to_corners_format } from "../../base/image_processors_utils.js";
+
+/**
+ * Get token ids of phrases from posmaps and input_ids.
+ * @param {import('../../utils/tensor.js').Tensor} posmaps A boolean tensor of unbatched text-thresholded logits related to the detected bounding boxes of shape `(hidden_size, )`.
+ * @param {import('../../utils/tensor.js').Tensor} input_ids A tensor of token ids of shape `(sequence_length, )`.
+ */
+function get_phrases_from_posmap(posmaps, input_ids) {
+
+    const left_idx = 0;
+    const right_idx = posmaps.dims.at(-1) - 1;
+
+    const posmaps_list = posmaps.tolist();
+    posmaps_list.fill(false, 0, left_idx + 1);
+    posmaps_list.fill(false, right_idx);
+
+    const input_ids_list = input_ids.tolist();
+    return posmaps_list
+        .map((val, idx) => val ? idx : null)
+        .filter(idx => idx !== null)
+        .map(i => input_ids_list[i]);
+}
+
+export class GroundingDinoProcessor extends Processor {
+    static tokenizer_class = AutoTokenizer
+    static image_processor_class = AutoImageProcessor
+
+    /**
+     * @typedef {import('../../utils/image.js').RawImage} RawImage
+     */
+    /**
+     * 
+     * @param {RawImage|RawImage[]|RawImage[][]} images  
+     * @param {string|string[]} text 
+     * @returns {Promise<any>}
+     */
+    async _call(images, text, options = {}) {
+
+        const image_inputs = images ? await this.image_processor(images, options) : {};
+        const text_inputs = text ? this.tokenizer(text, options) : {};
+
+        return {
+            ...text_inputs,
+            ...image_inputs,
+        }
+    }
+    post_process_grounded_object_detection(outputs, input_ids, {
+        box_threshold = 0.25,
+        text_threshold = 0.25,
+        target_sizes = null
+    } = {}) {
+        const { logits, pred_boxes } = outputs;
+        const batch_size = logits.dims[0];
+
+        if (target_sizes !== null && target_sizes.length !== batch_size) {
+            throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits")
+        }
+        const num_queries = logits.dims.at(1);
+
+        const probs = logits.sigmoid(); // (batch_size, num_queries, 256)
+        const scores = probs.max(-1).tolist(); // (batch_size, num_queries)
+
+        // Convert to [x0, y0, x1, y1] format
+        const boxes = pred_boxes.tolist() // (batch_size, num_queries, 4)
+            .map(batch => batch.map(box => center_to_corners_format(box)));
+
+        const results = [];
+        for (let i = 0; i < batch_size; ++i) {
+            const target_size = target_sizes !== null ? target_sizes[i] : null;
+
+            // Convert from relative [0, 1] to absolute [0, height] coordinates
+            if (target_size !== null) {
+                boxes[i] = boxes[i].map(box => box.map((x, j) => x * target_size[(j + 1) % 2]));
+            }
+
+            const batch_scores = scores[i];
+            const final_scores = [];
+            const final_phrases = [];
+            const final_boxes = [];
+            for (let j = 0; j < num_queries; ++j) {
+                const score = batch_scores[j];
+                if (score <= box_threshold) {
+                    continue;
+                }
+                const box = boxes[i][j];
+                const prob = probs[i][j];
+
+                final_scores.push(score);
+                final_boxes.push(box);
+
+                const phrases = get_phrases_from_posmap(prob.gt(text_threshold), input_ids[i]);
+                final_phrases.push(phrases);
+            }
+            results.push({ scores: final_scores, boxes: final_boxes, labels: this.batch_decode(final_phrases) });
+        }
+        return results;
+    }
+}
diff --git a/src/models/image_processors.js b/src/models/image_processors.js
index fd002c81c..ea762572f 100644
--- a/src/models/image_processors.js
+++ b/src/models/image_processors.js
@@ -10,6 +10,7 @@ export * from './donut/image_processing_donut.js'
 export * from './dpt/image_processing_dpt.js'
 export * from './efficientnet/image_processing_efficientnet.js'
 export * from './glpn/image_processing_glpn.js'
+export * from './grounding_dino/image_processing_grounding_dino.js'
 export * from './idefics3/image_processing_idefics3.js'
 export * from './janus/image_processing_janus.js'
 export * from './jina_clip/image_processing_jina_clip.js'
diff --git a/src/models/processors.js b/src/models/processors.js
index 9d08faf73..d9bff3da9 100644
--- a/src/models/processors.js
+++ b/src/models/processors.js
@@ -1,9 +1,10 @@
 export * from './florence2/processing_florence2.js';
-export * from './mgp_str/processing_mgp_str.js';
-export * from './moonshine/processing_moonshine.js';
+export * from './grounding_dino/processing_grounding_dino.js';
 export * from './idefics3/processing_idefics3.js';
 export * from './janus/processing_janus.js';
 export * from './jina_clip/processing_jina_clip.js';
+export * from './mgp_str/processing_mgp_str.js';
+export * from './moonshine/processing_moonshine.js';
 export * from './owlvit/processing_owlvit.js';
 export * from './phi3_v/processing_phi3_v.js';
 export * from './paligemma/processing_paligemma.js';
diff --git a/src/pipelines.js b/src/pipelines.js
index 87c489c38..105f1bb1f 100644
--- a/src/pipelines.js
+++ b/src/pipelines.js
@@ -2552,13 +2552,35 @@ export class ZeroShotObjectDetectionPipeline extends (/** @type {new (options: T
             // Run model with both text and pixel inputs
             const output = await this.model({ ...text_inputs, pixel_values });
 
-            // @ts-ignore
-            const processed = this.processor.image_processor.post_process_object_detection(output, threshold, imageSize, true)[0];
-            let result = processed.boxes.map((box, i) => ({
-                score: processed.scores[i],
-                label: candidate_labels[processed.classes[i]],
-                box: get_bounding_box(box, !percentage),
-            })).sort((a, b) => b.score - a.score);
+            let result;
+            if('post_process_grounded_object_detection' in this.processor) {
+                // @ts-ignore
+                const processed = this.processor.post_process_grounded_object_detection(
+                    output,
+                    text_inputs.input_ids,
+                    {
+                        // TODO: support separate threshold values
+                        box_threshold: threshold,
+                        text_threshold: threshold,
+                        target_sizes: imageSize,
+                    },
+                )[0];
+                result = processed.boxes.map((box, i) => ({
+                    score: processed.scores[i],
+                    label: processed.labels[i],
+                    box: get_bounding_box(box, !percentage),
+                }))
+            } else {
+                // @ts-ignore
+                const processed = this.processor.image_processor.post_process_object_detection(output, threshold, imageSize, true)[0];
+                result = processed.boxes.map((box, i) => ({
+                    score: processed.scores[i],
+                    label: candidate_labels[processed.classes[i]],
+                    box: get_bounding_box(box, !percentage),
+                }))
+            }
+            result.sort((a, b) => b.score - a.score);
+
             if (top_k !== null) {
                 result = result.slice(0, top_k);
             }
diff --git a/src/utils/tensor.js b/src/utils/tensor.js
index fda0aa04c..f13d5b8af 100644
--- a/src/utils/tensor.js
+++ b/src/utils/tensor.js
@@ -494,55 +494,22 @@ export class Tensor {
         }
 
         const this_data = this.data;
+        const fn = (a, b) => a + (b ** p);
 
         if (dim === null) {
             // @ts-ignore
-            let val = this_data.reduce((a, b) => a + (b ** p), 0) ** (1 / p);
+            const val = this_data.reduce(fn, 0) ** (1 / p);
             return new Tensor(this.type, [val], []);
         }
 
-        // Negative indexing
-        dim = safeIndex(dim, this.dims.length);
-
-        // Calculate the shape of the resulting array after summation
-        const resultDims = this.dims.slice(); // Copy the original dimensions
-        resultDims[dim] = 1; // Remove the specified axis
-
-        // Create a new array to store the accumulated values
-        // @ts-ignore
-        const result = new this_data.constructor(this_data.length / this.dims[dim]);
-
-        // Iterate over the data array
-        for (let i = 0; i < this_data.length; ++i) {
-
-            // Calculate the index in the resulting array
-            let resultIndex = 0;
-
-            for (let j = this.dims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {
-                const size = this.dims[j];
-                if (j !== dim) {
-                    const index = num % size;
-                    resultIndex += index * resultMultiplier;
-                    resultMultiplier *= resultDims[j];
-                }
-                num = Math.floor(num / size);
-            }
-
-            // Accumulate the value at the current index
-            result[resultIndex] += (this_data[i]) ** p;
-        }
+        const [type, result, resultDims] = reduce_helper(fn, this, dim, keepdim);
 
         if (p !== 1) {
             for (let i = 0; i < result.length; ++i) {
                 result[i] = result[i] ** (1 / p);
             }
         }
-
-        if (!keepdim) {
-            resultDims.splice(dim, 1);
-        }
-
-        return new Tensor(this.type, result, resultDims);
+        return new Tensor(type, result, resultDims);
     }
 
     /**
@@ -605,7 +572,7 @@ export class Tensor {
      * NOTE: The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.
      * If you would like a copy, use `tensor.clone()` before squeezing.
      *
-     * @param {number} [dim=null] If given, the input will be squeezed only in the specified dimensions.
+     * @param {number|number[]} [dim=null] If given, the input will be squeezed only in the specified dimensions.
      * @returns {Tensor} The squeezed tensor
      */
     squeeze(dim = null) {
@@ -715,6 +682,34 @@ export class Tensor {
         return this.clone().neg_();
     }
 
+    /**
+     * Computes input > val element-wise.
+     * @param {number} val The value to compare with.
+     * @returns {Tensor} A boolean tensor that is `true` where input is greater than other and `false` elsewhere.
+     */
+    gt(val) {
+        const mask = new Uint8Array(this.data.length);
+        const this_data = this.data;
+        for (let i = 0; i < this_data.length; ++i) {
+            mask[i] = this_data[i] > val ? 1 : 0;
+        }
+        return new Tensor('bool', mask, this.dims);
+    }
+
+    /**
+     * Computes input < val element-wise.
+     * @param {number} val The value to compare with.
+     * @returns {Tensor} A boolean tensor that is `true` where input is less than other and `false` elsewhere.
+     */
+    lt(val) {
+        const mask = new Uint8Array(this.data.length);
+        const this_data = this.data;
+        for (let i = 0; i < this_data.length; ++i) {
+            mask[i] = this_data[i] < val ? 1 : 0;
+        }
+        return new Tensor('bool', mask, this.dims);
+    }
+
     /**
      * In-place version of @see {@link Tensor.clamp}
      */
@@ -760,18 +755,23 @@ export class Tensor {
     }
 
     min(dim = null, keepdim = false) {
-        if (dim !== null) {
-            throw new Error("`dim !== null` not yet implemented.");
+        if (dim === null) {
+            // None to reduce over all dimensions.
+            const val = min(this.data)[0];
+            return new Tensor(this.type, [val], [/* scalar */]);
         }
-        const value = min(this.data)[0];
-        return new Tensor(this.type, [value], []);
+        const [type, result, resultDims] = reduce_helper((a, b) => Math.min(a, b), this, dim, keepdim, Infinity);
+        return new Tensor(type, result, resultDims);
     }
+
     max(dim = null, keepdim = false) {
-        if (dim !== null) {
-            throw new Error("`dim !== null` not yet implemented.");
+        if (dim === null) {
+            // None to reduce over all dimensions.
+            const val = max(this.data)[0];
+            return new Tensor(this.type, [val], [/* scalar */]);
         }
-        const value = max(this.data)[0];
-        return new Tensor(this.type, [value], []);
+        const [type, result, resultDims] = reduce_helper((a, b) => Math.max(a, b), this, dim, keepdim, -Infinity);
+        return new Tensor(type, result, resultDims);
     }
 
     argmin(dim = null, keepdim = false) {
@@ -1268,35 +1268,19 @@ export function stack(tensors, dim = 0) {
 
 
 /**
- * Calculates the standard deviation and mean over the dimensions specified by dim. dim can be a single dimension or `null` to reduce over all dimensions.
- * @param {Tensor} input the input tenso
- * @param {number|null} dim the dimension to reduce. If None, all dimensions are reduced.
- * @param {number} correction difference between the sample size and sample degrees of freedom. Defaults to Bessel's correction, correction=1.
+ * @param {(previousValue: any, currentValue: any, currentIndex?: number, resultIndex?: number) => any} callbackfn
+ * @param {Tensor} input the input tensor.
+ * @param {number|null} dim the dimension to reduce.
  * @param {boolean} keepdim whether the output tensor has dim retained or not.
- * @returns {Tensor[]} A tuple of (std, mean) tensors.
+ * @returns {[DataType, any, number[]]} The reduced tensor data.
  */
-export function std_mean(input, dim = null, correction = 1, keepdim = false) {
-    const inputData = /** @type {Float32Array} */(input.data);
+function reduce_helper(callbackfn, input, dim = null, keepdim = false, initialValue = null) {
+    const inputData = input.data;
     const inputDims = input.dims;
 
-    if (dim === null) {
-        // None to reduce over all dimensions.
-        const sum = inputData.reduce((a, b) => a + b, 0);
-        const mean = sum / inputData.length;
-        const std = Math.sqrt(inputData.reduce((a, b) => a + (b - mean) ** 2, 0) / (inputData.length - correction));
-
-        const meanTensor = new Tensor(input.type, [mean], [/* scalar */]);
-        const stdTensor = new Tensor(input.type, [std], [/* scalar */]);
-
-        return [stdTensor, meanTensor];
-    }
-
     // Negative indexing
     dim = safeIndex(dim, inputDims.length);
 
-    const meanTensor = mean(input, dim, keepdim);
-    const meanTensorData = meanTensor.data;
-
     // Calculate the shape of the resulting array after summation
     const resultDims = inputDims.slice(); // Copy the original dimensions
     resultDims[dim] = 1; // Remove the specified axis
@@ -1304,6 +1288,9 @@ export function std_mean(input, dim = null, correction = 1, keepdim = false) {
     // Create a new array to store the accumulated values
     // @ts-ignore
     const result = new inputData.constructor(inputData.length / inputDims[dim]);
+    if (initialValue !== null) {
+        result.fill(initialValue);
+    }
 
     // Iterate over the data array
     for (let i = 0; i < inputData.length; ++i) {
@@ -1322,23 +1309,55 @@ export function std_mean(input, dim = null, correction = 1, keepdim = false) {
         }
 
         // Accumulate the value at the current index
-        result[resultIndex] += (inputData[i] - meanTensorData[resultIndex]) ** 2;
+        result[resultIndex] = callbackfn(result[resultIndex], inputData[i], i, resultIndex);
     }
 
-    for (let i = 0; i < result.length; ++i) {
-        result[i] = Math.sqrt(result[i] / (inputDims[dim] - correction));
+    if (!keepdim) resultDims.splice(dim, 1);
+
+    return [input.type, result, resultDims];
+}
+
+
+/**
+ * Calculates the standard deviation and mean over the dimensions specified by dim. dim can be a single dimension or `null` to reduce over all dimensions.
+ * @param {Tensor} input the input tenso
+ * @param {number|null} dim the dimension to reduce. If None, all dimensions are reduced.
+ * @param {number} correction difference between the sample size and sample degrees of freedom. Defaults to Bessel's correction, correction=1.
+ * @param {boolean} keepdim whether the output tensor has dim retained or not.
+ * @returns {Tensor[]} A tuple of (std, mean) tensors.
+ */
+export function std_mean(input, dim = null, correction = 1, keepdim = false) {
+    const inputData = /** @type {Float32Array} */(input.data);
+    const inputDims = input.dims;
+
+    if (dim === null) {
+        // None to reduce over all dimensions.
+        const sum = inputData.reduce((a, b) => a + b, 0);
+        const mean = sum / inputData.length;
+        const std = Math.sqrt(inputData.reduce((a, b) => a + (b - mean) ** 2, 0) / (inputData.length - correction));
+
+        const meanTensor = new Tensor(input.type, [mean], [/* scalar */]);
+        const stdTensor = new Tensor(input.type, [std], [/* scalar */]);
+
+        return [stdTensor, meanTensor];
     }
+    dim = safeIndex(dim, inputDims.length);
+    const meanTensor = mean(input, dim, keepdim);
+    const meanTensorData = meanTensor.data;
+
+    // Compute squared sum
+    const [type, result, resultDims] = reduce_helper((a, b, i, j) => a + (b - meanTensorData[j]) ** 2, input, dim, keepdim);
 
-    if (!keepdim) {
-        resultDims.splice(dim, 1);
+    // Square root of the squared sum
+    for (let i = 0; i < result.length; ++i) {
+        result[i] = Math.sqrt(result[i] / (inputDims[dim] - correction));
     }
 
-    const stdTensor = new Tensor(input.type, result, resultDims);
+    const stdTensor = new Tensor(type, result, resultDims);
 
     return [stdTensor, meanTensor];
 }
 
-
 /**
  * Returns the mean value of each row of the input tensor in the given dimension dim.
  * @param {Tensor} input the input tensor.
@@ -1347,58 +1366,27 @@ export function std_mean(input, dim = null, correction = 1, keepdim = false) {
  * @returns {Tensor} A new tensor with means taken along the specified dimension.
  */
 export function mean(input, dim = null, keepdim = false) {
+    const inputDims = input.dims;
     const inputData = /** @type {Float32Array} */(input.data);
 
     if (dim === null) {
         // None to reduce over all dimensions.
-        // @ts-ignore
         const val = inputData.reduce((a, b) => a + b, 0);
         return new Tensor(input.type, [val / inputData.length], [/* scalar */]);
     }
-    const inputDims = input.dims;
-
-    // Negative indexing
     dim = safeIndex(dim, inputDims.length);
 
-    // Calculate the shape of the resulting array after summation
-    const resultDims = inputDims.slice(); // Copy the original dimensions
-    resultDims[dim] = 1; // Remove the specified axis
-
-    // Create a new array to store the accumulated values
-    // @ts-ignore
-    const result = new inputData.constructor(inputData.length / inputDims[dim]);
-
-    // Iterate over the data array
-    for (let i = 0; i < inputData.length; ++i) {
-
-        // Calculate the index in the resulting array
-        let resultIndex = 0;
-
-        for (let j = inputDims.length - 1, num = i, resultMultiplier = 1; j >= 0; --j) {
-            const size = inputDims[j];
-            if (j !== dim) {
-                const index = num % size;
-                resultIndex += index * resultMultiplier;
-                resultMultiplier *= resultDims[j];
-            }
-            num = Math.floor(num / size);
-        }
-
-        // Accumulate the value at the current index
-        result[resultIndex] += inputData[i];
-    }
+    // Compute sum
+    const [type, result, resultDims] = reduce_helper((a, b) => a + b, input, dim, keepdim);
 
+    // Divide by number of elements in the dimension
     if (inputDims[dim] !== 1) {
         for (let i = 0; i < result.length; ++i) {
-            result[i] = result[i] / inputDims[dim];
+            result[i] /= inputDims[dim];
         }
     }
 
-    if (!keepdim) {
-        resultDims.splice(dim, 1);
-    }
-
-    return new Tensor(input.type, result, resultDims);
+    return new Tensor(type, result, resultDims);
 }
 
 
diff --git a/tests/models/grounding_dino/test_modeling_grounding_dino.js b/tests/models/grounding_dino/test_modeling_grounding_dino.js
new file mode 100644
index 000000000..b1abb8826
--- /dev/null
+++ b/tests/models/grounding_dino/test_modeling_grounding_dino.js
@@ -0,0 +1,44 @@
+import { GroundingDinoProcessor, GroundingDinoForObjectDetection, RawImage } from "../../../src/transformers.js";
+
+import { MAX_MODEL_LOAD_TIME, MAX_TEST_EXECUTION_TIME, MAX_MODEL_DISPOSE_TIME, DEFAULT_MODEL_OPTIONS } from "../../init.js";
+
+export default () => {
+  const text = "a cat."; // NB: text query needs to be lowercased + end with a dot
+
+  // Empty white image
+  const dims = [224, 224, 3];
+  const image = new RawImage(new Uint8ClampedArray(dims[0] * dims[1] * dims[2]).fill(255), ...dims);
+
+  describe("GroundingDinoForObjectDetection", () => {
+    const model_id = "hf-internal-testing/tiny-random-GroundingDinoForObjectDetection";
+
+    /** @type {GroundingDinoForObjectDetection} */
+    let model;
+    /** @type {GroundingDinoProcessor} */
+    let processor;
+    beforeAll(async () => {
+      model = await GroundingDinoForObjectDetection.from_pretrained(model_id, DEFAULT_MODEL_OPTIONS);
+      processor = await GroundingDinoProcessor.from_pretrained(model_id);
+    }, MAX_MODEL_LOAD_TIME);
+
+    it(
+      "forward",
+      async () => {
+        const inputs = await processor(image, text);
+        const { d_model, num_queries } = model.config;
+
+        const { logits, pred_boxes } = await model(inputs);
+        expect(logits.dims).toEqual([1, num_queries, d_model]);
+        expect(pred_boxes.dims).toEqual([1, num_queries, 4]);
+        expect(logits.max().item()).toBeCloseTo(56.237613677978516, 2);
+        expect(logits.min().item()).toEqual(-Infinity);
+        expect(pred_boxes.mean().item()).toEqual(0.2500016987323761);
+      },
+      MAX_TEST_EXECUTION_TIME,
+    );
+
+    afterAll(async () => {
+      await model?.dispose();
+    }, MAX_MODEL_DISPOSE_TIME);
+  });
+};
diff --git a/tests/models/grounding_dino/test_processor_grounding_dino.js b/tests/models/grounding_dino/test_processor_grounding_dino.js
new file mode 100644
index 000000000..e3de5676c
--- /dev/null
+++ b/tests/models/grounding_dino/test_processor_grounding_dino.js
@@ -0,0 +1,57 @@
+import { AutoProcessor, full, GroundingDinoProcessor } from "../../../src/transformers.js";
+
+import { load_cached_image } from "../../asset_cache.js";
+import { MAX_PROCESSOR_LOAD_TIME, MAX_TEST_EXECUTION_TIME } from "../../init.js";
+
+export default () => {
+  const model_id = "hf-internal-testing/tiny-random-GroundingDinoForObjectDetection";
+
+  describe("GroundingDinoProcessor", () => {
+    /** @type {GroundingDinoProcessor} */
+    let processor;
+    let images = {};
+
+    beforeAll(async () => {
+      processor = await AutoProcessor.from_pretrained(model_id);
+      images = {
+        white_image: await load_cached_image("white_image"),
+      };
+    }, MAX_PROCESSOR_LOAD_TIME);
+
+    it(
+      "Single image & text",
+      async () => {
+        const { input_ids, pixel_values } = await processor(images.white_image, "a cat.");
+        expect(input_ids.dims).toEqual([1, 5]);
+        expect(pixel_values.dims).toEqual([1, 3, 800, 800]);
+      },
+      MAX_TEST_EXECUTION_TIME,
+    );
+
+    it(
+      "post_process_grounded_object_detection",
+      async () => {
+        const outputs = {
+          logits: full([1, 900, 256], 0.5),
+          pred_boxes: full([1, 900, 4], 0.5),
+        };
+        const inputs = {
+          input_ids: full([1, 5], 1n),
+        };
+
+        const results = processor.post_process_grounded_object_detection(outputs, inputs.input_ids, {
+          box_threshold: 0.3,
+          text_threshold: 0.3,
+          target_sizes: [[360, 240]],
+        });
+        const { scores, boxes, labels } = results[0];
+        expect(scores).toHaveLength(900);
+        expect(boxes).toHaveLength(900);
+        expect(labels).toHaveLength(900);
+        expect(boxes[0]).toEqual([60, 90, 180, 270]);
+        expect(scores[0]).toBeCloseTo(0.622459352016449, 6);
+      },
+      MAX_TEST_EXECUTION_TIME,
+    );
+  });
+};
diff --git a/tests/pipelines/test_pipelines_zero_shot_object_detection.js b/tests/pipelines/test_pipelines_zero_shot_object_detection.js
index f55690507..294ba7d39 100644
--- a/tests/pipelines/test_pipelines_zero_shot_object_detection.js
+++ b/tests/pipelines/test_pipelines_zero_shot_object_detection.js
@@ -7,128 +7,180 @@ const PIPELINE_ID = "zero-shot-object-detection";
 
 export default () => {
   describe("Zero-shot Object Detection", () => {
-    const model_id = "hf-internal-testing/tiny-random-OwlViTForObjectDetection";
-
-    const candidate_labels = ["hello", "hello world"];
-
-    /** @type {ZeroShotObjectDetectionPipeline} */
-    let pipe;
-    let images;
-    beforeAll(async () => {
-      pipe = await pipeline(PIPELINE_ID, model_id, DEFAULT_MODEL_OPTIONS);
-      images = await Promise.all([load_cached_image("white_image"), load_cached_image("blue_image")]);
-    }, MAX_MODEL_LOAD_TIME);
-
-    const targets = {
-      white_image: [
-        {
-          score: 0.6028420329093933,
-          label: "hello",
-          box: { xmin: 47, ymin: 117, xmax: 62, ymax: 134 },
-        },
-        {
-          score: 0.6026064157485962,
-          label: "hello world",
-          box: { xmin: 47, ymin: 117, xmax: 62, ymax: 134 },
-        },
-        {
-          score: 0.5987668037414551,
-          label: "hello world",
-          box: { xmin: 145, ymin: 47, xmax: 160, ymax: 63 },
-        },
-        {
-          score: 0.5986272692680359,
-          label: "hello",
-          box: { xmin: 89, ymin: 131, xmax: 104, ymax: 148 },
-        },
-        {
-          score: 0.5985949039459229,
-          label: "hello world",
-          box: { xmin: 89, ymin: 131, xmax: 104, ymax: 148 },
-        },
-        // ... many more
-      ],
-
-      blue_image: [
-        {
-          score: 0.6622366309165955,
-          label: "hello",
-          box: { xmin: 48, ymin: 45, xmax: 62, ymax: 61 },
-        },
-        {
-          score: 0.6562080383300781,
-          label: "hello world",
-          box: { xmin: 48, ymin: 45, xmax: 62, ymax: 61 },
-        },
-        {
-          score: 0.6493991613388062,
-          label: "hello world",
-          box: { xmin: 34, ymin: 58, xmax: 48, ymax: 74 },
-        },
-        {
-          score: 0.6476974487304688,
-          label: "hello",
-          box: { xmin: 34, ymin: 58, xmax: 48, ymax: 74 },
-        },
-        {
-          score: 0.6391685009002686,
-          label: "hello",
-          box: { xmin: 103, ymin: 59, xmax: 117, ymax: 75 },
-        },
-        // ... many more
-      ],
-    };
+    describe("w/ post_process_object_detection", () => {
+      const model_id = "hf-internal-testing/tiny-random-OwlViTForObjectDetection";
 
-    it("should be an instance of ZeroShotObjectDetectionPipeline", () => {
-      expect(pipe).toBeInstanceOf(ZeroShotObjectDetectionPipeline);
-    });
+      const candidate_labels = ["hello", "hello world"];
 
-    describe("batch_size=1", () => {
-      it(
-        "default",
-        async () => {
-          const output = await pipe(images[0], candidate_labels);
-          expect(output).toHaveLength(512);
-
-          expect(output.slice(0, targets.white_image.length)).toBeCloseToNested(targets.white_image, 5);
-        },
-        MAX_TEST_EXECUTION_TIME,
-      );
-      it(
-        "custom (w/ top_k & threshold)",
-        async () => {
-          const top_k = 3;
-          const output = await pipe(images[0], candidate_labels, { top_k, threshold: 0.05 });
-          expect(output).toBeCloseToNested(targets.white_image.slice(0, top_k), 5);
-        },
-        MAX_TEST_EXECUTION_TIME,
-      );
-    });
+      /** @type {ZeroShotObjectDetectionPipeline} */
+      let pipe;
+      let images;
+      beforeAll(async () => {
+        pipe = await pipeline(PIPELINE_ID, model_id, DEFAULT_MODEL_OPTIONS);
+        images = await Promise.all([load_cached_image("white_image"), load_cached_image("blue_image")]);
+      }, MAX_MODEL_LOAD_TIME);
+
+      const targets = {
+        white_image: [
+          {
+            score: 0.6028420329093933,
+            label: "hello",
+            box: { xmin: 47, ymin: 117, xmax: 62, ymax: 134 },
+          },
+          {
+            score: 0.6026064157485962,
+            label: "hello world",
+            box: { xmin: 47, ymin: 117, xmax: 62, ymax: 134 },
+          },
+          {
+            score: 0.5987668037414551,
+            label: "hello world",
+            box: { xmin: 145, ymin: 47, xmax: 160, ymax: 63 },
+          },
+          {
+            score: 0.5986272692680359,
+            label: "hello",
+            box: { xmin: 89, ymin: 131, xmax: 104, ymax: 148 },
+          },
+          {
+            score: 0.5985949039459229,
+            label: "hello world",
+            box: { xmin: 89, ymin: 131, xmax: 104, ymax: 148 },
+          },
+          // ... many more
+        ],
+
+        blue_image: [
+          {
+            score: 0.6622366309165955,
+            label: "hello",
+            box: { xmin: 48, ymin: 45, xmax: 62, ymax: 61 },
+          },
+          {
+            score: 0.6562080383300781,
+            label: "hello world",
+            box: { xmin: 48, ymin: 45, xmax: 62, ymax: 61 },
+          },
+          {
+            score: 0.6493991613388062,
+            label: "hello world",
+            box: { xmin: 34, ymin: 58, xmax: 48, ymax: 74 },
+          },
+          {
+            score: 0.6476974487304688,
+            label: "hello",
+            box: { xmin: 34, ymin: 58, xmax: 48, ymax: 74 },
+          },
+          {
+            score: 0.6391685009002686,
+            label: "hello",
+            box: { xmin: 103, ymin: 59, xmax: 117, ymax: 75 },
+          },
+          // ... many more
+        ],
+      };
+
+      it("should be an instance of ZeroShotObjectDetectionPipeline", () => {
+        expect(pipe).toBeInstanceOf(ZeroShotObjectDetectionPipeline);
+      });
+
+      describe("batch_size=1", () => {
+        it(
+          "default",
+          async () => {
+            const output = await pipe(images[0], candidate_labels);
+            expect(output).toHaveLength(512);
+
+            expect(output.slice(0, targets.white_image.length)).toBeCloseToNested(targets.white_image, 5);
+          },
+          MAX_TEST_EXECUTION_TIME,
+        );
+        it(
+          "custom (w/ top_k & threshold)",
+          async () => {
+            const top_k = 3;
+            const output = await pipe(images[0], candidate_labels, { top_k, threshold: 0.05 });
+            expect(output).toBeCloseToNested(targets.white_image.slice(0, top_k), 5);
+          },
+          MAX_TEST_EXECUTION_TIME,
+        );
+      });
 
-    describe("batch_size>1", () => {
-      it(
-        "default",
-        async () => {
-          const output = await pipe(images, candidate_labels);
-          const target = Object.values(targets);
-          expect(output.map((x, i) => x.slice(0, target[i].length))).toBeCloseToNested(target, 5);
-        },
-        MAX_TEST_EXECUTION_TIME,
-      );
-      it(
-        "custom (w/ top_k & threshold)",
-        async () => {
-          const top_k = 3;
-          const output = await pipe(images, candidate_labels, { top_k, threshold: 0.05 });
-          const target = Object.values(targets).map((x) => x.slice(0, top_k));
-          expect(output).toBeCloseToNested(target, 5);
-        },
-        MAX_TEST_EXECUTION_TIME,
-      );
+      describe("batch_size>1", () => {
+        it(
+          "default",
+          async () => {
+            const output = await pipe(images, candidate_labels);
+            const target = Object.values(targets);
+            expect(output.map((x, i) => x.slice(0, target[i].length))).toBeCloseToNested(target, 5);
+          },
+          MAX_TEST_EXECUTION_TIME,
+        );
+        it(
+          "custom (w/ top_k & threshold)",
+          async () => {
+            const top_k = 3;
+            const output = await pipe(images, candidate_labels, { top_k, threshold: 0.05 });
+            const target = Object.values(targets).map((x) => x.slice(0, top_k));
+            expect(output).toBeCloseToNested(target, 5);
+          },
+          MAX_TEST_EXECUTION_TIME,
+        );
+      });
+
+      afterAll(async () => {
+        await pipe.dispose();
+      }, MAX_MODEL_DISPOSE_TIME);
     });
 
-    afterAll(async () => {
-      await pipe.dispose();
-    }, MAX_MODEL_DISPOSE_TIME);
+    describe("w/ post_process_grounded_object_detection", () => {
+      const model_id = "hf-internal-testing/tiny-random-GroundingDinoForObjectDetection";
+
+      const candidate_labels = ["a cat."];
+
+      /** @type {ZeroShotObjectDetectionPipeline} */
+      let pipe;
+      let image;
+      beforeAll(async () => {
+        pipe = await pipeline(PIPELINE_ID, model_id, DEFAULT_MODEL_OPTIONS);
+        image = await load_cached_image("white_image");
+      }, MAX_MODEL_LOAD_TIME);
+
+      const target = [
+        { box: { xmax: 112, xmin: -111, ymax: 0, ymin: 0 }, label: "a cat. [SEP]", score: 1 },
+        { box: { xmax: 112, xmin: -111, ymax: 0, ymin: 0 }, label: "a cat. [SEP]", score: 1 },
+        { box: { xmax: 112, xmin: -111, ymax: 0, ymin: 0 }, label: "a cat. [SEP]", score: 1 },
+        // ... many more
+      ];
+
+      it("should be an instance of ZeroShotObjectDetectionPipeline", () => {
+        expect(pipe).toBeInstanceOf(ZeroShotObjectDetectionPipeline);
+      });
+
+      describe("batch_size=1", () => {
+        it(
+          "default",
+          async () => {
+            const output = await pipe(image, candidate_labels);
+            expect(output).toHaveLength(900);
+            expect(output.slice(0, target.length)).toBeCloseToNested(target, 5);
+          },
+          MAX_TEST_EXECUTION_TIME,
+        );
+        it(
+          "custom (w/ top_k & threshold)",
+          async () => {
+            const top_k = 3;
+            const output = await pipe(image, candidate_labels, { top_k, threshold: 0.05 });
+            expect(output).toBeCloseToNested(target.slice(0, top_k), 5);
+          },
+          MAX_TEST_EXECUTION_TIME,
+        );
+      });
+
+      afterAll(async () => {
+        await pipe.dispose();
+      }, MAX_MODEL_DISPOSE_TIME);
+    });
   });
 };
diff --git a/tests/utils/tensor.test.js b/tests/utils/tensor.test.js
index 684ded602..dacd46789 100644
--- a/tests/utils/tensor.test.js
+++ b/tests/utils/tensor.test.js
@@ -1,4 +1,4 @@
-import { Tensor, cat, mean, stack, layer_norm } from "../../src/transformers.js";
+import { Tensor, cat, stack, layer_norm, ones_like, zeros_like, full_like, rand, std_mean } from "../../src/transformers.js";
 import { init } from "../init.js";
 import { compare } from "../test_utils.js";
 
@@ -6,7 +6,7 @@ init();
 
 describe("Tensor operations", () => {
   describe("cat", () => {
-    it("should concatenate on dim=0", async () => {
+    it("should concatenate on dim=0", () => {
       const t1 = new Tensor("float32", [1, 2, 3], [1, 3]);
       const t2 = new Tensor("float32", [4, 5, 6, 7, 8, 9], [2, 3]);
       const t3 = new Tensor("float32", [10, 11, 12], [1, 3]);
@@ -23,7 +23,7 @@ describe("Tensor operations", () => {
       compare(concatenated2, target2, 1e-3);
     });
 
-    it("should concatenate on dim=1", async () => {
+    it("should concatenate on dim=1", () => {
       const t1 = new Tensor("float32", [1, 2, 3, -1, -2, -3], [2, 3, 1]);
       const t2 = new Tensor("float32", [4, -4], [2, 1, 1]);
       const t3 = new Tensor("float32", [5, 6, -5, -6], [2, 2, 1]);
@@ -40,7 +40,7 @@ describe("Tensor operations", () => {
       compare(concatenated2, target2, 1e-3);
     });
 
-    it("should concatenate on dim=-2", async () => {
+    it("should concatenate on dim=-2", () => {
       const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6, 11, 12, 13, 14, 15, 16], [2, 1, 3, 2]);
       const t2 = new Tensor("float32", [7, 8, 9, 10, 17, 18, 19, 20], [2, 1, 2, 2]);
 
@@ -55,7 +55,7 @@ describe("Tensor operations", () => {
   });
 
   describe("slice", () => {
-    it("should return a given row dim", async () => {
+    it("should return a given row dim", () => {
       const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [3, 2]);
       const t2 = t1.slice(1);
       const target = new Tensor("float32", [3, 4], [2]);
@@ -63,7 +63,7 @@ describe("Tensor operations", () => {
       compare(t2, target);
     });
 
-    it("should return a range of rows", async () => {
+    it("should return a range of rows", () => {
       const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [3, 2]);
       const t2 = t1.slice([1, 3]);
       const target = new Tensor("float32", [3, 4, 5, 6], [2, 2]);
@@ -71,7 +71,7 @@ describe("Tensor operations", () => {
       compare(t2, target);
     });
 
-    it("should return a crop", async () => {
+    it("should return a crop", () => {
       const t1 = new Tensor(
         "float32",
         Array.from({ length: 28 }, (_, i) => i + 1),
@@ -88,7 +88,7 @@ describe("Tensor operations", () => {
   describe("stack", () => {
     const t1 = new Tensor("float32", [0, 1, 2, 3, 4, 5], [1, 3, 2]);
 
-    it("should stack on dim=0", async () => {
+    it("should stack on dim=0", () => {
       const target1 = new Tensor("float32", [0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5], [2, 1, 3, 2]);
       const target2 = new Tensor("float32", [0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5], [3, 1, 3, 2]);
 
@@ -101,7 +101,7 @@ describe("Tensor operations", () => {
       compare(stacked2, target2, 1e-3);
     });
 
-    it("should stack on dim=1", async () => {
+    it("should stack on dim=1", () => {
       const target1 = new Tensor("float32", [0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5], [1, 2, 3, 2]);
       const target2 = new Tensor("float32", [0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5], [1, 3, 3, 2]);
 
@@ -114,7 +114,7 @@ describe("Tensor operations", () => {
       compare(stacked2, target2, 1e-3);
     });
 
-    it("should stack on dim=-1", async () => {
+    it("should stack on dim=-1", () => {
       const target1 = new Tensor("float32", [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5], [1, 3, 2, 2]);
       const target2 = new Tensor("float32", [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5], [1, 3, 2, 3]);
 
@@ -129,7 +129,7 @@ describe("Tensor operations", () => {
   });
 
   describe("permute", () => {
-    it("should permute", async () => {
+    it("should permute", () => {
       const x = new Tensor("float32", [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [2, 3, 4]);
       // Permute axes to (0, 1, 2) - No change
       const permuted_1 = x.permute(0, 1, 2);
@@ -164,7 +164,7 @@ describe("Tensor operations", () => {
   });
 
   describe("map", () => {
-    it("should double", async () => {
+    it("should double", () => {
       const original = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
       const target = new Tensor("float32", [2, 4, 6, 8, 10, 12], [2, 3]);
 
@@ -174,31 +174,126 @@ describe("Tensor operations", () => {
   });
 
   describe("mean", () => {
-    it("should calculate mean", async () => {
-      const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3, 1]);
-
+    const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3, 1]);
+    it("should calculate mean over the entire tensor", () => {
       const target = new Tensor("float32", [3.5], []);
+      compare(t1.mean(), target, 1e-3);
+    });
 
+    it("should calculate mean over dimension 0", () => {
       const target0 = new Tensor("float32", [2.5, 3.5, 4.5], [3, 1]);
+      compare(t1.mean(0), target0, 1e-3);
+    });
+
+    it("should calculate mean over dimension 1", () => {
       const target1 = new Tensor("float32", [2, 5], [2, 1]);
+      compare(t1.mean(1), target1, 1e-3);
+    });
+
+    it("should calculate mean over dimension -1", () => {
       const target2 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
+      compare(t1.mean(-1), target2, 1e-3);
+    });
+  });
+
+  describe("std_mean", () => {
+    it("should return std_mean for the entire tensor", () => {
+      const t = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
+      const [stdVal, meanVal] = std_mean(t);
+      compare(stdVal, new Tensor("float32", [1.8708287477493286], []), 1e-3);
+      compare(meanVal, new Tensor("float32", [3.5], []), 1e-3);
+    });
+  });
+
+  describe("min", () => {
+    it("should return the minimum over the entire tensor", () => {
+      const t1 = new Tensor("float32", [3, -2, 5, 0], [2, 2]);
+      const target = new Tensor("float32", [-2], []);
+      const result = t1.min();
+      compare(result, target, 1e-3);
+    });
+
+    it("should return the minimum over dimension 1", () => {
+      const t2 = new Tensor("float32", [4, 2, -1, 0, 6, 5], [3, 2]);
+      const target = new Tensor("float32", [2, -1, 5], [3]);
+      const result = t2.min(1);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("max", () => {
+    it("should return the maximum over the entire tensor", () => {
+      const t1 = new Tensor("float32", [3, 10, -2, 7], [2, 2]);
+      const target = new Tensor("float32", [10], []);
+      const result = t1.max();
+      compare(result, target, 1e-3);
+    });
 
-      let avg = mean(t1);
-      compare(avg, target, 1e-3);
+    it("should return the maximum over dimension 0", () => {
+      const t2 = new Tensor("float32", [1, 2, 4, 5, 9, 3], [3, 2]);
+      const target = new Tensor("float32", [9, 5], [2]);
+      const result = t2.max(0);
+      compare(result, target, 1e-3);
+    });
+  });
 
-      let avg0 = mean(t1, 0);
-      compare(avg0, target0, 1e-3);
+  describe("sum", () => {
+    it("should calculate sum over entire tensor", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
+      const target = new Tensor("float32", [21], []);
+      const result = t1.sum();
+      compare(result, target, 1e-3);
+    });
 
-      let avg1 = mean(t1, 1);
-      compare(avg1, target1, 1e-3);
+    it("should calculate sum over dimension 0", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
+      const target = new Tensor("float32", [5, 7, 9], [3]);
+      const result = t1.sum(0);
+      compare(result, target, 1e-3);
+    });
+
+    it("should calculate sum over dimension 1", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
+      const target = new Tensor("float32", [6, 15], [2]);
+      const result = t1.sum(1);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("norm", () => {
+    it("should calculate L2 norm over entire tensor", () => {
+      const t1 = new Tensor("float32", [3, 4], [2]);
+      const target = new Tensor("float32", [5], []);
+      const result = t1.norm();
+      compare(result, target, 1e-3);
+    });
 
-      let avg2 = mean(t1, 2);
-      compare(avg2, target2, 1e-3);
+    it("should calculate L2 norm over dimension 0", () => {
+      const t1 = new Tensor("float32", [3, 4, 6, 8], [2, 2]);
+      const target = new Tensor("float32", [6.7082, 8.9443], [2]);
+      const result = t1.norm(2, 0);
+      compare(result, target, 1e-2);
+    });
+  });
+
+  describe("normalize", () => {
+    it("should normalize a vector correctly", () => {
+      const t1 = new Tensor("float32", [3, 4], [1, 2]);
+      const target = new Tensor("float32", [0.6, 0.8], [1, 2]);
+      const normalized = t1.normalize();
+      compare(normalized, target, 1e-3);
+    });
+
+    it("should normalize along dimension", () => {
+      const t1 = new Tensor("float32", [1, 2, 2, 3], [2, 2]);
+      const target = new Tensor("float32", [0.4472, 0.8944, 0.5547, 0.8321], [2, 2]);
+      const normalized = t1.normalize();
+      compare(normalized, target, 1e-3);
     });
   });
 
   describe("layer_norm", () => {
-    it("should calculate layer norm", async () => {
+    it("should calculate layer norm", () => {
       const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
 
       const target = new Tensor("float32", [-1.2247356176376343, 0.0, 1.2247356176376343, -1.2247357368469238, -1.1920928955078125e-7, 1.2247354984283447], [2, 3]);
@@ -208,8 +303,234 @@ describe("Tensor operations", () => {
     });
   });
 
+  describe("sigmoid", () => {
+    it("should apply the sigmoid function to each element in the tensor", () => {
+      const t1 = new Tensor("float32", [0, 1, -1, 5, -5], [5]);
+      const target = new Tensor("float32", [0.5, 1 / (1 + Math.exp(-1)), 1 / (1 + Math.exp(1)), 1 / (1 + Math.exp(-5)), 1 / (1 + Math.exp(5))], [5]);
+
+      const result = t1.sigmoid();
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("tolist", () => {
+    it("should return nested arrays for a 2D tensor", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [2, 2]);
+      const arr = t1.tolist();
+      compare(arr, [
+        [1, 2],
+        [3, 4],
+      ]);
+    });
+  });
+
+  describe("mul", () => {
+    it("should multiply constant", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [2, 2]);
+      const target = new Tensor("float32", [2, 4, 6, 8], [2, 2]);
+
+      const result = t1.mul(2);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("div", () => {
+    it("should divide constant", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [2, 2]);
+      const target = new Tensor("float32", [0.5, 1, 1.5, 2], [2, 2]);
+
+      const result = t1.div(2);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("add", () => {
+    it("should add constant", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [2, 2]);
+      const target = new Tensor("float32", [3, 4, 5, 6], [2, 2]);
+
+      const result = t1.add(2);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("sub", () => {
+    it("should subtract constant", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [2, 2]);
+      const target = new Tensor("float32", [-1, 0, 1, 2], [2, 2]);
+
+      const result = t1.sub(2);
+      compare(result, target, 1e-3);
+    });
+  });
+  describe("gt", () => {
+    it("should perform element-wise greater than comparison with a scalar", () => {
+      const t1 = new Tensor("float32", [1, 5, 3, 7], [4]);
+      const target = new Tensor("bool", [0, 1, 0, 1], [4]);
+      const result = t1.gt(4);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("lt", () => {
+    it("should perform element-wise less than comparison with a scalar", () => {
+      const t1 = new Tensor("float32", [1, 5, 3, 7], [4]);
+      const target = new Tensor("bool", [1, 0, 1, 0], [4]);
+      const result = t1.lt(4);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("squeeze", () => {
+    it("should remove all dimensions of size 1", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [1, 4]);
+      const target = new Tensor("float32", [1, 2, 3, 4], [4]);
+
+      const result = t1.squeeze();
+      compare(result, target, 1e-3);
+    });
+    it("should remove a specified dimension", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [1, 1, 2, 2]);
+      const result = t1.squeeze(1);
+      const target = new Tensor("float32", [1, 2, 3, 4], [1, 2, 2]);
+      compare(result, target, 1e-3);
+    });
+    it("should remove multiple dimensions", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [1, 1, 2, 1, 2]);
+      const result = t1.squeeze([0, 3]);
+      const target = new Tensor("float32", [1, 2, 3, 4], [1, 2, 2]);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("unsqueeze", () => {
+    it("should add a dimension at the specified axis", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [4]);
+      const target = new Tensor("float32", [1, 2, 3, 4], [1, 4]);
+
+      const result = t1.unsqueeze(0);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("flatten", () => {
+    it("should flatten a 2D tensor into 1D by default", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
+      const target = new Tensor("float32", [1, 2, 3, 4, 5, 6], [6]);
+
+      const result = t1.flatten();
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("neg", () => {
+    it("should compute the negative of each element in the tensor", () => {
+      const t1 = new Tensor("float32", [1, -2, 0, 3], [4]);
+      const target = new Tensor("float32", [-1, 2, -0, -3], [4]);
+
+      const result = t1.neg();
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("view", () => {
+    it("should reshape the tensor to the specified dimensions", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
+      const target = new Tensor("float32", [1, 2, 3, 4, 5, 6], [3, 2]);
+
+      const result = t1.view(3, 2);
+      compare(result, target, 1e-3);
+    });
+
+    it("should reshape the tensor with an inferred dimension (-1)", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
+      const target = new Tensor("float32", [1, 2, 3, 4, 5, 6], [1, 6]);
+
+      const result = t1.view(1, -1);
+      compare(result, target, 1e-3);
+    });
+
+    it("should throw if multiple inferred dimensions are used", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
+      expect(() => t1.view(-1, -1)).toThrow();
+    });
+  });
+
+  describe("clamp", () => {
+    it("should clamp values between min and max", () => {
+      const t1 = new Tensor("float32", [-2, -1, 0, 1, 2, 3], [6]);
+      const target = new Tensor("float32", [-1, -1, 0, 1, 2, 2], [6]);
+
+      const result = t1.clamp(-1, 2);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("round", () => {
+    it("should round elements to the nearest integer", () => {
+      const t1 = new Tensor("float32", [0.1, 1.4, 2.5, 3.9, -1.2], [5]);
+      const target = new Tensor("float32", [0, 1, 3, 4, -1], [5]);
+
+      const result = t1.round();
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("ones_like", () => {
+    it("should create a tensor of all ones with the same shape as the input", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [2, 2]);
+      const result = ones_like(t1);
+      const target = new Tensor("int64", [1n, 1n, 1n, 1n], [2, 2]);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("zeros_like", () => {
+    it("should create a tensor of all zeros with the same shape as the input", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [2, 2]);
+      const result = zeros_like(t1);
+      const target = new Tensor("int64", [0n, 0n, 0n, 0n], [2, 2]);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("full_like", () => {
+    it("should create a tensor filled with a number, matching the shape of the original", () => {
+      const t1 = new Tensor("float32", [1, 2, 3, 4], [2, 2]);
+      const result = full_like(t1, 10);
+      const target = new Tensor("float32", [10, 10, 10, 10], [2, 2]);
+      compare(result, target, 1e-3);
+    });
+    it("should create a boolean tensor with the same shape", () => {
+      const t2 = new Tensor("bool", [true, false], [2]);
+      const result = full_like(t2, true);
+      const target = new Tensor("bool", [true, true], [2]);
+      compare(result, target, 1e-3);
+    });
+
+    it("should create a bigint tensor with the same shape", () => {
+      const t3 = new Tensor("int64", [1n, 2n], [2]);
+      const result = full_like(t3, 123n);
+      const target = new Tensor("int64", [123n, 123n], [2]);
+      compare(result, target, 1e-3);
+    });
+  });
+
+  describe("rand", () => {
+    it("should create a tensor of random values between 0 and 1 with the given shape", () => {
+      const shape = [2, 2];
+      const random = rand(shape);
+      expect(random.type).toBe("float32");
+      expect(random.dims).toEqual(shape);
+      random.data.forEach((val) => {
+        expect(val).toBeGreaterThanOrEqual(0);
+        expect(val).toBeLessThan(1);
+      });
+    });
+  });
+
   describe("to", () => {
-    it("float32 to int32 (number to number)", async () => {
+    it("float32 to int32 (number to number)", () => {
       const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
 
       const target = new Tensor("int32", [1, 2, 3, 4, 5, 6], [2, 3]);
@@ -217,7 +538,7 @@ describe("Tensor operations", () => {
       const t2 = t1.to("int32");
       compare(t2, target);
     });
-    it("float32 to int64 (number to bigint)", async () => {
+    it("float32 to int64 (number to bigint)", () => {
       const t1 = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
 
       const target = new Tensor("int64", [1n, 2n, 3n, 4n, 5n, 6n], [2, 3]);
@@ -225,7 +546,7 @@ describe("Tensor operations", () => {
       const t2 = t1.to("int64");
       compare(t2, target);
     });
-    it("int64 to float32 (bigint to number)", async () => {
+    it("int64 to float32 (bigint to number)", () => {
       const t1 = new Tensor("int64", [1n, 2n, 3n, 4n, 5n, 6n], [2, 3]);
 
       const target = new Tensor("float32", [1, 2, 3, 4, 5, 6], [2, 3]);
@@ -233,7 +554,7 @@ describe("Tensor operations", () => {
       const t2 = t1.to("float32");
       compare(t2, target);
     });
-    it("int32 to uint32", async () => {
+    it("int32 to uint32", () => {
       const t1 = new Tensor("int32", [-1, 2, -3, 4, -5, 6], [2, 3]);
 
       const target = new Tensor("uint32", [4294967295, 2, 4294967293, 4, 4294967291, 6], [2, 3]);
@@ -241,7 +562,7 @@ describe("Tensor operations", () => {
       const t2 = t1.to("uint32");
       compare(t2, target);
     });
-    it("int16 to int8 (overflow)", async () => {
+    it("int16 to int8 (overflow)", () => {
       const t1 = new Tensor("int16", [0, 1, 128, 256, 257, 512], [2, 3]);
 
       const target = new Tensor("int8", [0, 1, -128, 0, 1, 0], [2, 3]);
