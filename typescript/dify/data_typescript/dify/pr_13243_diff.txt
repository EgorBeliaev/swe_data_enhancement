diff --git a/api/core/model_runtime/model_providers/openai/llm/llm.py b/api/core/model_runtime/model_providers/openai/llm/llm.py
index 05872020583664..e9d59a4ae4997a 100644
--- a/api/core/model_runtime/model_providers/openai/llm/llm.py
+++ b/api/core/model_runtime/model_providers/openai/llm/llm.py
@@ -619,9 +619,9 @@ def _chat_generate(
         # clear illegal prompt messages
         prompt_messages = self._clear_illegal_prompt_messages(model, prompt_messages)
 
-        # o1 compatibility
+        # o1, o3 compatibility
         block_as_stream = False
-        if model.startswith("o1"):
+        if model.startswith(("o1", "o3")):
             if "max_tokens" in model_parameters:
                 model_parameters["max_completion_tokens"] = model_parameters["max_tokens"]
                 del model_parameters["max_tokens"]
@@ -941,7 +941,7 @@ def _clear_illegal_prompt_messages(self, model: str, prompt_messages: list[Promp
                                 ]
                             )
 
-        if model.startswith("o1"):
+        if model.startswith(("o1", "o3")):
             system_message_count = len([m for m in prompt_messages if isinstance(m, SystemPromptMessage)])
             if system_message_count > 0:
                 new_prompt_messages = []
@@ -1053,7 +1053,7 @@ def _num_tokens_from_messages(
             model = model.split(":")[1]
 
         # Currently, we can use gpt4o to calculate chatgpt-4o-latest's token.
-        if model == "chatgpt-4o-latest" or model.startswith("o1"):
+        if model == "chatgpt-4o-latest" or model.startswith(("o1", "o3")):
             model = "gpt-4o"
 
         try:
@@ -1068,7 +1068,7 @@ def _num_tokens_from_messages(
             tokens_per_message = 4
             # if there's a name, the role is omitted
             tokens_per_name = -1
-        elif model.startswith("gpt-3.5-turbo") or model.startswith("gpt-4") or model.startswith("o1"):
+        elif model.startswith("gpt-3.5-turbo") or model.startswith("gpt-4") or model.startswith(("o1", "o3")):
             tokens_per_message = 3
             tokens_per_name = 1
         else:
diff --git a/api/core/model_runtime/model_providers/openai/llm/o1-2024-12-17.yaml b/api/core/model_runtime/model_providers/openai/llm/o1-2024-12-17.yaml
index 7acbd0e2b1166b..643258a23e14c6 100644
--- a/api/core/model_runtime/model_providers/openai/llm/o1-2024-12-17.yaml
+++ b/api/core/model_runtime/model_providers/openai/llm/o1-2024-12-17.yaml
@@ -16,6 +16,19 @@ parameter_rules:
     default: 50000
     min: 1
     max: 50000
+  - name: reasoning_effort
+    label:
+      zh_Hans: 推理工作
+      en_US: reasoning_effort
+    type: string
+    help:
+      zh_Hans: 限制推理模型的推理工作
+      en_US: constrains effort on reasoning for reasoning models
+    required: false
+    options:
+      - low
+      - medium
+      - high
   - name: response_format
     label:
       zh_Hans: 回复格式
diff --git a/api/core/model_runtime/model_providers/openai/llm/o1.yaml b/api/core/model_runtime/model_providers/openai/llm/o1.yaml
index 3a84cf418ec873..53d5f719cc3f4e 100644
--- a/api/core/model_runtime/model_providers/openai/llm/o1.yaml
+++ b/api/core/model_runtime/model_providers/openai/llm/o1.yaml
@@ -17,6 +17,19 @@ parameter_rules:
     default: 50000
     min: 1
     max: 50000
+  - name: reasoning_effort
+    label:
+      zh_Hans: 推理工作
+      en_US: reasoning_effort
+    type: string
+    help:
+      zh_Hans: 限制推理模型的推理工作
+      en_US: constrains effort on reasoning for reasoning models
+    required: false
+    options:
+      - low
+      - medium
+      - high
   - name: response_format
     label:
       zh_Hans: 回复格式
diff --git a/api/core/model_runtime/model_providers/openai/llm/o3-mini-2025-01-31.yaml b/api/core/model_runtime/model_providers/openai/llm/o3-mini-2025-01-31.yaml
index 25f1d262c0d43d..3f717bf08dcdce 100644
--- a/api/core/model_runtime/model_providers/openai/llm/o3-mini-2025-01-31.yaml
+++ b/api/core/model_runtime/model_providers/openai/llm/o3-mini-2025-01-31.yaml
@@ -14,6 +14,19 @@ parameter_rules:
     default: 100000
     min: 1
     max: 100000
+  - name: reasoning_effort
+    label:
+      zh_Hans: 推理工作
+      en_US: reasoning_effort
+    type: string
+    help:
+      zh_Hans: 限制推理模型的推理工作
+      en_US: constrains effort on reasoning for reasoning models
+    required: false
+    options:
+      - low
+      - medium
+      - high
   - name: response_format
     label:
       zh_Hans: 回复格式
diff --git a/api/core/model_runtime/model_providers/openai/llm/o3-mini.yaml b/api/core/model_runtime/model_providers/openai/llm/o3-mini.yaml
index 2a717aea8e8aa7..755fc005958a32 100644
--- a/api/core/model_runtime/model_providers/openai/llm/o3-mini.yaml
+++ b/api/core/model_runtime/model_providers/openai/llm/o3-mini.yaml
@@ -14,6 +14,19 @@ parameter_rules:
     default: 100000
     min: 1
     max: 100000
+  - name: reasoning_effort
+    label:
+      zh_Hans: 推理工作
+      en_US: reasoning_effort
+    type: string
+    help:
+      zh_Hans: 限制推理模型的推理工作
+      en_US: constrains effort on reasoning for reasoning models
+    required: false
+    options:
+      - low
+      - medium
+      - high
   - name: response_format
     label:
       zh_Hans: 回复格式
diff --git a/api/poetry.lock b/api/poetry.lock
index eb6a2b5b8039b4..88689407624283 100644
--- a/api/poetry.lock
+++ b/api/poetry.lock
@@ -6562,15 +6562,15 @@ sympy = "*"
 
 [[package]]
 name = "openai"
-version = "1.52.2"
+version = "1.61.0"
 description = "The official Python library for the openai API"
 optional = false
-python-versions = ">=3.7.1"
+python-versions = ">=3.8"
 groups = ["main"]
 markers = "python_version == \"3.11\" or python_version >= \"3.12\""
 files = [
-    {file = "openai-1.52.2-py3-none-any.whl", hash = "sha256:57e9e37bc407f39bb6ec3a27d7e8fb9728b2779936daa1fcf95df17d3edfaccc"},
-    {file = "openai-1.52.2.tar.gz", hash = "sha256:87b7d0f69d85f5641678d414b7ee3082363647a5c66a462ed7f3ccb59582da0d"},
+    {file = "openai-1.61.0-py3-none-any.whl", hash = "sha256:e8c512c0743accbdbe77f3429a1490d862f8352045de8dc81969301eb4a4f666"},
+    {file = "openai-1.61.0.tar.gz", hash = "sha256:216f325a24ed8578e929b0f1b3fb2052165f3b04b0461818adaa51aa29c71f8a"},
 ]
 
 [package.dependencies]
@@ -6585,6 +6585,7 @@ typing-extensions = ">=4.11,<5"
 
 [package.extras]
 datalib = ["numpy (>=1)", "pandas (>=1.2.3)", "pandas-stubs (>=1.1.0.11)"]
+realtime = ["websockets (>=13,<15)"]
 
 [[package]]
 name = "opencensus"
@@ -12429,4 +12430,4 @@ cffi = ["cffi (>=1.11)"]
 [metadata]
 lock-version = "2.1"
 python-versions = ">=3.11,<3.13"
-content-hash = "14896fb8a4495238d70515986341128d0096ba130c3acce127a29a903a59a811"
+content-hash = "786f04736264fce986f53ea41aa0f5b94e18df807a9aad0aa621cc0ac7078065"
diff --git a/api/pyproject.toml b/api/pyproject.toml
index f887b82be2b20b..2c93f36324b892 100644
--- a/api/pyproject.toml
+++ b/api/pyproject.toml
@@ -58,7 +58,7 @@ nomic = "~3.1.2"
 novita-client = "~0.5.7"
 numpy = "~1.26.4"
 oci = "~2.135.1"
-openai = "~1.52.0"
+openai = "~1.61.0"
 openpyxl = "~3.1.5"
 opik = "~1.3.4"
 pandas = { version = "~2.2.2", extras = ["performance", "excel"] }
