[
    {
        "title": "Add support for op_block_list"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "**Background**\r\nAdded a new argument to the quantize script called \"op_block_list.\" If op_block_list is provided, do not quantize those ops. Sometimes you have ops that are incompatible with quantization.\r\n\r\n**Test Plan**\r\n\r\n**Regession Test**\r\n- This test is just making sure there are no regressions in behaviour when you don't provide an `op_block_list`\r\n- `git clone https://huggingface.co/onnx-models/sentence-t5-base-onnx`\r\n- This model has a `/model/model.0/auto_model/encoder/block.0/layer.0/SelfAttention/Range` node so we are checking that it is still excluded because it is part of the default exclude types (https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/float16.py#L108)\r\n- Test with `main` branch of transformers.js\r\n- `git checkout . && rm -rf ./*_*.onnx || true && PYTHONPATH=../transformers.js ../transformers.js/.venv/bin/python3 -m scripts.quantize --input_folder . --output_folder . --mode fp16`\r\n- Run stat: `stat -f \"%z\" model_fp16.onnx`\r\n```\r\n220762121\r\n```\r\n- Now test with this PR branch\r\n- `git checkout . && rm -rf ./*_*.onnx || true && PYTHONPATH=../transformers.js ../transformers.js/.venv/bin/python3 -m scripts.quantize --input_folder . --output_folder . --mode fp16`\r\n- Run stat: `stat -f \"%z\" model_fp16.onnx`\r\n```\r\n220762121\r\n```\r\n- **File size is the same so test passed**\r\n\r\n\r\n**Qwen2-VL Test**\r\n- Here we actually check that the `op_block_list` works\r\n- Checkout this PR for transformers.js\r\n- Clone this repo https://huggingface.co/pdufour/Qwen2-VL-2B-Instruct-ONNX-Q4-F16\r\n- Delete already quantized models so it doesn't double quant them\r\n- `rm -rf onnx/*_*_*.onnx`\r\n- Quantize the **A** model **without** the op_block_list\r\n- `PYTHONPATH=../transformers.js ../transformers.js/.venv/bin/python3 -m scripts.quantize --input_folder ./onnx --output_folder ./onnx-dest --mode q4f16`\r\n- Run the infer script: `python3 infer.py Qwen/Qwen2-VL-2B-Instruct ./onnx`\r\n  - Expected result: No error.\r\n  - Actual Result: `onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from ./onnx/QwenVL_A_q4f16.onnx failed:Type Error: Type parameter (T) of Optype (Sub) bound to different types (tensor(float) and tensor(float16) in node (/Sub).`\r\n- Now we quantize **with** the op block list\r\n- `rm -rf onnx/*_*_*.onnx`\r\n- `PYTHONPATH=../transformers.js ../transformers.js/.venv/bin/python3 -m scripts.quantize --input_folder ./onnx --output_folder ./onnx --mode q4f16 --op_block_list Conv DynamicQuantizeLinear DequantizeLinear Resize`\r\n- Try infer script again\r\n  - `python3 infer.py Qwen/Qwen2-VL-2B-Instruct ./onnx`\r\n  - ```The image shows a vintage teal-colored...```\r\n  - You see the correct result\r\n"
    },
    {
        "author": {
            "login": "xenova"
        },
        "body": ""
    },
    {
        "author": {
            "login": "xenova"
        },
        "body": ""
    },
    {
        "author": {
            "login": "xenova"
        },
        "body": ""
    },
    {
        "author": {
            "login": "xenova"
        },
        "body": "Thanks!"
    },
    {
        "author": {
            "login": "xenova"
        },
        "body": "Very useful! Thanks! Just for my testing, is the model you're testing available on the Hugging Face hub?"
    },
    {
        "author": {
            "login": "pdufour"
        },
        "body": "@xenova I have the exported model available here https://huggingface.co/pdufour/Qwen2-VL-2B-Instruct-ONNX-Q4-F16 but I haven't uploaded the source files. It might be easier to try on a smaller example. I've updated the description of the PR if you want to try that one."
    },
    {
        "author": {
            "login": "pdufour"
        },
        "body": "One curious behaviour is that if you do provide a op_block_list it doesn't include the defaults anymore https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/float16.py#L141. I am not sure if you want that or not, could also include the defaults if that's preferred. But then it's impossible to clear them."
    },
    {
        "author": {
            "login": "xenova"
        },
        "body": "> One curious behaviour is that if you do provide a op_block_list it doesn't include the defaults anymore https://github.com/microsoft/onnxconverter-common/blob/master/onnxconverter_common/float16.py#L141. I am not sure if you want that or not, could also include the defaults if that's preferred. But then it's impossible to clear them.\r\n\r\nGood point! We should then default to `None` instead of an empty array."
    },
    {
        "author": {
            "login": "pdufour"
        },
        "body": "@xenova Updated PR to use None and added some more comprehensive tests in the description."
    },
    {
        "author": {
            "login": "HuggingFaceDocBuilderDev"
        },
        "body": "The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers.js/pr_1036). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update."
    }
]