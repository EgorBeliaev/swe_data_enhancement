[
    {
        "title": "Add support for visualizing self-attention heatmaps + sequence classifier outputs w/ attentions"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "This PR allows users to visualize per-layer per-head attentions. Based on [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DINO/Visualize_self_attention_of_DINO.ipynb) by @NielsRogge.\r\n\r\nExample usage:\r\n```js\r\nimport { AutoProcessor, AutoModelForImageClassification, interpolate_4d, RawImage } from \"@huggingface/transformers\";\r\n\r\n// Load model and processor\r\nconst model_id = \"onnx-community/dinov2-with-registers-small-with-attentions\";\r\nconst model = await AutoModelForImageClassification.from_pretrained(model_id);\r\nconst processor = await AutoProcessor.from_pretrained(model_id);\r\n\r\n// Load image from URL\r\nconst image = await RawImage.read(\"https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cats.jpg\");\r\n\r\n// Pre-process image\r\nconst inputs = await processor(image);\r\n\r\n// Perform inference\r\nconst { logits, attentions } = await model(inputs);\r\n\r\n// Get the predicted class\r\nconst cls = logits[0].argmax().item();\r\nconst label = model.config.id2label[cls];\r\nconsole.log(`Predicted class: ${label}`);\r\n\r\n// Set config values\r\nconst patch_size = model.config.patch_size;\r\nconst [width, height] = inputs.pixel_values.dims.slice(-2);\r\nconst w_featmap = Math.floor(width / patch_size);\r\nconst h_featmap = Math.floor(height / patch_size);\r\nconst num_heads = model.config.num_attention_heads;\r\nconst num_cls_tokens = 1;\r\nconst num_register_tokens = model.config.num_register_tokens ?? 0;\r\n\r\n// Visualize attention maps\r\nconst selected_attentions = attentions\r\n    .at(-1) // we are only interested in the attention maps of the last layer\r\n    .slice(0, null, 0, [num_cls_tokens + num_register_tokens, null])\r\n    .view(num_heads, 1, w_featmap, h_featmap);\r\n\r\nconst upscaled = await interpolate_4d(selected_attentions, {\r\n    size: [width, height],\r\n    mode: \"nearest\",\r\n});\r\n\r\nfor (let i = 0; i < num_heads; ++i) {\r\n    const head_attentions = upscaled[i];\r\n    const minval = head_attentions.min().item();\r\n    const maxval = head_attentions.max().item();\r\n    const image = RawImage.fromTensor(\r\n        head_attentions\r\n            .sub_(minval)\r\n            .div_(maxval - minval)\r\n            .mul_(255)\r\n            .to(\"uint8\"),\r\n    );\r\n    await image.save(`attn-head-${i}.png`);\r\n}\r\n```\r\n\r\nAttention head heatmaps for last layer:\r\n| ![attn-head-0](https://github.com/user-attachments/assets/928c3d97-2c67-4ddb-9e9c-2a06745a532f) | ![attn-head-1](https://github.com/user-attachments/assets/e7725424-10fd-4a47-8350-8f367d21657d) | ![attn-head-2](https://github.com/user-attachments/assets/81790060-f4bf-4e5c-8d35-a9246acb9a36) |\r\n|--------|--------|--------|\r\n| ![attn-head-3](https://github.com/user-attachments/assets/ebe44550-8a40-4e17-84eb-75fe6fce5df5) | ![attn-head-4](https://github.com/user-attachments/assets/32439d8d-7798-40e2-a4aa-d0e109afe1b5) | ![attn-head-5](https://github.com/user-attachments/assets/2faff471-fba1-4456-8332-e66a4a05bc5d) |\r\n"
    },
    {
        "author": {
            "login": "HuggingFaceDocBuilderDev"
        },
        "body": "The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers.js/pr_1117). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update."
    }
]