[
    {
        "title": "Add support for Moonshine ASR"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "This PR adds support for Moonshine, a family of speech-to-text models optimized for fast and accurate automatic speech recognition (ASR) on resource-constrained devices. They are well-suited to real-time, on-device applications like live transcription and voice command recognition, and will be perfect for in-browser usage. This PR is using a dev branch of transformers by @eustlb (https://github.com/huggingface/transformers/pull/34784), and a dev branch of Optimum for ONNX conversion.\r\n\r\n- https://huggingface.co/onnx-community/moonshine-tiny-ONNX\r\n- https://huggingface.co/onnx-community/moonshine-base-ONNX\r\n\r\n## Example usage:\r\n\r\nWith pipeline API:\r\n```js\r\nimport { pipeline } from \"@huggingface/transformers\";\r\n\r\nconst transcriber = await pipeline(\"automatic-speech-recognition\", \"onnx-community/moonshine-tiny-ONNX\");\r\nconst output = await transcriber(\"https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav\");\r\nconsole.log(output);\r\n// { text: 'And so my fellow Americans ask not what your country can do for you as what you can do for your country.' }\r\n```\r\n\r\nWithout pipeline API:\r\n```js\r\nimport { MoonshineForConditionalGeneration, AutoProcessor, read_audio } from \"@huggingface/transformers\";\r\n\r\n// Load model and processor\r\nconst model_id = \"onnx-community/moonshine-tiny-ONNX\";\r\nconst model = await MoonshineForConditionalGeneration.from_pretrained(model_id, {\r\n    dtype: \"q4\",\r\n});\r\nconst processor = await AutoProcessor.from_pretrained(model_id);\r\n\r\n// Load audio and prepare inputs\r\nconst audio = await read_audio(\"https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav\", 16000);\r\nconst inputs = await processor(audio);\r\n\r\n// Generate outputs\r\nconst outputs = await model.generate({ ...inputs, max_new_tokens: 100 });\r\n\r\n// Decode outputs\r\nconst decoded = processor.batch_decode(outputs, { skip_special_tokens: true });\r\nconsole.log(decoded[0]);\r\n// And so my fellow Americans ask not what your country can do for you, ask what you can do for your country.\r\n```\r\n\r\ncloses #990 "
    },
    {
        "author": {
            "login": "HuggingFaceDocBuilderDev"
        },
        "body": "The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/transformers.js/pr_1099). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update."
    },
    {
        "author": {
            "login": "xenova"
        },
        "body": "Model works with WebGPU too, and I've adapted [this real-time demo](https://huggingface.co/spaces/Xenova/realtime-whisper-webgpu) to work with model. Significantly faster than the whisper version. \ud83d\udd25 "
    }
]