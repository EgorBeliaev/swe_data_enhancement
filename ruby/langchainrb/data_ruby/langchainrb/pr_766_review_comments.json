[
    {
        "title": "Allow setting response_object: {} parameter when initializing supported Langchain::LLM::* classes"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "Setting `default_options: {response_format: ... }` when supported classes are initialized:\r\n```ruby\r\n# Setting \r\nirb(main):002> llm = Langchain::LLM::MistralAI.new(api_key: ENV[\"MISTRAL_AI_API_KEY\"], default_options: {response_format: {type: \"json_object\"}})\r\n=>\r\n#<Langchain::LLM::MistralAI:0x0000000125076888\r\n```\r\n# Passes that option to the chat calls. JSON format is returned:\r\n```ruby\r\nirb(main):003> llm.chat messages:[{role:\"user\", content:\"Hello json!\"}]\r\n=>\r\n#<Langchain::LLM::MistralAIResponse:0x000000012431be68\r\n @model=nil,\r\n @raw_response=\r\n  {\"id\"=>\"4837d4b042874aab8b5c29a1cd0bef99\",\r\n   \"choices\"=>[{\"index\"=>0, \"message\"=>{\"role\"=>\"assistant\", \"content\"=>\"{\\\"message\\\": \\\"Hello! How can I assist you today?\\\"}\", \"tool_calls\"=>nil}, \"finish_reason\"=>\"stop\", \"logprobs\"=>nil}]\r\n   ```"
    },
    {
        "author": {
            "login": "codenamev"
        },
        "body": ""
    },
    {
        "author": {
            "login": "codenamev"
        },
        "body": ""
    },
    {
        "author": {
            "login": "andreibondarev"
        },
        "body": ""
    },
    {
        "author": {
            "login": "codenamev"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bibstha"
        },
        "body": "Looking forward to this change, as looks like OpenAI's structured response is quite reliable."
    },
    {
        "author": {
            "login": "andreibondarev"
        },
        "body": "> Looking forward to this change, as looks like OpenAI's structured response is quite reliable.\r\n\r\nI'll try to get this out later tonight!"
    }
]