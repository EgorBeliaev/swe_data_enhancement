[
    {
        "title": "feat: add field for environment variables for Llama server"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "This PR closes #549 \r\n\r\nAdds new text input field for setting additional environment variables for the LlamaCPP server to enable setting e.g. [CUDA variables](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars)\r\n![image](https://github.com/carlrobertoh/CodeGPT/assets/39240633/0034e14a-1713-4348-b6a0-7a6fcc3a36e2)\r\n"
    },
    {
        "data": {
            "repository": {
                "issue": {
                    "title": "Add a CUDA_VISIBLE_DEVICES option or a option to restrict the number of GPUs",
                    "body": "### Describe the need of your request\n\nCurrently, the project does not provide a way to limit the number of GPUs or control the GPU resources allocated to the application. This can lead to inefficient resource utilization, especially in environments with multiple GPUs. It can also be problematic in shared environments where multiple programs or processes need to share GPU resources without interference.\r\n\r\n\n\n### Proposed solution\n\nIntroduce a configuration option to set the CUDA_VISIBLE_DEVICES environment variable within the application. This will allow users to specify which GPUs should be visible and utilized by the application, thereby controlling the maximum GPU usage. The option can be set through a configuration variable.\r\n\r\n\n\n### Additional context\n\nImplementing this feature will enhance resource management, particularly in multi-GPU setups. It will provide users with greater control over their GPU resources, improving the efficiency and flexibility of the application. This is especially useful for scenarios involving varying task requirements\r\n\r\n\r\n\r\n",
                    "state": "CLOSED",
                    "comments": {
                        "nodes": []
                    }
                }
            }
        }
    }
]