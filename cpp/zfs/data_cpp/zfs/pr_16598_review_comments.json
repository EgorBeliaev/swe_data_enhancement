[
    {
        "title": "Always validate checksums for Direct I/O reads"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "This fixes an oversight in the Direct I/O PR. There is nothing stops a process from manipulating the contents of a buffer for a Direct I/O read while the I/O is in flight. This can lead checksum verify failures. However, the disk contents are stil correct, but this would lead false reporting of checksum validations.\r\n\r\nTo remedy this, all Direct I/O reads that have a checksum verification failure are treated as suspicious. In the event a checksum validation failure occurs for a Direct I/O read, then the I/O request will be reissued though the ARC. This allows for actual validation to happen and removes any possibility of the buffer being manipulated after the I/O has been issued.\r\n\r\nJust as with Direct I/O write checksum validation failures, Direct I/O read checksum validation failures are reported though zpool status -d in the DIO columnm. Also the zevent has been updated to have both:\r\n1. dio_verify_wr -> Checksum verification failure for writes\r\n2. dio_verify_rd -> Checksum verification failure for reads. This allows for determining what I/O operation was the culprit for the checksum verification failure. All DIO errors are reported only on the top-level VDEV.\r\n\r\nEven though FreeBSD can write protect pages (stable pages) it still has the same issue as Linux in department.\r\n\r\nThis commit updates the following:\r\n1. Propogates checksum failures for reads all the way up to the top-level VDEV.\r\n2. Reports errors through zpool status -d as DIO.\r\n3. Has two zevents for checksum verify errors with Direct I/O. One for read and one for write.\r\n4. Updates FreeBSD ABD code to also check for ABD_FLAG_FROM_PAGES and handle ABD buffer contents validation the same as Linux.\r\n5. Moves the declartion of nbytes in zfs_read() to the top of the function and outside of the while loop. This was needed due to a compliation failure in FreeBSD.\r\n6. Updated manipulate_user_buffer.c to also manipulate a buffer while a Direct I/O read is taking place.\r\n7. Adds a new test case dio_read_verify that stress tests the new code.\r\n\r\nThis issue was first observed when installing a Windows 11 VM on a ZFS dataset with the dataset property direct set to always. The zpool devices would report checksum failures, but running a subsequent zpool scrub would not repair any data and report no errors.\r\n\r\n<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->\r\n\r\n<!--- Provide a general summary of your changes in the Title above -->\r\nMake sure that checksum verification failures with Direct I/O reads are reported and always reissued through the ARC.\r\n<!---\r\nDocumentation on ZFS Buildbot options can be found at\r\nhttps://openzfs.github.io/openzfs-docs/Developer%20Resources/Buildbot%20Options.html\r\n-->\r\n\r\n### Motivation and Context\r\n<!--- Why is this change required? What problem does it solve? -->\r\nWithout this, there are false positive checksum verification failures reported when a Direct I/O read is issued and a process is manipulating the buffer contents.\r\n<!--- If it fixes an open issue, please link to the issue here. -->\r\n\r\n### Description\r\n<!--- Describe your changes in detail -->\r\nSee above.\r\n\r\n### How Has This Been Tested?\r\n<!--- Please describe in detail how you tested your changes. -->\r\nAdded new test case. Tested on Linux: 6.5.12-100.fc37.x86_64 kernel. On that same Linux kernel I used Virtual Manger to install Windows 11, Ubuntu 22.04, and Fedora 38 on a dataset with the direct dataset property set to always. Ran ZTS direct test cases as well.\r\n\r\nOn FreeBSD tested ZTS direct tests on FreeBSD 13.3 -RELEASE and FreeBSD 14-CURRENT VM's.\r\n<!--- Include details of your testing environment, and the tests you ran to -->\r\n<!--- see how your change affects other areas of the code, etc. -->\r\n<!--- If your change is a performance enhancement, please provide benchmarks here. -->\r\n<!--- Please think about using the draft PR feature if appropriate -->\r\n\r\n### Types of changes\r\n<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Performance enhancement (non-breaking change which improves efficiency)\r\n- [x] Code cleanup (non-breaking change which makes code smaller or more readable)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] Library ABI change (libzfs, libzfs\\_core, libnvpair, libuutil and libzfsbootenv)\r\n- [x] Documentation (a change to man pages or other documentation)\r\n\r\n### Checklist:\r\n<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->\r\n<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->\r\n- [x] My code follows the OpenZFS [code style requirements](https://github.com/openzfs/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).\r\n- [x] I have updated the documentation accordingly.\r\n- [x] I have read the [**contributing** document](https://github.com/openzfs/zfs/blob/master/.github/CONTRIBUTING.md).\r\n- [x] I have added [tests](https://github.com/openzfs/zfs/tree/master/tests) to cover my changes.\r\n- [x] I have run the ZFS Test Suite with this change applied.\r\n- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/openzfs/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).\r\n"
    },
    {
        "author": {
            "login": "tonyhutter"
        },
        "body": ""
    },
    {
        "author": {
            "login": "tonyhutter"
        },
        "body": ""
    },
    {
        "author": {
            "login": "tonyhutter"
        },
        "body": ""
    },
    {
        "author": {
            "login": "behlendorf"
        },
        "body": "The verification and error propagation looks good.  Thanks for running down this very strange but entirely possible case.  Most of the comments are for minor nits."
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "behlendorf"
        },
        "body": "Good catch on the `vdev_mirror_io_done()` case.  Yes, the updated logic looks correct to me, we always want to fall back to reading through the ARC in the event of a checksum failure.  It seems to me we also should do something similar for raidz and draid.  Repair IOs are issued from pages in the user ABD which could change after the checksum verification like with the mirror."
    },
    {
        "author": {
            "login": "amotin"
        },
        "body": ""
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": ""
    },
    {
        "author": {
            "login": "behlendorf"
        },
        "body": ""
    },
    {
        "author": {
            "login": "amotin"
        },
        "body": ""
    },
    {
        "author": {
            "login": "tonyhutter"
        },
        "body": "Note: I'm not seeing the DIO errors in the JSON output (`zpool status -d -j | jq`).  I dunno if you want to add that in this PR or another PR, but thought it was worth mentioning."
    },
    {
        "author": {
            "login": "tonyhutter"
        },
        "body": "This should give you the JSON:\r\n```diff\r\ndiff --git a/cmd/zpool/zpool_main.c b/cmd/zpool/zpool_main.c\r\nindex aa7da68aa..90f4225e1 100644\r\n--- a/cmd/zpool/zpool_main.c\r\n+++ b/cmd/zpool/zpool_main.c\r\n@@ -9222,6 +9222,11 @@ vdev_stats_nvlist(zpool_handle_t *zhp, status_cbdata_t *cb, nvlist_t *nv,\r\n                                fnvlist_add_string(vds, \"power_state\", \"-\");\r\n                        }\r\n                }\r\n+               if (cb->cb_print_dio_verify) {\r\n+                       nice_num_str_nvlist(vds, \"dio_verify_errors\",\r\n+                           vs->vs_dio_verify_errors, cb->cb_literal,\r\n+                           cb->cb_json_as_int, ZFS_NICENUM_1024);\r\n+               }\r\n        }\r\n \r\n        if (nvlist_lookup_uint64(nv, ZPOOL_CONFIG_NOT_PRESENT,\r\n```"
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": "> This should give you the JSON:\r\n> \r\n> ```diff\r\n> diff --git a/cmd/zpool/zpool_main.c b/cmd/zpool/zpool_main.c\r\n> index aa7da68aa..90f4225e1 100644\r\n> --- a/cmd/zpool/zpool_main.c\r\n> +++ b/cmd/zpool/zpool_main.c\r\n> @@ -9222,6 +9222,11 @@ vdev_stats_nvlist(zpool_handle_t *zhp, status_cbdata_t *cb, nvlist_t *nv,\r\n>                                 fnvlist_add_string(vds, \"power_state\", \"-\");\r\n>                         }\r\n>                 }\r\n> +               if (cb->cb_print_dio_verify) {\r\n> +                       nice_num_str_nvlist(vds, \"dio_verify_errors\",\r\n> +                           vs->vs_dio_verify_errors, cb->cb_literal,\r\n> +                           cb->cb_json_as_int, ZFS_NICENUM_1024);\r\n> +               }\r\n>         }\r\n>  \r\n>         if (nvlist_lookup_uint64(nv, ZPOOL_CONFIG_NOT_PRESENT,\r\n> ```\r\n\r\nGood catch. I add your patch to this PR."
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": "@amotin and I had a conversation. It was incorrect for me to allow the mirror VDEV to issue reads down to other children in the event of a Direct I/O read checksum verify failure. The issue is, that another child could get a valid checksum, and then the buffer is manipulated again. This would lead to self healing in `vdev_mirror_io_done()` issue bad data down to the other children.\r\n\r\nI updated `vdev_mirror_io_done()` to return immediately once a Direct I/O read checksum verification failure occurs. I also updated the comments in that code block. In `zio_vdev_child_io()` I now `ASSERT` that the parent can not have a Direct I/O checksum validation error. This makes sure children in the mirror are not being used to read after the initial checksum validation failure.\r\n\r\n@behlendorf and @tonyhutter, can you relook at those two spots in the code and make sure my comments make sense to you? "
    },
    {
        "author": {
            "login": "bwatkinson"
        },
        "body": "> Good catch on the `vdev_mirror_io_done()` case. Yes, the updated logic looks correct to me, we always want to fall back to reading through the ARC in the event of a checksum failure. It seems to me we also should do something similar for raidz and draid. Repair IOs are issued from pages in the user ABD which could change after the checksum verification like with the mirror.\r\n\r\nGood catch. As per your suggestion, I now leverage the `rc_allow_repair` to make sure we never issue self healing for Direct I/O reads for raids and dRAID VDEVs."
    }
]