[
    {
        "title": "Fix Flux: clip_l support (SD3/3.5 improvements included)"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": "Fixes #396 \r\n\r\nI made the clip backend skip the last text projection if the `text_projection` tensor doesn't exist This is mathematically equaivalent to replacing the `text_projection` with the identity matrix (aka `torch.eye()`).\r\nAlso replaced the matrix multiplication with a biasless linear layer when `text_projection` exists (somehow this changes the outcome).\r\n\r\n### Flux.1 Schnell (q3_k): \r\n|  | clip-L |  ViT-L-14 |\r\n| - | - | - |\r\n| master | ![test-schnell-clip](https://github.com/user-attachments/assets/fdfab0f1-4160-4174-93bf-e4aedecfbef2) | ![test-schnell-vit](https://github.com/user-attachments/assets/77f9bb03-26c5-4f0a-a1d5-c0685e5193bf) |\r\n| PR | ![test-schnell-clip](https://github.com/user-attachments/assets/1349f00e-b6ed-4780-aaeb-bf0144b0f3fd) | ![test-schnell-vit](https://github.com/user-attachments/assets/fc566964-04ad-4cb1-8438-28f1a5d8d9f9) |\r\n\r\n\r\n### SD3 2B (q8_0):\r\n| Master | PR |\r\n| - | - |\r\n| ![sd3-master](https://github.com/user-attachments/assets/3efbf7c3-bf4b-4b0f-a0ba-9f05083506e2) | ![output](https://github.com/user-attachments/assets/5a3c7013-0f87-4489-84a2-01e205080b21) |\r\n\r\n### SD3.5 Large Turbo (q4_1)\r\n\r\n| Master | PR |\r\n| - | - |\r\n| ![output](https://github.com/user-attachments/assets/a0978348-ad7a-4669-a5fa-1333efde3ce7) | ![output](https://github.com/user-attachments/assets/89e2aa31-2c3b-4a05-9e8a-321f1ed2b09d) |\r\n"
    },
    {
        "author": {
            "login": "Green-Sky"
        },
        "body": ""
    },
    {
        "author": {
            "login": "stduhpf"
        },
        "body": "If anyone know how this is all supposed to work, feel free to improve the code."
    },
    {
        "author": {
            "login": "stduhpf"
        },
        "body": "I'm pretty sure this is not the way it's supposed to work, so I'm drafting this PR until I figure out how to make it work properly. ~~Right now, I think this is only adding some \"noise\" the the t5 prompt.~~"
    },
    {
        "author": {
            "login": "stduhpf"
        },
        "body": "Ok I believe i got it now"
    },
    {
        "author": {
            "login": "stduhpf"
        },
        "body": "Hard-coding the prompt for clip to always be `\"Painting, in the style of starry night by Van Gogh\"`, while keeping the same example prompt for T5 (`\"a lovely cat holding a sign says 'flux.cpp'\"`) now gives this result: \r\n![test](https://github.com/user-attachments/assets/40574338-aab8-4b8b-b241-b5eaee080f75)\r\nI'm 99.99% certain this PR is now ready for merge.\r\n\r\n<details>\r\n<summary> Patch for hardcoded Clip prompt </summary>\r\n\r\n```patch\r\ndiff --git a/conditioner.hpp b/conditioner.hpp\r\nindex 8a710d1..75efb08 100644\r\n--- a/conditioner.hpp\r\n+++ b/conditioner.hpp\r\n@@ -1038,9 +1038,10 @@ struct FluxCLIPEmbedder : public Conditioner {\r\n         std::vector<float> t5_weights;\r\n         for (const auto& item : parsed_attention) {\r\n             const std::string& curr_text = item.first;\r\n+            const std::string& curr_text_l = \"Painting, in the style of starry night by Van Gogh\";\r\n             float curr_weight            = item.second;\r\n \r\n-            std::vector<int> curr_tokens = clip_l_tokenizer.encode(curr_text, on_new_token_cb);\r\n+            std::vector<int> curr_tokens = clip_l_tokenizer.encode(curr_text_l, on_new_token_cb);\r\n             clip_l_tokens.insert(clip_l_tokens.end(), curr_tokens.begin(), curr_tokens.end());\r\n             clip_l_weights.insert(clip_l_weights.end(), curr_tokens.size(), curr_weight);\r\n \r\n```\r\n\r\n</details>\r\n\r\nEdit: I tried this test with my previous attempt at fixing clip ([e6314d3](https://github.com/leejet/stable-diffusion.cpp/pull/397/commits/e6314d39a8d157dd35a39be781e3163754bcd155)) and I got a similar result. It was actually working-ish even though it was definitely not implemented like in the official Flux inference code.\r\n\r\n<details>\r\n<summary> Comparison </summary>\r\n\r\n(all of those with the same hardcoded prompt for clip_l)\r\n\r\n| PR (af4f83f) | Previous attempt (d7679c9) | Master |\r\n| - | - | - |\r\n| ![test](https://github.com/user-attachments/assets/40574338-aab8-4b8b-b241-b5eaee080f75) | ![test](https://github.com/user-attachments/assets/b54659a8-74cd-4777-9070-3bbed4e18125) | ![test](https://github.com/user-attachments/assets/2607ee0b-2f93-4560-85bf-70873c760512) |\r\n</details>\r\n\r\n\r\n"
    },
    {
        "author": {
            "login": "stduhpf"
        },
        "body": "@leejet does this look good enough to merge?"
    },
    {
        "author": {
            "login": "stduhpf"
        },
        "body": "This breaks SD3.5 support somehow!"
    },
    {
        "author": {
            "login": "stduhpf"
        },
        "body": "Ok it works again now."
    },
    {
        "author": {
            "login": "leejet"
        },
        "body": "Thank you for your contribution"
    },
    {
        "data": {
            "repository": {
                "issue": {
                    "title": "Bug: Clip-L does absolutely nothing with Flux models.",
                    "body": "I was testing out alternative clip models for Flux (https://huggingface.co/zer0int/CLIP-GmP-ViT-L-14), and noticed the output were oddly similar to the ones I was getting with the original clip-l. \r\nTurns out the result are actually 100% identical (when using the same seed of course), not even a single bit of difference. Using the same models with ComfyUI does lead to noticably different outputs (with fixed seed).\r\n\r\n|  | clip-L |  ViT-L-14 |\r\n| - | - | - |\r\n| Dev q3_k (sd.cpp) | ![test-dev-clip](https://github.com/user-attachments/assets/84b921d7-62c7-4251-ba68-f73ac60d5057) | ![test-dev-vit](https://github.com/user-attachments/assets/c7824e1d-cab0-4460-9513-97a51548bc8a) |\r\n| Schnell q3_k  (sd.cpp) | ![test-schnell-clip](https://github.com/user-attachments/assets/fdfab0f1-4160-4174-93bf-e4aedecfbef2) | ![test-schnell-vit](https://github.com/user-attachments/assets/77f9bb03-26c5-4f0a-a1d5-c0685e5193bf) |\r\n| Schnell fp8 (Comfyui) | ![ComfyUI_00515_](https://github.com/user-attachments/assets/524c01a0-23c5-4dc2-8bb6-ed73d427549f) | ![ComfyUI_00514_](https://github.com/user-attachments/assets/a90cce3a-a323-4c26-b8cf-91f29ae26e4e) |\r\n| Schnell q3_k (Comfyui) | ![ComfyUI_00517_](https://github.com/user-attachments/assets/6ad61f77-879e-43cd-a691-25ebd2d9c6bb) | ![ComfyUI_00516_](https://github.com/user-attachments/assets/b395f1c9-889d-4abf-8169-9a97c5b5f164) |\r\n\r\nAs a sidenote, this could explain why I was feeling like the generations I get with sdcpp look less detailed and more artifacty than when I'm using ComfyUI at the same quants.\r\n\r\n",
                    "state": "CLOSED",
                    "comments": {
                        "nodes": [
                            {
                                "author": {
                                    "login": "Green-Sky"
                                },
                                "body": "You are right, it is commented out and just set to 0.\r\nhttps://github.com/leejet/stable-diffusion.cpp/blob/14206fd48832ab600d9db75f15acb5062ae2c296/conditioner.hpp#L1107-L1121"
                            }
                        ]
                    }
                }
            }
        }
    }
]