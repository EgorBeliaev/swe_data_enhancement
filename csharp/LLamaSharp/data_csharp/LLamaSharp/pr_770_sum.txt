### Changes to be Made:

1. **BatchedExecutor Support:**
   - Add support for embeddings prompting to `BatchedExecutor`, enabling support for Llava.

2. **Batch Queue Update:**
   - Modify the batch queue in `BatchedExecutor` to allow two possible types: token batches and embedding batches.

3. **Inference Lock Update:**
   - Switch the inference lock to an integer using Interlocked for better interaction with async operations.

4. **New Methods Added:**
   - Implement `Conversation.Prompt` method for handling `Span<float>` representing raw embeddings.
   - Implement support for `SafeLlavaImageEmbedHandle`.

5. **New Batch Class Implemented:**
   - Introduce `LLamaBatchEmbeddings`, analogous to `LLamaBatch`, specifically for managing embeddings instead of tokens. 

### Issue Addressed:

This PR primarily aims to extend the functionality of `BatchedExecutor` to handle embeddings, facilitating expanded use cases such as supporting Llava. The changes provide foundational support for future high-level executor implementations by offering more versatile handling of data types within the executor, thus aligning it with modern requirements and improving usability for developers working on related features.