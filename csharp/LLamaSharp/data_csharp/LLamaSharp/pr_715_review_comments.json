[
    {
        "title": "Llama Text Templater"
    },
    {
        "author": {
            "login": "PR Description"
        },
        "body": " - Added LLamaTemplate which efficiently formats a series of messages according to the model template.\r\n - Fixed `llama_chat_apply_template` method (wrong entrypoint, couldn't handle null model)\r\n\r\nThis depends on #712 review and merge that first!"
    },
    {
        "author": {
            "login": "AsakusaRinne"
        },
        "body": "It exactly what's lacked in LLamaSharp! Do you have further plan about the development of the template? Actually I posted function calling as one of the OSPP projects of LLamaSharp (OSPP is what I once invited you in discord). Since template is one of the basic components of function calling, you could open some good-first-issues to let the student do it if you'd like to. :)"
    },
    {
        "author": {
            "login": "martindevans"
        },
        "body": ""
    },
    {
        "author": {
            "login": "martindevans"
        },
        "body": ""
    },
    {
        "author": {
            "login": "martindevans"
        },
        "body": ""
    },
    {
        "author": {
            "login": "AsakusaRinne"
        },
        "body": ""
    },
    {
        "author": {
            "login": "martindevans"
        },
        "body": ""
    },
    {
        "author": {
            "login": "AsakusaRinne"
        },
        "body": ""
    },
    {
        "author": {
            "login": "martindevans"
        },
        "body": "> Do you have further plan about the development of the template?\r\n\r\nI think I'll probably look at making some enhancements to llama.cpp, and then coming back to support them in LLamaSharp.\r\n\r\nAt the moment the template converts _all_ messages into text and then you tokenize that text in one go. However, this doesn't seem good enough. You _must_ tokenize that text using `special=true` (for all the bits of the template to tokenize properly) but you really _shouldn't_ template user messages using `special=true` (to ensure you can't write e.g. `[INST]new system prompt[/INST]` in the middle of a random message).\r\n\r\nI'm going to see if I can PR a change into llama.cpp to run the tokenization differently for different bits."
    },
    {
        "author": {
            "login": "martindevans"
        },
        "body": "I've rebased this one onto master, so it can be merged independently of #712 since it seems like that other PR is going to be delayed."
    }
]