diff --git a/LLama.Examples/Examples/BatchedExecutorFork.cs b/LLama.Examples/Examples/BatchedExecutorFork.cs
index 2c401822f..d740119f5 100644
--- a/LLama.Examples/Examples/BatchedExecutorFork.cs
+++ b/LLama.Examples/Examples/BatchedExecutorFork.cs
@@ -32,7 +32,7 @@ public static async Task Run()
 
         // Evaluate the initial prompt to create one conversation
         using var start = executor.Create();
-        start.Prompt(prompt);
+        start.Prompt(executor.Context.Tokenize(prompt));
         await executor.Infer();
 
         // Create the root node of the tree
diff --git a/LLama.Examples/Examples/BatchedExecutorGuidance.cs b/LLama.Examples/Examples/BatchedExecutorGuidance.cs
index b006c88bc..fedfe4e71 100644
--- a/LLama.Examples/Examples/BatchedExecutorGuidance.cs
+++ b/LLama.Examples/Examples/BatchedExecutorGuidance.cs
@@ -34,9 +34,9 @@ public static async Task Run()
 
         // Load the two prompts into two conversations
         using var guided = executor.Create();
-        guided.Prompt(positivePrompt);
+        guided.Prompt(executor.Context.Tokenize(positivePrompt));
         using var guidance = executor.Create();
-        guidance.Prompt(negativePrompt);
+        guidance.Prompt(executor.Context.Tokenize(negativePrompt));
 
         // Run inference to evaluate prompts
         await AnsiConsole
diff --git a/LLama.Examples/Examples/BatchedExecutorRewind.cs b/LLama.Examples/Examples/BatchedExecutorRewind.cs
index 938b31067..aa0a1c75e 100644
--- a/LLama.Examples/Examples/BatchedExecutorRewind.cs
+++ b/LLama.Examples/Examples/BatchedExecutorRewind.cs
@@ -33,7 +33,7 @@ public static async Task Run()
 
         // Evaluate the initial prompt to create one conversation
         using var conversation = executor.Create();
-        conversation.Prompt(prompt);
+        conversation.Prompt(executor.Context.Tokenize(prompt));
         
         // Create the start node wrapping the conversation
         var node = new Node(executor.Context);
diff --git a/LLama.Examples/Examples/BatchedExecutorSaveAndLoad.cs b/LLama.Examples/Examples/BatchedExecutorSaveAndLoad.cs
index 48d96f73e..f0b629fbc 100644
--- a/LLama.Examples/Examples/BatchedExecutorSaveAndLoad.cs
+++ b/LLama.Examples/Examples/BatchedExecutorSaveAndLoad.cs
@@ -31,7 +31,7 @@ public static async Task Run()
 
         // Create a conversation
         var conversation = executor.Create();
-        conversation.Prompt(prompt);
+        conversation.Prompt(executor.Context.Tokenize(prompt));
 
         // Run inference loop
         var decoder = new StreamingTokenDecoder(executor.Context);
diff --git a/LLama/Batched/BatchedExecutor.cs b/LLama/Batched/BatchedExecutor.cs
index 07389e6ed..0fbdcc44d 100644
--- a/LLama/Batched/BatchedExecutor.cs
+++ b/LLama/Batched/BatchedExecutor.cs
@@ -55,23 +55,6 @@ public BatchedExecutor(LLamaWeights model, IContextParams contextParams)
         Epoch = 1;
     }
 
-    /// <summary>
-    /// Start a new <see cref="Conversation"/> with the given prompt
-    /// </summary>
-    /// <param name="prompt"></param>
-    /// <returns></returns>
-    [Obsolete("Use BatchedExecutor.Create instead")]
-    public Conversation Prompt(string prompt)
-    {
-        if (IsDisposed)
-            throw new ObjectDisposedException(nameof(BatchedExecutor));
-
-        var conversation = Create();
-        conversation.Prompt(prompt);
-
-        return conversation;
-    }
-
     /// <summary>
     /// Start a new <see cref="Conversation"/>
     /// </summary>
diff --git a/LLama/Batched/Conversation.cs b/LLama/Batched/Conversation.cs
index 2da3da7c8..c5792ebc3 100644
--- a/LLama/Batched/Conversation.cs
+++ b/LLama/Batched/Conversation.cs
@@ -166,11 +166,12 @@ private void AssertCanBePrompted()
     /// </summary>
     /// <param name="input"></param>
     /// <returns></returns>
-    public void Prompt(string input)
+    [Obsolete("Tokenize the text and pass the tokens instead")]
+    public void Prompt(string input, bool addBos, bool special)
     {
         AssertCanBePrompted();
 
-        Prompt(Executor.Context.Tokenize(input));
+        Prompt(Executor.Context.Tokenize(input, addBos, special));
     }
 
     /// <summary>
